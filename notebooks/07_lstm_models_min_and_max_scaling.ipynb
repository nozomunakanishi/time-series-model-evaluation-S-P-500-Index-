{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32641d36-247e-4e4c-83c5-a15d0be3fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries for the project. \n",
    "import pandas as pd \n",
    "import yfinance as yf\n",
    "import numpy as np # for numerical operations\n",
    "import seaborn as sns #visualisation\n",
    "import matplotlib.pyplot as plt # visualization\n",
    "%matplotlib inline \n",
    "import matplotlib.ticker as ticker # Library to customize ticks\n",
    "from datetime import date\n",
    "import matplotlib.dates as mdates\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from math import sqrt\n",
    "# Mean Squared Error and Mean Absolute Error metrics from sklearn for evaluating model accuracy.\n",
    "from tensorflow.keras.losses import MeanAbsoluteError, MeanSquaredError\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "# TimeSeriesSplit for cross-validation with time series data to preserve the temporal ordering of data.\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import BatchNormalization, Dropout, Dense, LSTM\n",
    "import random\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from keras_tuner import BayesianOptimization\n",
    "\n",
    "import warnings # filter warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5813a563-6848-479a-8ea8-ca37e3f61555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset after the Log Returns transformation.\n",
    "df1 = pd.read_csv('df1.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "# Along with the importation, the code will set the date as the index, value of 0 because is the first column.\n",
    "## Setting parse_dates=true to ensure the dates are in datetime, making more appropriate for ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e064483-70f6-4fad-bfba-da3571c3b66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>lag_1</th>\n",
       "      <th>lag_4</th>\n",
       "      <th>rolling_mean_4w</th>\n",
       "      <th>rolling_mean_12w</th>\n",
       "      <th>volatility_4w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1031.000000</td>\n",
       "      <td>1031.000000</td>\n",
       "      <td>1031.000000</td>\n",
       "      <td>1031.000000</td>\n",
       "      <td>1031.000000</td>\n",
       "      <td>1031.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.001575</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>0.001508</td>\n",
       "      <td>0.001578</td>\n",
       "      <td>0.001516</td>\n",
       "      <td>7.731465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.020723</td>\n",
       "      <td>0.020744</td>\n",
       "      <td>0.020750</td>\n",
       "      <td>0.009796</td>\n",
       "      <td>0.005364</td>\n",
       "      <td>23.505939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.069935</td>\n",
       "      <td>-0.069935</td>\n",
       "      <td>-0.069935</td>\n",
       "      <td>-0.051352</td>\n",
       "      <td>-0.026288</td>\n",
       "      <td>0.154122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.007633</td>\n",
       "      <td>-0.007633</td>\n",
       "      <td>-0.007858</td>\n",
       "      <td>-0.003032</td>\n",
       "      <td>-0.001013</td>\n",
       "      <td>1.538535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.003635</td>\n",
       "      <td>0.003666</td>\n",
       "      <td>0.003626</td>\n",
       "      <td>0.002937</td>\n",
       "      <td>0.002346</td>\n",
       "      <td>2.585030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.013568</td>\n",
       "      <td>0.013643</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.007596</td>\n",
       "      <td>0.004979</td>\n",
       "      <td>6.157669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.051947</td>\n",
       "      <td>0.051947</td>\n",
       "      <td>0.051947</td>\n",
       "      <td>0.031089</td>\n",
       "      <td>0.019145</td>\n",
       "      <td>338.128664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Close        lag_1        lag_4  rolling_mean_4w  \\\n",
       "count  1031.000000  1031.000000  1031.000000      1031.000000   \n",
       "mean      0.001575     0.001608     0.001508         0.001578   \n",
       "std       0.020723     0.020744     0.020750         0.009796   \n",
       "min      -0.069935    -0.069935    -0.069935        -0.051352   \n",
       "25%      -0.007633    -0.007633    -0.007858        -0.003032   \n",
       "50%       0.003635     0.003666     0.003626         0.002937   \n",
       "75%       0.013568     0.013643     0.013514         0.007596   \n",
       "max       0.051947     0.051947     0.051947         0.031089   \n",
       "\n",
       "       rolling_mean_12w  volatility_4w  \n",
       "count       1031.000000    1031.000000  \n",
       "mean           0.001516       7.731465  \n",
       "std            0.005364      23.505939  \n",
       "min           -0.026288       0.154122  \n",
       "25%           -0.001013       1.538535  \n",
       "50%            0.002346       2.585030  \n",
       "75%            0.004979       6.157669  \n",
       "max            0.019145     338.128664  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "678c9628-17d6-4b3c-9e54-2fd8aa738302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the selected exogenous variables and target variable.\n",
    "features = df1[['rolling_mean_4w', 'rolling_mean_12w', 'volatility_4w', 'lag_1', 'lag_4']]\n",
    "target = df1['Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0825cf31-1753-4b18-bcfa-32cb7b2b990e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Training Features - Min: -1.0 Max: 1.0\n",
      "Scaled Training Target - Min: -1.0 Max: 0.9999999999999999\n",
      "Scaled Test Features - Min: -1.0 Max: 4.748476030153254\n",
      "Scaled Test Target - Min: -1.0 Max: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing sets.\n",
    "train_size = int(0.85 * len(features))\n",
    "X_train, X_test = features[:train_size], features[train_size:]\n",
    "y_train, y_test = target[:train_size], target[train_size:]\n",
    "\n",
    "# Initialize MinMaxScalers to scale data to the range [-1, 1].\n",
    "scaler_X = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_y = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit scalers on training data and transform both training and test data.\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)  # Fit scaler on training features and transform.\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))  # Fit scaler on training target and transform.\n",
    "\n",
    "# Transform test data without fitting again to avoid data leakage.\n",
    "X_test_scaled = scaler_X.transform(X_test)  # Transform test features using the scaler fitted on training data.\n",
    "y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1))  # Transform test target using the scaler fitted on training data.\n",
    "\n",
    "# Print minimum and maximum values of the scaled training and test data for verification.\n",
    "print(\"Scaled Training Features - Min:\", X_train_scaled.min(), \"Max:\", X_train_scaled.max())\n",
    "print(\"Scaled Training Target - Min:\", y_train_scaled.min(), \"Max:\", y_train_scaled.max())\n",
    "print(\"Scaled Test Features - Min:\", X_test_scaled.min(), \"Max:\", X_test_scaled.max())\n",
    "print(\"Scaled Test Target - Min:\", y_test_scaled.min(), \"Max:\", y_test_scaled.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6158c5fe-3304-4ad6-a240-ab4237d5a9f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sequence Length = 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fc19fe2-577b-4943-9d26-996b2682d78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped X_train_seq shape: (872, 4, 5)\n",
      "Reshaped y_train_seq shape: (872, 1)\n",
      "Reshaped X_test_seq shape: (151, 4, 5)\n",
      "Reshaped  y_test_seq shape: (151, 1)\n"
     ]
    }
   ],
   "source": [
    "# Set the sequence length for LSTM input, representing the number of time steps.\n",
    "sequence_length = 4  # Number of time steps used for target prediction, 4 weeks.\n",
    "\n",
    "# Reshape data into sequences for LSTM.\n",
    "# Initialize empty lists to hold the sequences for the training set.\n",
    "X_train_seq, y_train_seq = [], []\n",
    "# Loop through the training data to create sequences of features and corresponding target values.\n",
    "for i in range(len(X_train_scaled) - sequence_length):\n",
    "    # Append a sequence of features for the current window.\n",
    "    X_train_seq.append(X_train_scaled[i:i + sequence_length]) # Sequence of features.\n",
    "    # Append the target value that comes immediately after the sequence.\n",
    "    y_train_seq.append(y_train_scaled[i + sequence_length]) # Target value following the sequence.\n",
    "# Initialize empty lists to hold the sequences for the test set    \n",
    "X_test_seq, y_test_seq = [], []\n",
    "# Same process for the test sets.\n",
    "for i in range(len(X_test_scaled) - sequence_length):\n",
    "    # Append a sequence of features for the current window in the test set.\n",
    "    X_test_seq.append(X_test_scaled[i:i + sequence_length])\n",
    "    # Append the target value immediately after the sequence in the test set.\n",
    "    y_test_seq.append(y_test_scaled[i + sequence_length])\n",
    "\n",
    "# Convert lists to numpy arrays to use in Neural Networks.\n",
    "X_train_seq, y_train_seq = np.array(X_train_seq), np.array(y_train_seq)\n",
    "X_test_seq, y_test_seq = np.array(X_test_seq), np.array(y_test_seq)\n",
    "\n",
    "# Print the reshaped input data for LSTM.\n",
    "print(f\"Reshaped X_train_seq shape: {X_train_seq.shape}\")\n",
    "print(f\"Reshaped y_train_seq shape: {y_train_seq.shape}\")\n",
    "print(f\"Reshaped X_test_seq shape: {X_test_seq.shape}\")\n",
    "print(f\"Reshaped  y_test_seq shape: { y_test_seq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95804565-cdc0-470c-96a3-74b387960184",
   "metadata": {},
   "source": [
    "### Random Parameters. (Model 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "379a96f6-0d6c-484c-99be-3229bd110680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 191ms/step - loss: 1.2371 - val_loss: 1.2218\n",
      "Epoch 2/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.1956 - val_loss: 1.1838\n",
      "Epoch 3/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.1582 - val_loss: 1.1495\n",
      "Epoch 4/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.1252 - val_loss: 1.1177\n",
      "Epoch 5/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.0954 - val_loss: 1.0874\n",
      "Epoch 6/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0647 - val_loss: 1.0580\n",
      "Epoch 7/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0347 - val_loss: 1.0294\n",
      "Epoch 8/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.0074 - val_loss: 1.0016\n",
      "Epoch 9/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.9798 - val_loss: 0.9746\n",
      "Epoch 10/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.9544 - val_loss: 0.9484\n",
      "Epoch 11/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.9275 - val_loss: 0.9230\n",
      "Epoch 12/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.9026 - val_loss: 0.8983\n",
      "Epoch 13/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.8780 - val_loss: 0.8743\n",
      "Epoch 14/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.8546 - val_loss: 0.8509\n",
      "Epoch 15/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.8316 - val_loss: 0.8282\n",
      "Epoch 16/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.8081 - val_loss: 0.8062\n",
      "Epoch 17/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.7864 - val_loss: 0.7848\n",
      "Epoch 18/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.7646 - val_loss: 0.7641\n",
      "Epoch 19/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.7446 - val_loss: 0.7439\n",
      "Epoch 20/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.7250 - val_loss: 0.7243\n",
      "Epoch 21/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.7050 - val_loss: 0.7052\n",
      "Epoch 22/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.6866 - val_loss: 0.6867\n",
      "Epoch 23/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.6686 - val_loss: 0.6687\n",
      "Epoch 24/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.6505 - val_loss: 0.6513\n",
      "Epoch 25/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.6330 - val_loss: 0.6343\n",
      "Epoch 26/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.6169 - val_loss: 0.6179\n",
      "Epoch 27/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.6000 - val_loss: 0.6019\n",
      "Epoch 28/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.5844 - val_loss: 0.5864\n",
      "Epoch 29/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.5694 - val_loss: 0.5713\n",
      "Epoch 30/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.5549 - val_loss: 0.5566\n",
      "Epoch 31/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.5412 - val_loss: 0.5424\n",
      "Epoch 32/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.5258 - val_loss: 0.5286\n",
      "Epoch 33/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.5124 - val_loss: 0.5153\n",
      "Epoch 34/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.4991 - val_loss: 0.5023\n",
      "Epoch 35/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.4867 - val_loss: 0.4897\n",
      "Epoch 36/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.4739 - val_loss: 0.4775\n",
      "Epoch 37/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.4620 - val_loss: 0.4656\n",
      "Epoch 38/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.4498 - val_loss: 0.4541\n",
      "Epoch 39/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.4385 - val_loss: 0.4429\n",
      "Epoch 40/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.4274 - val_loss: 0.4321\n",
      "Epoch 41/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.4169 - val_loss: 0.4216\n",
      "Epoch 42/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.4060 - val_loss: 0.4114\n",
      "Epoch 43/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.3965 - val_loss: 0.4015\n",
      "Epoch 44/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.3867 - val_loss: 0.3920\n",
      "Epoch 45/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.3770 - val_loss: 0.3827\n",
      "Epoch 46/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.3684 - val_loss: 0.3737\n",
      "Epoch 47/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.3583 - val_loss: 0.3650\n",
      "Epoch 48/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.3502 - val_loss: 0.3565\n",
      "Epoch 49/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3420 - val_loss: 0.3484\n",
      "Epoch 50/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.3343 - val_loss: 0.3404\n",
      "Final Training Loss: 0.33735528588294983\n",
      "Final Validation Loss: 0.3404273986816406\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Define hyperparameters\n",
    "dropout_rate = 0.3          # Dropout rate to help prevent overfitting by randomly dropping nodes during training.\n",
    "l2_lambda = 0.01            # L2 regularization factor, penalizes large weights and attempt to prevent overfitting.\n",
    "learning_rate = 0.0002      # Learning rate for the optimizer to control how much to adjust weights during backpropagation.\n",
    "epochs = 50                 # Maximum number of training epochs.\n",
    "batch_size = 128            # Batch size for each training step, determines how many samples are used in each gradient update.\n",
    "\n",
    "# Initialize the LSTM model.\n",
    "model = Sequential()\n",
    "\n",
    "# First LSTM layer with L2 regularization and dropout.\n",
    "model.add(LSTM(units=64,                    # Number of LSTM units (neurons) in the layer.\n",
    "               return_sequences=True,       # Ensures that the layer returns the entire sequence for the next LSTM layer.\n",
    "               input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),  # Input shape: (sequence length, number of features).\n",
    "               kernel_regularizer=l2(l2_lambda)))  # Applies L2 regularization to reduce overfitting.\n",
    "model.add(Dropout(dropout_rate))            # Adds dropout layer to reduce overfitting by randomly setting some neurons to 0.\n",
    "\n",
    "# Second LSTM layer with L2 regularization and dropout.\n",
    "model.add(LSTM(units=64,                    # Number of LSTM units in the second layer.\n",
    "               return_sequences=False,      # Last LSTM layer should not return sequences; it outputs only the final state.\n",
    "               kernel_regularizer=l2(l2_lambda)))  # L2 regularization to prevent overfitting.\n",
    "model.add(Dropout(dropout_rate))            # Dropout for regularization.\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1))                         # Dense layer with a single unit, since its predicting a single value (regression).\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate),  # Adam optimizer with a specified learning rate.\n",
    "              loss='mean_squared_error')                    # Mean Squared Error (MSE) as the loss function for regression.\n",
    "\n",
    "# Set up early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss',          # Monitor validation loss to stop if it stops improving.\n",
    "                               patience=10,                 # Wait for 10 epochs with no improvement before stopping.\n",
    "                               restore_best_weights=True)   # Restore model weights from the epoch with the lowest validation loss.\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_seq, y_train_seq,               # Training data (features and target).\n",
    "                    epochs=epochs,                          # Maximum number of epochs.\n",
    "                    batch_size=batch_size,                  # Batch size.\n",
    "                    validation_data=(X_test_seq, y_test_seq),  # Validation data to monitor overfitting.\n",
    "                    callbacks=[early_stopping],             # Callback for early stopping. \n",
    "                    shuffle=False,\n",
    "                    verbose=1)                              # Verbose output for progress during training.\n",
    "\n",
    "# Print final training and validation loss values.\n",
    "print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1bc5900-140c-4c2a-a41d-4b12929b7769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "LSTM Model Performance:\n",
      "Training RMSE: 0.020314455269155252\n",
      "Test RMSE: 0.023123872296837152\n",
      "Training MAE: 0.014739463882543874\n",
      "Test MAE: 0.01761689452726008\n",
      "Directional Accuracy on Training Data: 51.43513203214696%\n",
      "Directional Accuracy on Test Data: 52.0%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict on both training and test sequences\n",
    "y_train_pred = model.predict(X_train_seq)\n",
    "y_test_pred = model.predict(X_test_seq)\n",
    "\n",
    "# Inverse scale predictions and actual values to original scale\n",
    "y_train_pred_rescaled = scaler_y.inverse_transform(y_train_pred)\n",
    "y_test_pred_rescaled = scaler_y.inverse_transform(y_test_pred)\n",
    "y_train_actual_rescaled = scaler_y.inverse_transform(y_train_seq)\n",
    "y_test_actual_rescaled = scaler_y.inverse_transform(y_test_seq)\n",
    "\n",
    "# Define a function for directional accuracy\n",
    "def directional_accuracy(y_true, y_pred):\n",
    "    return np.mean((np.sign(y_true[1:] - y_true[:-1]) == np.sign(y_pred[1:] - y_pred[:-1])).astype(int)) * 100\n",
    "\n",
    "# Calculate metrics.\n",
    "# Root Mean Squared Error (RMSE) for training set\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_actual_rescaled, y_train_pred_rescaled))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_actual_rescaled, y_test_pred_rescaled)) # RMSE for test set.\n",
    "train_mae = mean_absolute_error(y_train_actual_rescaled, y_train_pred_rescaled) # Mean Absolute Error (MAE) for training set.\n",
    "test_mae = mean_absolute_error(y_test_actual_rescaled, y_test_pred_rescaled) # MAE for test set.\n",
    "# Directional accuracy for training set.\n",
    "train_directional_accuracy = directional_accuracy(y_train_actual_rescaled.flatten(), y_train_pred_rescaled.flatten())\n",
    "# Directional accuracy for test set.\n",
    "test_directional_accuracy = directional_accuracy(y_test_actual_rescaled.flatten(), y_test_pred_rescaled.flatten())\n",
    "\n",
    "# Display score metrics to evaluate the model's performance.\n",
    "print(\"LSTM Model Performance:\")\n",
    "print(f\"Training RMSE: {train_rmse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Training MAE: {train_mae}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Directional Accuracy on Training Data: {train_directional_accuracy}%\")\n",
    "print(f\"Directional Accuracy on Test Data: {test_directional_accuracy}%\")\n",
    "\n",
    "# Plot training & validation loss values, it will help visualize how well the model fits the data over training epochs.\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ada747e-da00-4b8b-bd2e-89e9ae916292",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51dd98d9-2643-444b-86f9-2ac515677346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 43ms/step - loss: 2.6348 - val_loss: 0.4189\n",
      "Epoch 2/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.5496 - val_loss: 0.4135\n",
      "Epoch 3/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.3137 - val_loss: 0.4124\n",
      "Epoch 4/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.1985 - val_loss: 0.4118\n",
      "Epoch 5/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.1538 - val_loss: 0.4100\n",
      "Epoch 6/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.9565 - val_loss: 0.4078\n",
      "Epoch 7/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.9465 - val_loss: 0.4046\n",
      "Epoch 8/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.8697 - val_loss: 0.4000\n",
      "Epoch 9/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.7981 - val_loss: 0.3952\n",
      "Epoch 10/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.7915 - val_loss: 0.3931\n",
      "Epoch 11/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.7333 - val_loss: 0.3885\n",
      "Epoch 12/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6823 - val_loss: 0.3821\n",
      "Epoch 13/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.5699 - val_loss: 0.3746\n",
      "Epoch 14/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.5675 - val_loss: 0.3705\n",
      "Epoch 15/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.5459 - val_loss: 0.3639\n",
      "Epoch 16/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4928 - val_loss: 0.3594\n",
      "Epoch 17/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4540 - val_loss: 0.3529\n",
      "Epoch 18/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.4326 - val_loss: 0.3502\n",
      "Epoch 19/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.4155 - val_loss: 0.3438\n",
      "Epoch 20/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3832 - val_loss: 0.3406\n",
      "Epoch 21/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3806 - val_loss: 0.3370\n",
      "Epoch 22/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3446 - val_loss: 0.3304\n",
      "Epoch 23/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3376 - val_loss: 0.3261\n",
      "Epoch 24/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3416 - val_loss: 0.3220\n",
      "Epoch 25/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3338 - val_loss: 0.3176\n",
      "Epoch 26/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3213 - val_loss: 0.3140\n",
      "Epoch 27/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3036 - val_loss: 0.3071\n",
      "Epoch 28/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2959 - val_loss: 0.3049\n",
      "Epoch 29/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2831 - val_loss: 0.2989\n",
      "Epoch 30/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2807 - val_loss: 0.2934\n",
      "Epoch 31/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2774 - val_loss: 0.2893\n",
      "Epoch 32/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2638 - val_loss: 0.2849\n",
      "Epoch 33/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2575 - val_loss: 0.2813\n",
      "Epoch 34/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2536 - val_loss: 0.2779\n",
      "Epoch 35/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2512 - val_loss: 0.2745\n",
      "Epoch 36/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2433 - val_loss: 0.2683\n",
      "Epoch 37/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2417 - val_loss: 0.2659\n",
      "Epoch 38/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2358 - val_loss: 0.2628\n",
      "Epoch 39/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2337 - val_loss: 0.2596\n",
      "Epoch 40/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2329 - val_loss: 0.2563\n",
      "Epoch 41/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2247 - val_loss: 0.2529\n",
      "Epoch 42/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2222 - val_loss: 0.2495\n",
      "Epoch 43/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2171 - val_loss: 0.2458\n",
      "Epoch 44/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2159 - val_loss: 0.2432\n",
      "Epoch 45/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2135 - val_loss: 0.2412\n",
      "Epoch 46/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2076 - val_loss: 0.2384\n",
      "Epoch 47/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2053 - val_loss: 0.2378\n",
      "Epoch 48/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2041 - val_loss: 0.2342\n",
      "Epoch 49/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2025 - val_loss: 0.2328\n",
      "Epoch 50/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1979 - val_loss: 0.2304\n",
      "Epoch 51/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1975 - val_loss: 0.2270\n",
      "Epoch 52/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1932 - val_loss: 0.2260\n",
      "Epoch 53/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1918 - val_loss: 0.2232\n",
      "Epoch 54/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1879 - val_loss: 0.2203\n",
      "Epoch 55/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1875 - val_loss: 0.2177\n",
      "Epoch 56/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1854 - val_loss: 0.2163\n",
      "Epoch 57/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1851 - val_loss: 0.2144\n",
      "Epoch 58/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1827 - val_loss: 0.2132\n",
      "Epoch 59/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1797 - val_loss: 0.2105\n",
      "Epoch 60/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1754 - val_loss: 0.2119\n",
      "Epoch 61/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1754 - val_loss: 0.2090\n",
      "Epoch 62/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1740 - val_loss: 0.2080\n",
      "Epoch 63/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1717 - val_loss: 0.2050\n",
      "Epoch 64/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1711 - val_loss: 0.2047\n",
      "Epoch 65/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1692 - val_loss: 0.2056\n",
      "Epoch 66/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1696 - val_loss: 0.2012\n",
      "Epoch 67/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1667 - val_loss: 0.2018\n",
      "Epoch 68/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1665 - val_loss: 0.1986\n",
      "Epoch 69/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1631 - val_loss: 0.1978\n",
      "Epoch 70/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1616 - val_loss: 0.1931\n",
      "Epoch 71/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1613 - val_loss: 0.1912\n",
      "Epoch 72/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1598 - val_loss: 0.1928\n",
      "Epoch 73/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1595 - val_loss: 0.1915\n",
      "Epoch 74/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1552 - val_loss: 0.1894\n",
      "Epoch 75/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1560 - val_loss: 0.1886\n",
      "Epoch 76/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1558 - val_loss: 0.1882\n",
      "Epoch 77/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1537 - val_loss: 0.1895\n",
      "Epoch 78/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1527 - val_loss: 0.1871\n",
      "Epoch 79/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1488 - val_loss: 0.1867\n",
      "Epoch 80/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1497 - val_loss: 0.1877\n",
      "Epoch 81/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1497 - val_loss: 0.1848\n",
      "Epoch 82/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1438 - val_loss: 0.1849\n",
      "Epoch 83/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1462 - val_loss: 0.1844\n",
      "Epoch 84/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1483 - val_loss: 0.1808\n",
      "Epoch 85/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1439 - val_loss: 0.1795\n",
      "Epoch 86/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1442 - val_loss: 0.1815\n",
      "Epoch 87/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1457 - val_loss: 0.1806\n",
      "Epoch 88/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1427 - val_loss: 0.1775\n",
      "Epoch 89/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1405 - val_loss: 0.1763\n",
      "Epoch 90/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1411 - val_loss: 0.1743\n",
      "Epoch 91/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1399 - val_loss: 0.1748\n",
      "Epoch 92/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1384 - val_loss: 0.1763\n",
      "Epoch 93/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1378 - val_loss: 0.1751\n",
      "Epoch 94/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1376 - val_loss: 0.1760\n",
      "Epoch 95/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1361 - val_loss: 0.1716\n",
      "Epoch 96/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1358 - val_loss: 0.1739\n",
      "Epoch 97/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1347 - val_loss: 0.1716\n",
      "Epoch 98/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1328 - val_loss: 0.1714\n",
      "Epoch 99/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1310 - val_loss: 0.1709\n",
      "Epoch 100/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1317 - val_loss: 0.1711\n",
      "Epoch 101/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1303 - val_loss: 0.1690\n",
      "Epoch 102/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1357 - val_loss: 0.1678\n",
      "Epoch 103/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1299 - val_loss: 0.1657\n",
      "Epoch 104/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1298 - val_loss: 0.1662\n",
      "Epoch 105/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1272 - val_loss: 0.1660\n",
      "Epoch 106/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1284 - val_loss: 0.1671\n",
      "Epoch 107/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1290 - val_loss: 0.1649\n",
      "Epoch 108/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1241 - val_loss: 0.1660\n",
      "Epoch 109/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1256 - val_loss: 0.1660\n",
      "Epoch 110/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1282 - val_loss: 0.1637\n",
      "Epoch 111/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1240 - val_loss: 0.1625\n",
      "Epoch 112/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1236 - val_loss: 0.1634\n",
      "Epoch 113/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1230 - val_loss: 0.1639\n",
      "Epoch 114/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1247 - val_loss: 0.1657\n",
      "Epoch 115/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1209 - val_loss: 0.1622\n",
      "Epoch 116/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1237 - val_loss: 0.1652\n",
      "Epoch 117/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1246 - val_loss: 0.1606\n",
      "Epoch 118/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1227 - val_loss: 0.1603\n",
      "Epoch 119/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1211 - val_loss: 0.1598\n",
      "Epoch 120/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1176 - val_loss: 0.1570\n",
      "Epoch 121/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1219 - val_loss: 0.1600\n",
      "Epoch 122/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1208 - val_loss: 0.1590\n",
      "Epoch 123/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1210 - val_loss: 0.1591\n",
      "Epoch 124/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1166 - val_loss: 0.1593\n",
      "Epoch 125/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1208 - val_loss: 0.1596\n",
      "Epoch 126/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1191 - val_loss: 0.1564\n",
      "Epoch 127/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1165 - val_loss: 0.1608\n",
      "Epoch 128/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1176 - val_loss: 0.1582\n",
      "Epoch 129/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1164 - val_loss: 0.1584\n",
      "Epoch 130/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1173 - val_loss: 0.1573\n",
      "Epoch 131/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1167 - val_loss: 0.1593\n",
      "Epoch 132/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1174 - val_loss: 0.1603\n",
      "Epoch 133/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1141 - val_loss: 0.1567\n",
      "Epoch 134/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1128 - val_loss: 0.1562\n",
      "Epoch 135/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1105 - val_loss: 0.1598\n",
      "Epoch 136/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1118 - val_loss: 0.1567\n",
      "Epoch 137/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1139 - val_loss: 0.1533\n",
      "Epoch 138/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1150 - val_loss: 0.1539\n",
      "Epoch 139/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1138 - val_loss: 0.1551\n",
      "Epoch 140/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1134 - val_loss: 0.1571\n",
      "Epoch 141/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1111 - val_loss: 0.1597\n",
      "Epoch 142/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1132 - val_loss: 0.1558\n",
      "Epoch 143/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1070 - val_loss: 0.1575\n",
      "Epoch 144/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1107 - val_loss: 0.1576\n",
      "Epoch 145/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1109 - val_loss: 0.1590\n",
      "Epoch 146/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1123 - val_loss: 0.1587\n",
      "Epoch 147/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1118 - val_loss: 0.1573\n",
      "Final Training Loss: 0.11889035254716873\n",
      "Final Validation Loss: 0.15729330480098724\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Best hyperparameters found from tuning\n",
    "best_params = {\n",
    "    'units1': 128,\n",
    "    'units2': 64,\n",
    "    'units3': 64,\n",
    "    'recurrent_dropout': 0.1,\n",
    "    'optimizer': 'adam',\n",
    "    'loss_function': 'huber_loss',\n",
    "    'learning_rate_decay': 1e-6,\n",
    "    'learning_rate': 0.001,\n",
    "    'l2_lambda': 0.001,\n",
    "    'epochs': 150,\n",
    "    'dropout_rate': 0.3,\n",
    "    'clipnorm': 1.0,\n",
    "    'batch_size': 64\n",
    "}\n",
    "\n",
    "# Build the model using the best parameters\n",
    "def build_best_model(params):\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units1'], return_sequences=True, \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "                   kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Second LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units2'], return_sequences=True, kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Third LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units3'], return_sequences=False, kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Select the optimizer\n",
    "    if params['optimizer'] == 'adam':\n",
    "        optimizer = Adam(learning_rate=params['learning_rate'], decay=params['learning_rate_decay'], clipnorm=params['clipnorm'])\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizer '{params['optimizer']}' is not implemented in this setup.\")\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate the model with the best hyperparameters\n",
    "model = build_best_model(best_params)\n",
    "\n",
    "# Early stopping to stop training when validation loss stops improving\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq, \n",
    "    epochs=best_params['epochs'], \n",
    "    batch_size=best_params['batch_size'], \n",
    "    validation_data=(X_test_seq, y_test_seq), \n",
    "    callbacks=[early_stopping],\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# Print the final training and validation loss\n",
    "print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b74ea1da-c56c-4b53-9b4d-8ec6d2858598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "LSTM Model Performance:\n",
      "Training RMSE: 0.019941764887832156\n",
      "Test RMSE: 0.023330304649123633\n",
      "Training MAE: 0.014341672913890324\n",
      "Test MAE: 0.01762012678185044\n",
      "Directional Accuracy on Training Data: 59.01262916188289%\n",
      "Directional Accuracy on Test Data: 52.0%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict on both training and test sequences\n",
    "y_train_pred = model.predict(X_train_seq)\n",
    "y_test_pred = model.predict(X_test_seq)\n",
    "\n",
    "# Inverse scale predictions and actual values to original scale\n",
    "y_train_pred_rescaled = scaler_y.inverse_transform(y_train_pred)\n",
    "y_test_pred_rescaled = scaler_y.inverse_transform(y_test_pred)\n",
    "y_train_actual_rescaled = scaler_y.inverse_transform(y_train_seq)\n",
    "y_test_actual_rescaled = scaler_y.inverse_transform(y_test_seq)\n",
    "\n",
    "# Define a function for directional accuracy\n",
    "def directional_accuracy(y_true, y_pred):\n",
    "    return np.mean((np.sign(y_true[1:] - y_true[:-1]) == np.sign(y_pred[1:] - y_pred[:-1])).astype(int)) * 100\n",
    "\n",
    "# Calculate metrics.\n",
    "# Root Mean Squared Error (RMSE) for training set\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_actual_rescaled, y_train_pred_rescaled))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_actual_rescaled, y_test_pred_rescaled)) # RMSE for test set.\n",
    "train_mae = mean_absolute_error(y_train_actual_rescaled, y_train_pred_rescaled) # Mean Absolute Error (MAE) for training set.\n",
    "test_mae = mean_absolute_error(y_test_actual_rescaled, y_test_pred_rescaled) # MAE for test set.\n",
    "# Directional accuracy for training set.\n",
    "train_directional_accuracy = directional_accuracy(y_train_actual_rescaled.flatten(), y_train_pred_rescaled.flatten())\n",
    "# Directional accuracy for test set.\n",
    "test_directional_accuracy = directional_accuracy(y_test_actual_rescaled.flatten(), y_test_pred_rescaled.flatten())\n",
    "\n",
    "# Display score metrics to evaluate the model's performance.\n",
    "print(\"LSTM Model Performance:\")\n",
    "print(f\"Training RMSE: {train_rmse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Training MAE: {train_mae}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Directional Accuracy on Training Data: {train_directional_accuracy}%\")\n",
    "print(f\"Directional Accuracy on Test Data: {test_directional_accuracy}%\")\n",
    "\n",
    "# Plot training & validation loss values, it will help visualize how well the model fits the data over training epochs.\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200cc922-d295-4ac7-9823-8a4d2bf892c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1.Random Search "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8bdbad-8ea1-4c8c-b49f-4cd6885ebde9",
   "metadata": {},
   "source": [
    "### Two Layers (Model 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15266065-c4f4-4e6d-a05a-3e030f256e93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running combination 1/30: {'units2': 32, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 64}\n",
      "Final Training Loss: 0.15291404724121094\n",
      "Final Validation Loss: 0.1699262112379074\n",
      "Running combination 2/30: {'units2': 128, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 0.8291551470756531\n",
      "Final Validation Loss: 0.6107314229011536\n",
      "Running combination 3/30: {'units2': 64, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 64}\n",
      "Final Training Loss: 0.9499660730361938\n",
      "Final Validation Loss: 0.3035249412059784\n",
      "Running combination 4/30: {'units2': 64, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 50, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 32}\n",
      "Final Training Loss: 5.678334712982178\n",
      "Final Validation Loss: 4.911175727844238\n",
      "Running combination 5/30: {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 5.0, 'batch_size': 64}\n",
      "Final Training Loss: 1.7656917572021484\n",
      "Final Validation Loss: 1.277626395225525\n",
      "Running combination 6/30: {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 120}\n",
      "Final Training Loss: 0.6003242135047913\n",
      "Final Validation Loss: 0.18832048773765564\n",
      "Running combination 7/30: {'units2': 32, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 64}\n",
      "Final Training Loss: 9.038475036621094\n",
      "Final Validation Loss: 8.171303749084473\n",
      "Running combination 8/30: {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 50, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 0.905655562877655\n",
      "Final Validation Loss: 0.5557719469070435\n",
      "Running combination 9/30: {'units2': 64, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 1.857315182685852\n",
      "Final Validation Loss: 1.194636344909668\n",
      "Running combination 10/30: {'units2': 32, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 0.39866209030151367\n",
      "Final Validation Loss: 0.2194484919309616\n",
      "Running combination 11/30: {'units2': 64, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 32}\n",
      "Final Training Loss: 5.268160820007324\n",
      "Final Validation Loss: 4.726815223693848\n",
      "Running combination 12/30: {'units2': 32, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 0.7644068002700806\n",
      "Final Validation Loss: 0.18393701314926147\n",
      "Running combination 13/30: {'units2': 128, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 3.587581157684326\n",
      "Final Validation Loss: 2.7618374824523926\n",
      "Running combination 14/30: {'units2': 64, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 256}\n",
      "Final Training Loss: 0.8020449876785278\n",
      "Final Validation Loss: 0.21053266525268555\n",
      "Running combination 15/30: {'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 32}\n",
      "Final Training Loss: 0.8270387649536133\n",
      "Final Validation Loss: 0.2670920789241791\n",
      "Running combination 16/30: {'units2': 32, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 256}\n",
      "Final Training Loss: 1.7866706848144531\n",
      "Final Validation Loss: 1.0330716371536255\n",
      "Running combination 17/30: {'units2': 128, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 5.0, 'batch_size': 120}\n",
      "Final Training Loss: 10.432931900024414\n",
      "Final Validation Loss: 8.957893371582031\n",
      "Running combination 18/30: {'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 200, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 13.299388885498047\n",
      "Final Validation Loss: 12.35477352142334\n",
      "Running combination 19/30: {'units2': 128, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 18.670883178710938\n",
      "Final Validation Loss: 17.875080108642578\n",
      "Running combination 20/30: {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 1.574950098991394\n",
      "Final Validation Loss: 0.7998050451278687\n",
      "Running combination 21/30: {'units2': 32, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.001, 'l2_lambda': 0.01, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 0.4804726541042328\n",
      "Final Validation Loss: 0.3525100350379944\n",
      "Running combination 22/30: {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.01, 'epochs': 50, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 1.1596918106079102\n",
      "Final Validation Loss: 0.601171612739563\n",
      "Running combination 23/30: {'units2': 32, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 50, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 6.147163391113281\n",
      "Final Validation Loss: 4.975543022155762\n",
      "Running combination 24/30: {'units2': 64, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 2.144038677215576\n",
      "Final Validation Loss: 1.6925166845321655\n",
      "Running combination 25/30: {'units2': 32, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 256}\n",
      "Final Training Loss: 13.995078086853027\n",
      "Final Validation Loss: 13.070289611816406\n",
      "Running combination 26/30: {'units2': 32, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 64}\n",
      "Final Training Loss: 1.4441250562667847\n",
      "Final Validation Loss: 0.6569451689720154\n",
      "Running combination 27/30: {'units2': 64, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 1.6525062322616577\n",
      "Final Validation Loss: 0.23159344494342804\n",
      "Running combination 28/30: {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 0.4147290587425232\n",
      "Final Validation Loss: 0.201966792345047\n",
      "Running combination 29/30: {'units2': 128, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0001, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 256}\n",
      "Final Training Loss: 0.9967998266220093\n",
      "Final Validation Loss: 0.2880844175815582\n",
      "Running combination 30/30: {'units2': 32, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 200, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 64}\n",
      "Final Training Loss: 4.384966850280762\n",
      "Final Validation Loss: 3.8641197681427\n",
      "Top results:\n",
      "{'params': {'units2': 32, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 64}, 'final_train_loss': 0.15291404724121094, 'final_val_loss': 0.1699262112379074}\n",
      "{'params': {'units2': 32, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 120}, 'final_train_loss': 0.7644068002700806, 'final_val_loss': 0.18393701314926147}\n",
      "{'params': {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 120}, 'final_train_loss': 0.6003242135047913, 'final_val_loss': 0.18832048773765564}\n",
      "{'params': {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 32}, 'final_train_loss': 0.4147290587425232, 'final_val_loss': 0.201966792345047}\n",
      "{'params': {'units2': 64, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 256}, 'final_train_loss': 0.8020449876785278, 'final_val_loss': 0.21053266525268555}\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Define parameter grid for random hyperparameter search.\n",
    "param_grid = {\n",
    "    'dropout_rate': [0.2, 0.3, 0.4],               # Dropout rate for regularization.\n",
    "    'recurrent_dropout': [0.1, 0.2],               # Recurrent dropout within LSTM layers.\n",
    "    'l2_lambda': [0.001, 0.01, 0.1],               # L2 regularization strength.\n",
    "    'learning_rate': [0.001, 0.0005, 0.0001],      # Learning rate for optimizer.\n",
    "    'learning_rate_decay': [1e-6, 1e-5, 0],        # Decay rate for learning rate over time.\n",
    "    'units1': [32, 64, 128],                       # Number of units in the first LSTM layer.\n",
    "    'units2': [32, 64, 128],                       # Number of units in the second LSTM layer.\n",
    "    'batch_size': [32, 64, 120, 256],              # Batch size for training.\n",
    "    'epochs': [50, 100, 200],                      # Number of epochs to train.\n",
    "    'optimizer': ['adam'],                         # Optimizer to use.\n",
    "    'clipnorm': [1.0, 5.0]                         # Gradient clipping to avoid exploding gradients.\n",
    "}\n",
    "\n",
    "# Generate a list of random combinations of hyperparameters.\n",
    "n_iter_search = 30  # Number of random combinations to attempt.\n",
    "# Random state being applied for reproducibility. \n",
    "random_combinations = list(ParameterSampler(param_grid, n_iter=n_iter_search, random_state=42)) \n",
    "\n",
    "# Define a function to build the LSTM model with given parameters.\n",
    "def build_model(dropout_rate, recurrent_dropout, l2_lambda, learning_rate, learning_rate_decay, clipnorm, units1, units2, optimizer_name, loss_function):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First LSTM layer with BatchNorm, Dropout, and recurrent_dropout.\n",
    "    model.add(LSTM(units=units1, return_sequences=True, \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),  # Input shape based on sequence data.\n",
    "                   kernel_regularizer=l2(l2_lambda), recurrent_dropout=recurrent_dropout))\n",
    "    model.add(BatchNormalization())  # Batch normalization for stable training.\n",
    "    model.add(Dropout(dropout_rate))  # Dropout for regularization.\n",
    "    \n",
    "    # Second LSTM layer with BatchNorm, Dropout, and recurrent_dropout.\n",
    "    model.add(LSTM(units=units2, return_sequences=False, kernel_regularizer=l2(l2_lambda), recurrent_dropout=recurrent_dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer with a single unit for regression output.\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Select optimizer with learning rate decay and gradient clipping.\n",
    "    optimizer = Adam(learning_rate=learning_rate, decay=learning_rate_decay, clipnorm=clipnorm)\n",
    "    \n",
    "    # Compile the model with the chosen optimizer and loss function.\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define early stopping to stop training when validation loss does not improve.\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Placeholder to store results of each model configuration.\n",
    "results = []\n",
    "\n",
    "# Run random search\n",
    "for i, params in enumerate(random_combinations):\n",
    "    print(f\"Running combination {i+1}/{len(random_combinations)}: {params}\")\n",
    "    \n",
    "    # Build model with the current parameters\n",
    "    model = build_model(\n",
    "        dropout_rate=params['dropout_rate'],\n",
    "        recurrent_dropout=params['recurrent_dropout'],\n",
    "        l2_lambda=params['l2_lambda'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        learning_rate_decay=params['learning_rate_decay'],\n",
    "        clipnorm=params['clipnorm'],\n",
    "        units1=params['units1'],\n",
    "        units2=params['units2'],\n",
    "        optimizer_name=params['optimizer'],\n",
    "        loss_function='mean_squared_error'\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq, \n",
    "        epochs=params['epochs'], \n",
    "        batch_size=params['batch_size'], \n",
    "        validation_data=(X_test_seq, y_test_seq), \n",
    "        callbacks=[early_stopping],\n",
    "        shuffle=False,\n",
    "        verbose=0  # Set verbose=0 to reduce output\n",
    "    )\n",
    "    \n",
    "    # Get final validation loss and training loss\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    \n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'params': params,\n",
    "        'final_train_loss': final_train_loss,\n",
    "        'final_val_loss': final_val_loss\n",
    "    })\n",
    "    \n",
    "    print(f\"Final Training Loss: {final_train_loss}\")\n",
    "    print(f\"Final Validation Loss: {final_val_loss}\")\n",
    "\n",
    "results = sorted(results, key=lambda x: x['final_val_loss'])\n",
    "print(\"Top results:\")\n",
    "for res in results[:5]:  # Show top 5 results\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1ab8964-d227-4e34-81f5-28849a43c67c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 1.6953 - val_loss: 0.2133\n",
      "Epoch 2/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.3776 - val_loss: 0.2094\n",
      "Epoch 3/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0675 - val_loss: 0.2084\n",
      "Epoch 4/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0660 - val_loss: 0.2084\n",
      "Epoch 5/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9442 - val_loss: 0.2089\n",
      "Epoch 6/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7416 - val_loss: 0.2072\n",
      "Epoch 7/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7200 - val_loss: 0.2068\n",
      "Epoch 8/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6494 - val_loss: 0.2062\n",
      "Epoch 9/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4691 - val_loss: 0.2035\n",
      "Epoch 10/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4322 - val_loss: 0.2018\n",
      "Epoch 11/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3582 - val_loss: 0.2014\n",
      "Epoch 12/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3538 - val_loss: 0.1981\n",
      "Epoch 13/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2928 - val_loss: 0.1966\n",
      "Epoch 14/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2650 - val_loss: 0.1934\n",
      "Epoch 15/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2296 - val_loss: 0.1938\n",
      "Epoch 16/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2054 - val_loss: 0.1914\n",
      "Epoch 17/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1848 - val_loss: 0.1867\n",
      "Epoch 18/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1691 - val_loss: 0.1924\n",
      "Epoch 19/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1599 - val_loss: 0.1869\n",
      "Epoch 20/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1521 - val_loss: 0.1911\n",
      "Epoch 21/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1426 - val_loss: 0.1799\n",
      "Epoch 22/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1427 - val_loss: 0.1769\n",
      "Epoch 23/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1386 - val_loss: 0.1765\n",
      "Epoch 24/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1368 - val_loss: 0.1839\n",
      "Epoch 25/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1361 - val_loss: 0.1831\n",
      "Epoch 26/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1321 - val_loss: 0.1739\n",
      "Epoch 27/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1331 - val_loss: 0.1721\n",
      "Epoch 28/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1278 - val_loss: 0.1695\n",
      "Epoch 29/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1301 - val_loss: 0.1683\n",
      "Epoch 30/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1256 - val_loss: 0.1683\n",
      "Epoch 31/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1255 - val_loss: 0.1676\n",
      "Epoch 32/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1253 - val_loss: 0.1655\n",
      "Epoch 33/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1219 - val_loss: 0.1652\n",
      "Epoch 34/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1214 - val_loss: 0.1642\n",
      "Epoch 35/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1215 - val_loss: 0.1640\n",
      "Epoch 36/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1260 - val_loss: 0.1663\n",
      "Epoch 37/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1253 - val_loss: 0.1611\n",
      "Epoch 38/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1172 - val_loss: 0.1663\n",
      "Epoch 39/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1213 - val_loss: 0.1602\n",
      "Epoch 40/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1183 - val_loss: 0.1609\n",
      "Epoch 41/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1188 - val_loss: 0.1650\n",
      "Epoch 42/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1172 - val_loss: 0.1691\n",
      "Epoch 43/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1176 - val_loss: 0.1602\n",
      "Epoch 44/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1160 - val_loss: 0.1572\n",
      "Epoch 45/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1152 - val_loss: 0.1572\n",
      "Epoch 46/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1155 - val_loss: 0.1565\n",
      "Epoch 47/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1137 - val_loss: 0.1564\n",
      "Epoch 48/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1159 - val_loss: 0.1610\n",
      "Epoch 49/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1114 - val_loss: 0.1644\n",
      "Epoch 50/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1145 - val_loss: 0.1564\n",
      "Epoch 51/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1130 - val_loss: 0.1548\n",
      "Epoch 52/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1126 - val_loss: 0.1545\n",
      "Epoch 53/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1101 - val_loss: 0.1569\n",
      "Epoch 54/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1112 - val_loss: 0.1551\n",
      "Epoch 55/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1151 - val_loss: 0.1533\n",
      "Epoch 56/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1113 - val_loss: 0.1558\n",
      "Epoch 57/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1100 - val_loss: 0.1591\n",
      "Epoch 58/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1125 - val_loss: 0.1571\n",
      "Epoch 59/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1111 - val_loss: 0.1518\n",
      "Epoch 60/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1095 - val_loss: 0.1542\n",
      "Epoch 61/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1084 - val_loss: 0.1552\n",
      "Epoch 62/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1095 - val_loss: 0.1554\n",
      "Epoch 63/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1112 - val_loss: 0.1661\n",
      "Epoch 64/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1100 - val_loss: 0.1549\n",
      "Epoch 65/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1076 - val_loss: 0.1590\n",
      "Epoch 66/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1084 - val_loss: 0.1563\n",
      "Epoch 67/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1107 - val_loss: 0.1514\n",
      "Epoch 68/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1096 - val_loss: 0.1513\n",
      "Epoch 69/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1064 - val_loss: 0.1551\n",
      "Epoch 70/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1070 - val_loss: 0.1517\n",
      "Epoch 71/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1076 - val_loss: 0.1503\n",
      "Epoch 72/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1060 - val_loss: 0.1494\n",
      "Epoch 73/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1046 - val_loss: 0.1557\n",
      "Epoch 74/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1041 - val_loss: 0.1567\n",
      "Epoch 75/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1060 - val_loss: 0.1541\n",
      "Epoch 76/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1082 - val_loss: 0.1504\n",
      "Epoch 77/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1070 - val_loss: 0.1491\n",
      "Epoch 78/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1046 - val_loss: 0.1537\n",
      "Epoch 79/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1064 - val_loss: 0.1560\n",
      "Epoch 80/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1064 - val_loss: 0.1522\n",
      "Epoch 81/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1064 - val_loss: 0.1530\n",
      "Epoch 82/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1025 - val_loss: 0.1549\n",
      "Epoch 83/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1051 - val_loss: 0.1503\n",
      "Epoch 84/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1056 - val_loss: 0.1562\n",
      "Epoch 85/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1043 - val_loss: 0.1524\n",
      "Epoch 86/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1057 - val_loss: 0.1490\n",
      "Epoch 87/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1055 - val_loss: 0.1479\n",
      "Epoch 88/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1060 - val_loss: 0.1459\n",
      "Epoch 89/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1040 - val_loss: 0.1473\n",
      "Epoch 90/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1030 - val_loss: 0.1501\n",
      "Epoch 91/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1075 - val_loss: 0.1517\n",
      "Epoch 92/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1061 - val_loss: 0.1538\n",
      "Epoch 93/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1024 - val_loss: 0.1478\n",
      "Epoch 94/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1054 - val_loss: 0.1476\n",
      "Epoch 95/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1052 - val_loss: 0.1524\n",
      "Epoch 96/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1029 - val_loss: 0.1472\n",
      "Epoch 97/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1058 - val_loss: 0.1498\n",
      "Epoch 98/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1014 - val_loss: 0.1593\n",
      "Final Training Loss: 0.10988687723875046\n",
      "Final Validation Loss: 0.15930011868476868\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Best hyperparameters found from tuning\n",
    "best_params = {\n",
    "    'units1': 32,\n",
    "    'units2': 128,\n",
    "    'recurrent_dropout': 0.1,\n",
    "    'optimizer': 'adam',\n",
    "    'loss_function': 'mean_squared_error',\n",
    "    'learning_rate_decay': 1e-06,\n",
    "    'learning_rate': 0.001,\n",
    "    'l2_lambda': 0.001,\n",
    "    'epochs': 200,\n",
    "    'dropout_rate': 0.4,\n",
    "    'clipnorm': 1.0,\n",
    "    'batch_size': 32\n",
    "}\n",
    "\n",
    "# Build the model using the best parameters\n",
    "def build_best_model(params):\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units1'], return_sequences=True, \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "                   kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Second LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units2'], return_sequences=False, kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Select the optimizer\n",
    "    if params['optimizer'] == 'adam':\n",
    "        optimizer = Adam(learning_rate=params['learning_rate'], decay=params['learning_rate_decay'], clipnorm=params['clipnorm'])\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizer '{params['optimizer']}' is not implemented in this setup.\")\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate the model with the best hyperparameters\n",
    "model = build_best_model(best_params)\n",
    "\n",
    "# Early stopping to stop training when validation loss stops improving\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq, \n",
    "    epochs=best_params['epochs'], \n",
    "    batch_size=best_params['batch_size'], \n",
    "    validation_data=(X_test_seq, y_test_seq),\n",
    "    shuffle=False,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Print the final training and validation loss\n",
    "print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f2ddcbc-91d3-4b6b-9e83-60dfc6c0f4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "LSTM Model Performance:\n",
      "Training RMSE: 0.02038309010392759\n",
      "Test RMSE: 0.022973741159911368\n",
      "Training MAE: 0.014790372016464057\n",
      "Test MAE: 0.017618449356682518\n",
      "Directional Accuracy on Training Data: 61.882893226176805%\n",
      "Directional Accuracy on Test Data: 51.33333333333333%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict on both training and test sequences\n",
    "y_train_pred = model.predict(X_train_seq)\n",
    "y_test_pred = model.predict(X_test_seq)\n",
    "\n",
    "# Inverse scale predictions and actual values to original scale\n",
    "y_train_pred_rescaled = scaler_y.inverse_transform(y_train_pred)\n",
    "y_test_pred_rescaled = scaler_y.inverse_transform(y_test_pred)\n",
    "y_train_actual_rescaled = scaler_y.inverse_transform(y_train_seq)\n",
    "y_test_actual_rescaled = scaler_y.inverse_transform(y_test_seq)\n",
    "\n",
    "# Define a function for directional accuracy\n",
    "def directional_accuracy(y_true, y_pred):\n",
    "    return np.mean((np.sign(y_true[1:] - y_true[:-1]) == np.sign(y_pred[1:] - y_pred[:-1])).astype(int)) * 100\n",
    "\n",
    "# Calculate metrics.\n",
    "# Root Mean Squared Error (RMSE) for training set\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_actual_rescaled, y_train_pred_rescaled))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_actual_rescaled, y_test_pred_rescaled)) # RMSE for test set.\n",
    "train_mae = mean_absolute_error(y_train_actual_rescaled, y_train_pred_rescaled) # Mean Absolute Error (MAE) for training set.\n",
    "test_mae = mean_absolute_error(y_test_actual_rescaled, y_test_pred_rescaled) # MAE for test set.\n",
    "# Directional accuracy for training set.\n",
    "train_directional_accuracy = directional_accuracy(y_train_actual_rescaled.flatten(), y_train_pred_rescaled.flatten())\n",
    "# Directional accuracy for test set.\n",
    "test_directional_accuracy = directional_accuracy(y_test_actual_rescaled.flatten(), y_test_pred_rescaled.flatten())\n",
    "\n",
    "# Display score metrics to evaluate the model's performance.\n",
    "print(\"LSTM Model Performance:\")\n",
    "print(f\"Training RMSE: {train_rmse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Training MAE: {train_mae}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Directional Accuracy on Training Data: {train_directional_accuracy}%\")\n",
    "print(f\"Directional Accuracy on Test Data: {test_directional_accuracy}%\")\n",
    "\n",
    "# Plot training & validation loss values, it will help visualize how well the model fits the data over training epochs.\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a07da9e-1971-48e6-8599-31c6e4f2c6bc",
   "metadata": {},
   "source": [
    "### Three Layers (Model 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26f70769-d9de-4baa-a9d8-8ac915710f7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running combination 1/30: {'units3': 32, 'units2': 32, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 32}\n",
      "Final Training Loss: 0.11058776825666428\n",
      "Final Validation Loss: 0.1438879519701004\n",
      "Running combination 2/30: {'units3': 128, 'units2': 64, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 3.23075532913208\n",
      "Final Validation Loss: 2.73872709274292\n",
      "Running combination 3/30: {'units3': 128, 'units2': 64, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.4, 'clipnorm': 5.0, 'batch_size': 256}\n",
      "Final Training Loss: 1.627591848373413\n",
      "Final Validation Loss: 0.37090885639190674\n",
      "Running combination 4/30: {'units3': 128, 'units2': 64, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 5.0, 'batch_size': 120}\n",
      "Final Training Loss: 6.560494422912598\n",
      "Final Validation Loss: 5.464375972747803\n",
      "Running combination 5/30: {'units3': 64, 'units2': 64, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 3.4853694438934326\n",
      "Final Validation Loss: 2.8658432960510254\n",
      "Running combination 6/30: {'units3': 64, 'units2': 32, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 1.4089709520339966\n",
      "Final Validation Loss: 0.256446897983551\n",
      "Running combination 7/30: {'units3': 32, 'units2': 32, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 256}\n",
      "Final Training Loss: 2.054506778717041\n",
      "Final Validation Loss: 1.4922308921813965\n",
      "Running combination 8/30: {'units3': 32, 'units2': 128, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 64}\n",
      "Final Training Loss: 3.5342254638671875\n",
      "Final Validation Loss: 2.344458818435669\n",
      "Running combination 9/30: {'units3': 32, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 256}\n",
      "Final Training Loss: 1.5361895561218262\n",
      "Final Validation Loss: 0.3309127688407898\n",
      "Running combination 10/30: {'units3': 64, 'units2': 64, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0001, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 64}\n",
      "Final Training Loss: 1.5003982782363892\n",
      "Final Validation Loss: 0.35477396845817566\n",
      "Running combination 11/30: {'units3': 128, 'units2': 128, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 1.5954704284667969\n",
      "Final Validation Loss: 0.5581293106079102\n",
      "Running combination 12/30: {'units3': 32, 'units2': 64, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 32}\n",
      "Final Training Loss: 14.881759643554688\n",
      "Final Validation Loss: 13.96423053741455\n",
      "Running combination 13/30: {'units3': 128, 'units2': 64, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 64}\n",
      "Final Training Loss: 0.7721856832504272\n",
      "Final Validation Loss: 0.41575634479522705\n",
      "Running combination 14/30: {'units3': 32, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 120}\n",
      "Final Training Loss: 0.5752654075622559\n",
      "Final Validation Loss: 0.3013242781162262\n",
      "Running combination 15/30: {'units3': 128, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 3.0224900245666504\n",
      "Final Validation Loss: 1.8832988739013672\n",
      "Running combination 16/30: {'units3': 64, 'units2': 64, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 1.7875701189041138\n",
      "Final Validation Loss: 1.4252582788467407\n",
      "Running combination 17/30: {'units3': 128, 'units2': 32, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 14.681894302368164\n",
      "Final Validation Loss: 13.85850715637207\n",
      "Running combination 18/30: {'units3': 32, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 1.9561998844146729\n",
      "Final Validation Loss: 0.30938220024108887\n",
      "Running combination 19/30: {'units3': 128, 'units2': 128, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 4.074248313903809\n",
      "Final Validation Loss: 2.995677947998047\n",
      "Running combination 20/30: {'units3': 64, 'units2': 32, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 120}\n",
      "Final Training Loss: 2.6241652965545654\n",
      "Final Validation Loss: 1.6391658782958984\n",
      "Running combination 21/30: {'units3': 64, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 50, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 2.8080079555511475\n",
      "Final Validation Loss: 1.9738152027130127\n",
      "Running combination 22/30: {'units3': 128, 'units2': 128, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 5.0, 'batch_size': 256}\n",
      "Final Training Loss: 30.652591705322266\n",
      "Final Validation Loss: 28.273338317871094\n",
      "Running combination 23/30: {'units3': 32, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 200, 'dropout_rate': 0.4, 'clipnorm': 5.0, 'batch_size': 256}\n",
      "Final Training Loss: 16.83570098876953\n",
      "Final Validation Loss: 14.911806106567383\n",
      "Running combination 24/30: {'units3': 64, 'units2': 128, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 200, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 256}\n",
      "Final Training Loss: 36.630489349365234\n",
      "Final Validation Loss: 35.5998420715332\n",
      "Running combination 25/30: {'units3': 64, 'units2': 64, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 10.343450546264648\n",
      "Final Validation Loss: 9.370462417602539\n",
      "Running combination 26/30: {'units3': 32, 'units2': 64, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 100, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 256}\n",
      "Final Training Loss: 3.0597174167633057\n",
      "Final Validation Loss: 1.991112232208252\n",
      "Running combination 27/30: {'units3': 64, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 64}\n",
      "Final Training Loss: 1.9874039888381958\n",
      "Final Validation Loss: 1.5517362356185913\n",
      "Running combination 28/30: {'units3': 64, 'units2': 64, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.01, 'epochs': 100, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 120}\n",
      "Final Training Loss: 2.093331813812256\n",
      "Final Validation Loss: 1.4691298007965088\n",
      "Running combination 29/30: {'units3': 64, 'units2': 32, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 256}\n",
      "Final Training Loss: 10.19898796081543\n",
      "Final Validation Loss: 9.47281265258789\n",
      "Running combination 30/30: {'units3': 128, 'units2': 128, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.4, 'clipnorm': 5.0, 'batch_size': 32}\n",
      "Final Training Loss: 4.158230304718018\n",
      "Final Validation Loss: 2.7446656227111816\n",
      "Top results:\n",
      "{'params': {'units3': 32, 'units2': 32, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 32}, 'final_train_loss': 0.11058776825666428, 'final_val_loss': 0.1438879519701004}\n",
      "{'params': {'units3': 64, 'units2': 32, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 32}, 'final_train_loss': 1.4089709520339966, 'final_val_loss': 0.256446897983551}\n",
      "{'params': {'units3': 32, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 120}, 'final_train_loss': 0.5752654075622559, 'final_val_loss': 0.3013242781162262}\n",
      "{'params': {'units3': 32, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 120}, 'final_train_loss': 1.9561998844146729, 'final_val_loss': 0.30938220024108887}\n",
      "{'params': {'units3': 32, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 256}, 'final_train_loss': 1.5361895561218262, 'final_val_loss': 0.3309127688407898}\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'dropout_rate': [0.2, 0.3, 0.4],\n",
    "    'recurrent_dropout': [0.1, 0.2],\n",
    "    'l2_lambda': [0.001, 0.01, 0.1],\n",
    "    'learning_rate': [0.001, 0.0005, 0.0001],\n",
    "    'learning_rate_decay': [1e-6, 1e-5, 0],\n",
    "    'units1': [32, 64, 128],\n",
    "    'units2': [32, 64, 128],\n",
    "    'units3': [32, 64, 128],\n",
    "    'batch_size': [32, 64, 120, 256],  \n",
    "    'epochs': [50, 100, 200],\n",
    "    'optimizer': ['adam'],\n",
    "    'clipnorm': [1.0, 5.0]\n",
    "}\n",
    "\n",
    "# Generate a list of random combinations with ParameterSampler\n",
    "n_iter_search = 30\n",
    "random_combinations = list(ParameterSampler(param_grid, n_iter=n_iter_search, random_state=42))\n",
    "\n",
    "# Define the function to build the model with variable parameters\n",
    "def build_model(dropout_rate, recurrent_dropout, l2_lambda, learning_rate, learning_rate_decay, clipnorm, units1, units2, units3, optimizer_name, loss_function):\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=units1, return_sequences=True, \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "                   kernel_regularizer=l2(l2_lambda), recurrent_dropout=recurrent_dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Second LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=units2, return_sequences=True, kernel_regularizer=l2(l2_lambda), recurrent_dropout=recurrent_dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Third LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=units3, return_sequences=False, kernel_regularizer=l2(l2_lambda), recurrent_dropout=recurrent_dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer.\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Select optimizer with decay and clipping.\n",
    "    optimizer = Adam(learning_rate=learning_rate, decay=learning_rate_decay, clipnorm=clipnorm)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Placeholder to keep track of results\n",
    "results = []\n",
    "\n",
    "# Run random search\n",
    "for i, params in enumerate(random_combinations):\n",
    "    print(f\"Running combination {i+1}/{len(random_combinations)}: {params}\")\n",
    "    \n",
    "    # Build model with the current parameters\n",
    "    model = build_model(\n",
    "        dropout_rate=params['dropout_rate'],\n",
    "        recurrent_dropout=params['recurrent_dropout'],\n",
    "        l2_lambda=params['l2_lambda'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        learning_rate_decay=params['learning_rate_decay'],\n",
    "        clipnorm=params['clipnorm'],\n",
    "        units1=params['units1'],\n",
    "        units2=params['units2'],\n",
    "        units3=params['units3'],\n",
    "        optimizer_name=params['optimizer'],\n",
    "        loss_function='mean_squared_error'\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq, \n",
    "        epochs=params['epochs'], \n",
    "        batch_size=params['batch_size'], \n",
    "        validation_data=(X_test_seq, y_test_seq), \n",
    "        callbacks=[early_stopping],\n",
    "        shuffle=False,\n",
    "        verbose=0  # Set verbose=0 to reduce output\n",
    "    )\n",
    "    \n",
    "    # Get final validation loss and training loss\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    \n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'params': params,\n",
    "        'final_train_loss': final_train_loss,\n",
    "        'final_val_loss': final_val_loss\n",
    "    })\n",
    "    \n",
    "    print(f\"Final Training Loss: {final_train_loss}\")\n",
    "    print(f\"Final Validation Loss: {final_val_loss}\")\n",
    "\n",
    "results = sorted(results, key=lambda x: x['final_val_loss'])\n",
    "print(\"Top results:\")\n",
    "for res in results[:5]:  # Show top 5 results\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e3480ec-9c0f-4951-b2be-ca37f4c86204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 31ms/step - loss: 13.4038 - val_loss: 10.3586\n",
      "Epoch 2/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 11.0829 - val_loss: 9.2126\n",
      "Epoch 3/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 9.7346 - val_loss: 8.1123\n",
      "Epoch 4/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.5419 - val_loss: 7.1304\n",
      "Epoch 5/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.6010 - val_loss: 6.2975\n",
      "Epoch 6/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.6380 - val_loss: 5.5659\n",
      "Epoch 7/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.9677 - val_loss: 4.9359\n",
      "Epoch 8/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.2437 - val_loss: 4.3920\n",
      "Epoch 9/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.6489 - val_loss: 3.9295\n",
      "Epoch 10/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.2564 - val_loss: 3.5312\n",
      "Epoch 11/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.9122 - val_loss: 3.1854\n",
      "Epoch 12/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.4406 - val_loss: 2.8898\n",
      "Epoch 13/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.1878 - val_loss: 2.6297\n",
      "Epoch 14/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.8630 - val_loss: 2.4022\n",
      "Epoch 15/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6728 - val_loss: 2.2113\n",
      "Epoch 16/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.4827 - val_loss: 2.0383\n",
      "Epoch 17/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2873 - val_loss: 1.8914\n",
      "Epoch 18/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.0990 - val_loss: 1.7504\n",
      "Epoch 19/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.9661 - val_loss: 1.6341\n",
      "Epoch 20/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.8303 - val_loss: 1.5298\n",
      "Epoch 21/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.7229 - val_loss: 1.4460\n",
      "Epoch 22/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.6381 - val_loss: 1.3519\n",
      "Epoch 23/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.5413 - val_loss: 1.2718\n",
      "Epoch 24/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.4359 - val_loss: 1.1979\n",
      "Epoch 25/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.4103 - val_loss: 1.1559\n",
      "Epoch 26/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.2853 - val_loss: 1.1016\n",
      "Epoch 27/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.2232 - val_loss: 1.0246\n",
      "Epoch 28/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1575 - val_loss: 0.9955\n",
      "Epoch 29/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0866 - val_loss: 0.9153\n",
      "Epoch 30/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.0267 - val_loss: 0.8744\n",
      "Epoch 31/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9921 - val_loss: 0.8233\n",
      "Epoch 32/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9446 - val_loss: 0.8055\n",
      "Epoch 33/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9501 - val_loss: 0.7795\n",
      "Epoch 34/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.8736 - val_loss: 0.7084\n",
      "Epoch 35/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7985 - val_loss: 0.6752\n",
      "Epoch 36/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.7521 - val_loss: 0.6453\n",
      "Epoch 37/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7267 - val_loss: 0.6412\n",
      "Epoch 38/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6972 - val_loss: 0.6053\n",
      "Epoch 39/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6553 - val_loss: 0.6011\n",
      "Epoch 40/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6063 - val_loss: 0.5602\n",
      "Epoch 41/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.6015 - val_loss: 0.5124\n",
      "Epoch 42/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6014 - val_loss: 0.4959\n",
      "Epoch 43/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5615 - val_loss: 0.4783\n",
      "Epoch 44/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.4883 - val_loss: 0.4518\n",
      "Epoch 45/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.4802 - val_loss: 0.4297\n",
      "Epoch 46/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.4702 - val_loss: 0.4197\n",
      "Epoch 47/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.4342 - val_loss: 0.3987\n",
      "Epoch 48/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.4130 - val_loss: 0.3883\n",
      "Epoch 49/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.4088 - val_loss: 0.3689\n",
      "Epoch 50/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3708 - val_loss: 0.3457\n",
      "Epoch 51/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3680 - val_loss: 0.3405\n",
      "Epoch 52/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3374 - val_loss: 0.3198\n",
      "Epoch 53/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3263 - val_loss: 0.3073\n",
      "Epoch 54/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3135 - val_loss: 0.2987\n",
      "Epoch 55/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2851 - val_loss: 0.2931\n",
      "Epoch 56/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2716 - val_loss: 0.2892\n",
      "Epoch 57/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2558 - val_loss: 0.2644\n",
      "Epoch 58/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2370 - val_loss: 0.2559\n",
      "Epoch 59/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2176 - val_loss: 0.2740\n",
      "Epoch 60/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2158 - val_loss: 0.2557\n",
      "Epoch 61/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2064 - val_loss: 0.2331\n",
      "Epoch 62/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1977 - val_loss: 0.2228\n",
      "Epoch 63/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1919 - val_loss: 0.2184\n",
      "Epoch 64/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1802 - val_loss: 0.2189\n",
      "Epoch 65/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1733 - val_loss: 0.2085\n",
      "Epoch 66/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1680 - val_loss: 0.2118\n",
      "Epoch 67/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1634 - val_loss: 0.2039\n",
      "Epoch 68/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1592 - val_loss: 0.1926\n",
      "Epoch 69/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1545 - val_loss: 0.1891\n",
      "Epoch 70/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1503 - val_loss: 0.1875\n",
      "Epoch 71/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1456 - val_loss: 0.1837\n",
      "Epoch 72/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1430 - val_loss: 0.1793\n",
      "Epoch 73/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1377 - val_loss: 0.1789\n",
      "Epoch 74/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1356 - val_loss: 0.1742\n",
      "Epoch 75/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1335 - val_loss: 0.1719\n",
      "Epoch 76/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1303 - val_loss: 0.1695\n",
      "Epoch 77/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1274 - val_loss: 0.1666\n",
      "Epoch 78/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1237 - val_loss: 0.1646\n",
      "Epoch 79/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1232 - val_loss: 0.1633\n",
      "Epoch 80/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1229 - val_loss: 0.1618\n",
      "Epoch 81/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1199 - val_loss: 0.1593\n",
      "Epoch 82/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1186 - val_loss: 0.1577\n",
      "Epoch 83/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1160 - val_loss: 0.1572\n",
      "Epoch 84/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1150 - val_loss: 0.1567\n",
      "Epoch 85/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1149 - val_loss: 0.1543\n",
      "Epoch 86/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1137 - val_loss: 0.1525\n",
      "Epoch 87/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1111 - val_loss: 0.1520\n",
      "Epoch 88/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1114 - val_loss: 0.1518\n",
      "Epoch 89/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1095 - val_loss: 0.1504\n",
      "Epoch 90/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1089 - val_loss: 0.1493\n",
      "Epoch 91/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1079 - val_loss: 0.1486\n",
      "Epoch 92/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1079 - val_loss: 0.1476\n",
      "Epoch 93/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1065 - val_loss: 0.1478\n",
      "Epoch 94/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1070 - val_loss: 0.1471\n",
      "Epoch 95/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1051 - val_loss: 0.1474\n",
      "Epoch 96/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1058 - val_loss: 0.1475\n",
      "Epoch 97/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1048 - val_loss: 0.1462\n",
      "Epoch 98/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1041 - val_loss: 0.1455\n",
      "Epoch 99/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1044 - val_loss: 0.1441\n",
      "Epoch 100/100\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1037 - val_loss: 0.1439\n",
      "Final Training Loss: 0.11058776825666428\n",
      "Final Validation Loss: 0.1438879519701004\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Best hyperparameters found from tuning\n",
    "best_params = {\n",
    "    'units1': 32,\n",
    "    'units2': 32,\n",
    "    'units3': 32,\n",
    "    'recurrent_dropout': 0.2,\n",
    "    'optimizer': 'adam',\n",
    "    'loss_function': 'mean_squared_error',\n",
    "    'learning_rate_decay': 1e-05,\n",
    "    'learning_rate': 0.0005,\n",
    "    'l2_lambda': 0.1,\n",
    "    'epochs': 100,\n",
    "    'dropout_rate': 0.2,\n",
    "    'clipnorm': 5.0,\n",
    "    'batch_size': 32\n",
    "}\n",
    "\n",
    "# Build the model using the best parameters\n",
    "def build_best_model(params):\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units1'], return_sequences=True, \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "                   kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Second LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units2'], return_sequences=True, kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Third LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units3'], return_sequences=False, kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Select the optimizer\n",
    "    if params['optimizer'] == 'adam':\n",
    "        optimizer = Adam(learning_rate=params['learning_rate'], decay=params['learning_rate_decay'], clipnorm=params['clipnorm'])\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizer '{params['optimizer']}' is not implemented in this setup.\")\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate the model with the best hyperparameters\n",
    "model = build_best_model(best_params)\n",
    "\n",
    "# Early stopping to stop training when validation loss stops improving\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq, \n",
    "    epochs=best_params['epochs'], \n",
    "    batch_size=best_params['batch_size'], \n",
    "    validation_data=(X_test_seq, y_test_seq), \n",
    "    callbacks=[early_stopping],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Print the final training and validation loss\n",
    "print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e906dba-764c-4218-8e63-50d7be5f8798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "LSTM Model Performance:\n",
      "Training RMSE: 0.0205118824370475\n",
      "Test RMSE: 0.02305943075310435\n",
      "Training MAE: 0.014811615142594131\n",
      "Test MAE: 0.01743595933085359\n",
      "Directional Accuracy on Training Data: 55.223880597014926%\n",
      "Directional Accuracy on Test Data: 53.333333333333336%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict on both training and test sequences\n",
    "y_train_pred = model.predict(X_train_seq)\n",
    "y_test_pred = model.predict(X_test_seq)\n",
    "\n",
    "# Inverse scale predictions and actual values to original scale\n",
    "y_train_pred_rescaled = scaler_y.inverse_transform(y_train_pred)\n",
    "y_test_pred_rescaled = scaler_y.inverse_transform(y_test_pred)\n",
    "y_train_actual_rescaled = scaler_y.inverse_transform(y_train_seq)\n",
    "y_test_actual_rescaled = scaler_y.inverse_transform(y_test_seq)\n",
    "\n",
    "# Define a function for directional accuracy\n",
    "def directional_accuracy(y_true, y_pred):\n",
    "    return np.mean((np.sign(y_true[1:] - y_true[:-1]) == np.sign(y_pred[1:] - y_pred[:-1])).astype(int)) * 100\n",
    "\n",
    "# Calculate metrics.\n",
    "# Root Mean Squared Error (RMSE) for training set\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_actual_rescaled, y_train_pred_rescaled))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_actual_rescaled, y_test_pred_rescaled)) # RMSE for test set.\n",
    "train_mae = mean_absolute_error(y_train_actual_rescaled, y_train_pred_rescaled) # Mean Absolute Error (MAE) for training set.\n",
    "test_mae = mean_absolute_error(y_test_actual_rescaled, y_test_pred_rescaled) # MAE for test set.\n",
    "# Directional accuracy for training set.\n",
    "train_directional_accuracy = directional_accuracy(y_train_actual_rescaled.flatten(), y_train_pred_rescaled.flatten())\n",
    "# Directional accuracy for test set.\n",
    "test_directional_accuracy = directional_accuracy(y_test_actual_rescaled.flatten(), y_test_pred_rescaled.flatten())\n",
    "\n",
    "# Display score metrics to evaluate the model's performance.\n",
    "print(\"LSTM Model Performance:\")\n",
    "print(f\"Training RMSE: {train_rmse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Training MAE: {train_mae}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Directional Accuracy on Training Data: {train_directional_accuracy}%\")\n",
    "print(f\"Directional Accuracy on Test Data: {test_directional_accuracy}%\")\n",
    "\n",
    "# Plot training & validation loss values, it will help visualize how well the model fits the data over training epochs.\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269ba51d-3d4b-4f9e-8bf1-1f4a05e77097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "030d6715-d5fc-4e68-8b99-78636a6f3766",
   "metadata": {},
   "source": [
    "### 2. Optuna (Model 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef2b43a6-b5a7-4b83-854b-1deaa4503ebf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 22:05:50,007] A new study created in memory with name: no-name-1776b06a-ebda-4baf-a193-63eb7841a69b\n",
      "[I 2024-11-13 22:05:59,725] Trial 0 finished with value: 0.5658991932868958 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.2, 'l2_lambda': 0.0037550037026405905, 'learning_rate': 0.0001, 'learning_rate_decay': 1e-05, 'clipnorm': 1.0, 'units': 32, 'num_layers': 3, 'epochs': 100, 'batch_size': 64}. Best is trial 0 with value: 0.5658991932868958.\n",
      "[I 2024-11-13 22:06:06,987] Trial 1 finished with value: 2.5200958251953125 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.2, 'l2_lambda': 0.04641587067816149, 'learning_rate': 0.0001, 'learning_rate_decay': 1e-05, 'clipnorm': 1.0, 'units': 32, 'num_layers': 2, 'epochs': 50, 'batch_size': 256}. Best is trial 0 with value: 0.5658991932868958.\n",
      "[I 2024-11-13 22:06:23,010] Trial 2 finished with value: 2.4657230377197266 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.2, 'l2_lambda': 0.02754390996680696, 'learning_rate': 0.0001, 'learning_rate_decay': 1e-05, 'clipnorm': 1.0, 'units': 128, 'num_layers': 2, 'epochs': 100, 'batch_size': 120}. Best is trial 0 with value: 0.5658991932868958.\n",
      "[I 2024-11-13 22:06:34,392] Trial 3 finished with value: 0.30485692620277405 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.2, 'l2_lambda': 0.06953241045943279, 'learning_rate': 0.0001, 'learning_rate_decay': 1e-05, 'clipnorm': 1.0, 'units': 128, 'num_layers': 1, 'epochs': 150, 'batch_size': 256}. Best is trial 3 with value: 0.30485692620277405.\n",
      "[I 2024-11-13 22:06:42,474] Trial 4 finished with value: 0.14163048565387726 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.1, 'l2_lambda': 0.07320157289972352, 'learning_rate': 0.0005, 'learning_rate_decay': 1e-05, 'clipnorm': 1.0, 'units': 64, 'num_layers': 1, 'epochs': 150, 'batch_size': 32}. Best is trial 4 with value: 0.14163048565387726.\n",
      "[I 2024-11-13 22:06:56,260] Trial 5 finished with value: 0.23004084825515747 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.2, 'l2_lambda': 0.06621901977279396, 'learning_rate': 0.0005, 'learning_rate_decay': 0, 'clipnorm': 1.0, 'units': 64, 'num_layers': 2, 'epochs': 150, 'batch_size': 256}. Best is trial 4 with value: 0.14163048565387726.\n",
      "[I 2024-11-13 22:07:08,520] Trial 6 finished with value: 1.683044672012329 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0038300508438275606, 'learning_rate': 0.0001, 'learning_rate_decay': 0, 'clipnorm': 5.0, 'units': 128, 'num_layers': 3, 'epochs': 50, 'batch_size': 256}. Best is trial 4 with value: 0.14163048565387726.\n",
      "[I 2024-11-13 22:07:14,121] Trial 7 finished with value: 0.13858722150325775 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0019223481607694784, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 150, 'batch_size': 64}. Best is trial 7 with value: 0.13858722150325775.\n",
      "[I 2024-11-13 22:07:25,742] Trial 8 finished with value: 0.48415589332580566 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0017384974140603195, 'learning_rate': 0.0001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 64, 'num_layers': 3, 'epochs': 150, 'batch_size': 120}. Best is trial 7 with value: 0.13858722150325775.\n",
      "[I 2024-11-13 22:07:33,169] Trial 9 finished with value: 0.1343405395746231 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.1, 'l2_lambda': 0.028980866560207673, 'learning_rate': 0.0005, 'learning_rate_decay': 1e-06, 'clipnorm': 1.0, 'units': 64, 'num_layers': 1, 'epochs': 200, 'batch_size': 120}. Best is trial 9 with value: 0.1343405395746231.\n",
      "[I 2024-11-13 22:07:40,033] Trial 10 finished with value: 0.16332140564918518 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.1, 'l2_lambda': 0.015225998328909448, 'learning_rate': 0.0005, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 64, 'num_layers': 1, 'epochs': 200, 'batch_size': 120}. Best is trial 9 with value: 0.1343405395746231.\n",
      "[I 2024-11-13 22:07:45,169] Trial 11 finished with value: 0.13795514404773712 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0011454828358668492, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 200, 'batch_size': 64}. Best is trial 9 with value: 0.1343405395746231.\n",
      "[I 2024-11-13 22:07:51,359] Trial 12 finished with value: 0.14035919308662415 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.009816216006353566, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 200, 'batch_size': 64}. Best is trial 9 with value: 0.1343405395746231.\n",
      "[I 2024-11-13 22:07:55,116] Trial 13 finished with value: 0.14879068732261658 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0010058522170355295, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 64, 'num_layers': 1, 'epochs': 200, 'batch_size': 32}. Best is trial 9 with value: 0.1343405395746231.\n",
      "[I 2024-11-13 22:08:11,996] Trial 14 finished with value: 0.13121415674686432 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.1, 'l2_lambda': 0.012271684484652668, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 1.0, 'units': 32, 'num_layers': 2, 'epochs': 200, 'batch_size': 64}. Best is trial 14 with value: 0.13121415674686432.\n",
      "[I 2024-11-13 22:08:28,681] Trial 15 finished with value: 0.1611047089099884 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.1, 'l2_lambda': 0.011066584576546148, 'learning_rate': 0.0005, 'learning_rate_decay': 1e-06, 'clipnorm': 1.0, 'units': 32, 'num_layers': 2, 'epochs': 200, 'batch_size': 120}. Best is trial 14 with value: 0.13121415674686432.\n",
      "[I 2024-11-13 22:08:45,711] Trial 16 finished with value: 0.14102797210216522 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.1, 'l2_lambda': 0.022952286152095068, 'learning_rate': 0.0005, 'learning_rate_decay': 1e-06, 'clipnorm': 1.0, 'units': 32, 'num_layers': 2, 'epochs': 200, 'batch_size': 120}. Best is trial 14 with value: 0.13121415674686432.\n",
      "[I 2024-11-13 22:09:01,555] Trial 17 finished with value: 0.14973340928554535 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0068044806710884165, 'learning_rate': 0.001, 'learning_rate_decay': 0, 'clipnorm': 1.0, 'units': 32, 'num_layers': 3, 'epochs': 100, 'batch_size': 64}. Best is trial 14 with value: 0.13121415674686432.\n",
      "[I 2024-11-13 22:09:21,991] Trial 18 finished with value: 0.14012834429740906 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.1, 'l2_lambda': 0.028254688012189548, 'learning_rate': 0.0005, 'learning_rate_decay': 1e-06, 'clipnorm': 1.0, 'units': 32, 'num_layers': 2, 'epochs': 200, 'batch_size': 32}. Best is trial 14 with value: 0.13121415674686432.\n",
      "[I 2024-11-13 22:09:39,354] Trial 19 finished with value: 0.13283653557300568 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.1, 'l2_lambda': 0.005515274544193159, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 1.0, 'units': 64, 'num_layers': 2, 'epochs': 150, 'batch_size': 64}. Best is trial 14 with value: 0.13121415674686432.\n",
      "[I 2024-11-13 22:09:56,821] Trial 20 finished with value: 0.13296867907047272 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.1, 'l2_lambda': 0.005717298641707219, 'learning_rate': 0.001, 'learning_rate_decay': 0, 'clipnorm': 1.0, 'units': 64, 'num_layers': 2, 'epochs': 150, 'batch_size': 64}. Best is trial 14 with value: 0.13121415674686432.\n",
      "[I 2024-11-13 22:10:14,267] Trial 21 finished with value: 0.13336269557476044 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.1, 'l2_lambda': 0.005054885007014729, 'learning_rate': 0.001, 'learning_rate_decay': 0, 'clipnorm': 1.0, 'units': 64, 'num_layers': 2, 'epochs': 150, 'batch_size': 64}. Best is trial 14 with value: 0.13121415674686432.\n",
      "[I 2024-11-13 22:10:27,387] Trial 22 finished with value: 0.13735507428646088 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.1, 'l2_lambda': 0.007425060119041896, 'learning_rate': 0.001, 'learning_rate_decay': 0, 'clipnorm': 1.0, 'units': 64, 'num_layers': 2, 'epochs': 100, 'batch_size': 64}. Best is trial 14 with value: 0.13121415674686432.\n",
      "[I 2024-11-13 22:10:44,631] Trial 23 finished with value: 0.13663272559642792 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0031763071987281994, 'learning_rate': 0.001, 'learning_rate_decay': 0, 'clipnorm': 1.0, 'units': 64, 'num_layers': 2, 'epochs': 150, 'batch_size': 64}. Best is trial 14 with value: 0.13121415674686432.\n",
      "[I 2024-11-13 22:11:00,502] Trial 24 finished with value: 0.1311413198709488 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0151867360016515, 'learning_rate': 0.001, 'learning_rate_decay': 0, 'clipnorm': 1.0, 'units': 64, 'num_layers': 2, 'epochs': 150, 'batch_size': 64}. Best is trial 24 with value: 0.1311413198709488.\n",
      "[I 2024-11-13 22:11:12,501] Trial 25 finished with value: 0.13708092272281647 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.1, 'l2_lambda': 0.016197797874953533, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 1.0, 'units': 32, 'num_layers': 2, 'epochs': 100, 'batch_size': 64}. Best is trial 24 with value: 0.1311413198709488.\n",
      "[I 2024-11-13 22:11:32,689] Trial 26 finished with value: 0.13237778842449188 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.2, 'l2_lambda': 0.011662399350623912, 'learning_rate': 0.001, 'learning_rate_decay': 0, 'clipnorm': 1.0, 'units': 64, 'num_layers': 3, 'epochs': 150, 'batch_size': 64}. Best is trial 24 with value: 0.1311413198709488.\n",
      "[I 2024-11-13 22:11:53,590] Trial 27 finished with value: 0.13134633004665375 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.2, 'l2_lambda': 0.016206230275316817, 'learning_rate': 0.001, 'learning_rate_decay': 0, 'clipnorm': 1.0, 'units': 32, 'num_layers': 3, 'epochs': 150, 'batch_size': 64}. Best is trial 24 with value: 0.1311413198709488.\n",
      "[I 2024-11-13 22:12:09,409] Trial 28 finished with value: 0.13119502365589142 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.2, 'l2_lambda': 0.04254837972802355, 'learning_rate': 0.001, 'learning_rate_decay': 0, 'clipnorm': 1.0, 'units': 32, 'num_layers': 3, 'epochs': 100, 'batch_size': 64}. Best is trial 24 with value: 0.1311413198709488.\n",
      "[I 2024-11-13 22:12:20,678] Trial 29 finished with value: 0.16170422732830048 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.2, 'l2_lambda': 0.04372010965288467, 'learning_rate': 0.001, 'learning_rate_decay': 0, 'clipnorm': 1.0, 'units': 32, 'num_layers': 3, 'epochs': 50, 'batch_size': 64}. Best is trial 24 with value: 0.1311413198709488.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0151867360016515, 'learning_rate': 0.001, 'learning_rate_decay': 0, 'clipnorm': 1.0, 'units': 64, 'num_layers': 2, 'epochs': 150, 'batch_size': 64}\n",
      "Best validation loss: 0.1311413198709488\n"
     ]
    }
   ],
   "source": [
    "# Set global random state for reproducibility.\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# Optuna's TPESampler with a fixed random seed for reproducibility in parameter search.\n",
    "sampler = TPESampler(seed=random_seed)\n",
    "\n",
    "# Create an Optuna study with direction \"minimize\" to minimize the validation loss.\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "\n",
    "# Define the objective function for hyperparameter optimization.\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna to optimize hyperparameters of an LSTM model.\n",
    "    Takes a trial object from Optuna and returns the validation loss of the model with given parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define hyperparameters to tune.\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.2, 0.4, step=0.1)  # Dropout rate for LSTM layers.\n",
    "    recurrent_dropout = trial.suggest_categorical(\"recurrent_dropout\", [0.1, 0.2])  # Recurrent dropout rate for LSTM layers.\n",
    "    l2_lambda = trial.suggest_loguniform(\"l2_lambda\", 1e-3, 1e-1)  # L2 regularization factor.\n",
    "    learning_rate = trial.suggest_categorical(\"learning_rate\", [0.001, 0.0005, 0.0001])  # Learning rate for the optimizer.\n",
    "    learning_rate_decay = trial.suggest_categorical(\"learning_rate_decay\", [1e-6, 1e-5, 0])  # Learning rate decay.\n",
    "    clipnorm = trial.suggest_categorical(\"clipnorm\", [1.0, 5.0])  # Gradient clipping norm.\n",
    "    units = trial.suggest_categorical(\"units\", [32, 64, 128])  # Number of units in LSTM layers.\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)  # Number of LSTM layers.\n",
    "    epochs = trial.suggest_int(\"epochs\", 50, 200, step=50)  # Number of epochs.\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 120, 256])  # Batch size.\n",
    "\n",
    "    # Initialize the Sequential model.\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add LSTM layers based on num_layers.\n",
    "    for i in range(num_layers):\n",
    "        # Set return_sequences=True for all but the last LSTM layer.\n",
    "        return_sequences = (i < num_layers - 1)\n",
    "        \n",
    "        # Add LSTM layer with specified hyperparameters.\n",
    "        model.add(LSTM(\n",
    "            units=units,\n",
    "            return_sequences=return_sequences,\n",
    "            input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]) if i == 0 else None,  # Set input shape only for the first layer.\n",
    "            kernel_regularizer=l2(l2_lambda),\n",
    "            recurrent_dropout=recurrent_dropout\n",
    "        ))\n",
    "\n",
    "        # Add BatchNormalization and Dropout after each LSTM layer.\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Add the output layer with a single unit (regression).\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Initialize the optimizer with learning rate, decay, and gradient clipping norm.\n",
    "    optimizer = Adam(learning_rate=learning_rate, decay=learning_rate_decay, clipnorm=clipnorm)\n",
    "\n",
    "    # Compile the model with mean squared error loss function.\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Early stopping callback to avoid overfitting.\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model with given hyperparameters.\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test_seq, y_test_seq),\n",
    "        callbacks=[early_stopping],\n",
    "        shuffle=False,\n",
    "        verbose=0  # Set verbose=0 to suppress training logs for faster experimentation.\n",
    "    )\n",
    "\n",
    "    # Retrieve the minimum validation loss from the training history.\n",
    "    val_loss = min(history.history['val_loss'])\n",
    "    \n",
    "    # Return the validation loss to be minimized by Optuna.\n",
    "    return val_loss\n",
    "\n",
    "# Run the Optuna study for a given number of trials.\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Print the best parameters found by the study.\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "# Print the best validation loss achieved with the optimal parameters.\n",
    "print(\"Best validation loss:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "93575018-c305-4e9a-92e0-8f040b0312b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - loss: 4.0277 - val_loss: 1.7664\n",
      "Epoch 2/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.0139 - val_loss: 1.6824\n",
      "Epoch 3/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.0577 - val_loss: 1.6041\n",
      "Epoch 4/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.7362 - val_loss: 1.5199\n",
      "Epoch 5/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.4927 - val_loss: 1.4370\n",
      "Epoch 6/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.3444 - val_loss: 1.3537\n",
      "Epoch 7/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2194 - val_loss: 1.2698\n",
      "Epoch 8/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9157 - val_loss: 1.1867\n",
      "Epoch 9/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7669 - val_loss: 1.1054\n",
      "Epoch 10/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6056 - val_loss: 1.0213\n",
      "Epoch 11/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.5633 - val_loss: 0.9507\n",
      "Epoch 12/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.2525 - val_loss: 0.8790\n",
      "Epoch 13/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2432 - val_loss: 0.8110\n",
      "Epoch 14/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0360 - val_loss: 0.7460\n",
      "Epoch 15/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0055 - val_loss: 0.6848\n",
      "Epoch 16/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.8921 - val_loss: 0.6265\n",
      "Epoch 17/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8197 - val_loss: 0.5740\n",
      "Epoch 18/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7323 - val_loss: 0.5276\n",
      "Epoch 19/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6735 - val_loss: 0.4871\n",
      "Epoch 20/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5971 - val_loss: 0.4496\n",
      "Epoch 21/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5364 - val_loss: 0.4188\n",
      "Epoch 22/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4869 - val_loss: 0.3918\n",
      "Epoch 23/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4344 - val_loss: 0.3672\n",
      "Epoch 24/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3866 - val_loss: 0.3465\n",
      "Epoch 25/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3602 - val_loss: 0.3290\n",
      "Epoch 26/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3342 - val_loss: 0.3136\n",
      "Epoch 27/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3229 - val_loss: 0.2971\n",
      "Epoch 28/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2965 - val_loss: 0.2848\n",
      "Epoch 29/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2715 - val_loss: 0.2737\n",
      "Epoch 30/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2500 - val_loss: 0.2632\n",
      "Epoch 31/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2487 - val_loss: 0.2528\n",
      "Epoch 32/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2362 - val_loss: 0.2446\n",
      "Epoch 33/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2277 - val_loss: 0.2366\n",
      "Epoch 34/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2181 - val_loss: 0.2299\n",
      "Epoch 35/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2056 - val_loss: 0.2226\n",
      "Epoch 36/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1958 - val_loss: 0.2165\n",
      "Epoch 37/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1939 - val_loss: 0.2112\n",
      "Epoch 38/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1838 - val_loss: 0.2057\n",
      "Epoch 39/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1796 - val_loss: 0.2004\n",
      "Epoch 40/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1712 - val_loss: 0.1963\n",
      "Epoch 41/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1664 - val_loss: 0.1920\n",
      "Epoch 42/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1619 - val_loss: 0.1881\n",
      "Epoch 43/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1630 - val_loss: 0.1850\n",
      "Epoch 44/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1572 - val_loss: 0.1818\n",
      "Epoch 45/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1557 - val_loss: 0.1781\n",
      "Epoch 46/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1529 - val_loss: 0.1757\n",
      "Epoch 47/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1473 - val_loss: 0.1727\n",
      "Epoch 48/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1471 - val_loss: 0.1701\n",
      "Epoch 49/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1441 - val_loss: 0.1678\n",
      "Epoch 50/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1404 - val_loss: 0.1660\n",
      "Epoch 51/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1382 - val_loss: 0.1636\n",
      "Epoch 52/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1347 - val_loss: 0.1615\n",
      "Epoch 53/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1347 - val_loss: 0.1602\n",
      "Epoch 54/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1314 - val_loss: 0.1584\n",
      "Epoch 55/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1306 - val_loss: 0.1566\n",
      "Epoch 56/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1287 - val_loss: 0.1553\n",
      "Epoch 57/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1278 - val_loss: 0.1540\n",
      "Epoch 58/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1271 - val_loss: 0.1525\n",
      "Epoch 59/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1250 - val_loss: 0.1512\n",
      "Epoch 60/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1231 - val_loss: 0.1504\n",
      "Epoch 61/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1239 - val_loss: 0.1491\n",
      "Epoch 62/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1202 - val_loss: 0.1474\n",
      "Epoch 63/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1205 - val_loss: 0.1473\n",
      "Epoch 64/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1212 - val_loss: 0.1461\n",
      "Epoch 65/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1190 - val_loss: 0.1458\n",
      "Epoch 66/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1180 - val_loss: 0.1451\n",
      "Epoch 67/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1173 - val_loss: 0.1441\n",
      "Epoch 68/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1169 - val_loss: 0.1433\n",
      "Epoch 69/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1151 - val_loss: 0.1431\n",
      "Epoch 70/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1160 - val_loss: 0.1434\n",
      "Epoch 71/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1144 - val_loss: 0.1413\n",
      "Epoch 72/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1129 - val_loss: 0.1409\n",
      "Epoch 73/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1127 - val_loss: 0.1409\n",
      "Epoch 74/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1120 - val_loss: 0.1401\n",
      "Epoch 75/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1124 - val_loss: 0.1393\n",
      "Epoch 76/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1122 - val_loss: 0.1391\n",
      "Epoch 77/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1102 - val_loss: 0.1389\n",
      "Epoch 78/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1111 - val_loss: 0.1386\n",
      "Epoch 79/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1106 - val_loss: 0.1377\n",
      "Epoch 80/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1098 - val_loss: 0.1370\n",
      "Epoch 81/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1092 - val_loss: 0.1371\n",
      "Epoch 82/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1091 - val_loss: 0.1373\n",
      "Epoch 83/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1092 - val_loss: 0.1370\n",
      "Epoch 84/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1084 - val_loss: 0.1359\n",
      "Epoch 85/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1080 - val_loss: 0.1358\n",
      "Epoch 86/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1093 - val_loss: 0.1357\n",
      "Epoch 87/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1081 - val_loss: 0.1357\n",
      "Epoch 88/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1068 - val_loss: 0.1358\n",
      "Epoch 89/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1073 - val_loss: 0.1352\n",
      "Epoch 90/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1070 - val_loss: 0.1347\n",
      "Epoch 91/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1072 - val_loss: 0.1356\n",
      "Epoch 92/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1063 - val_loss: 0.1361\n",
      "Epoch 93/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1076 - val_loss: 0.1344\n",
      "Epoch 94/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1067 - val_loss: 0.1341\n",
      "Epoch 95/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1067 - val_loss: 0.1337\n",
      "Epoch 96/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1058 - val_loss: 0.1339\n",
      "Epoch 97/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1063 - val_loss: 0.1340\n",
      "Epoch 98/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1066 - val_loss: 0.1335\n",
      "Epoch 99/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1060 - val_loss: 0.1333\n",
      "Epoch 100/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1055 - val_loss: 0.1329\n",
      "Epoch 101/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1052 - val_loss: 0.1326\n",
      "Epoch 102/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1054 - val_loss: 0.1327\n",
      "Epoch 103/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1059 - val_loss: 0.1327\n",
      "Epoch 104/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1052 - val_loss: 0.1325\n",
      "Epoch 105/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1038 - val_loss: 0.1326\n",
      "Epoch 106/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1052 - val_loss: 0.1324\n",
      "Epoch 107/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1056 - val_loss: 0.1321\n",
      "Epoch 108/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1052 - val_loss: 0.1321\n",
      "Epoch 109/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1043 - val_loss: 0.1320\n",
      "Epoch 110/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1051 - val_loss: 0.1319\n",
      "Epoch 111/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1042 - val_loss: 0.1320\n",
      "Epoch 112/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1056 - val_loss: 0.1322\n",
      "Epoch 113/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1037 - val_loss: 0.1325\n",
      "Epoch 114/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1046 - val_loss: 0.1325\n",
      "Epoch 115/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1042 - val_loss: 0.1325\n",
      "Epoch 116/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1049 - val_loss: 0.1318\n",
      "Epoch 117/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1038 - val_loss: 0.1318\n",
      "Epoch 118/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1038 - val_loss: 0.1321\n",
      "Epoch 119/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1040 - val_loss: 0.1320\n",
      "Epoch 120/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1041 - val_loss: 0.1320\n",
      "Epoch 121/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1039 - val_loss: 0.1321\n",
      "Epoch 122/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1035 - val_loss: 0.1322\n",
      "Epoch 123/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1048 - val_loss: 0.1317\n",
      "Epoch 124/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1040 - val_loss: 0.1314\n",
      "Epoch 125/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1044 - val_loss: 0.1314\n",
      "Epoch 126/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1040 - val_loss: 0.1315\n",
      "Epoch 127/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1039 - val_loss: 0.1315\n",
      "Epoch 128/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1039 - val_loss: 0.1314\n",
      "Epoch 129/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1044 - val_loss: 0.1312\n",
      "Epoch 130/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1041 - val_loss: 0.1310\n",
      "Epoch 131/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1039 - val_loss: 0.1310\n",
      "Epoch 132/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1041 - val_loss: 0.1312\n",
      "Epoch 133/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1037 - val_loss: 0.1311\n",
      "Epoch 134/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1035 - val_loss: 0.1309\n",
      "Epoch 135/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1038 - val_loss: 0.1309\n",
      "Epoch 136/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1029 - val_loss: 0.1310\n",
      "Epoch 137/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1038 - val_loss: 0.1312\n",
      "Epoch 138/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1031 - val_loss: 0.1315\n",
      "Epoch 139/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1039 - val_loss: 0.1311\n",
      "Epoch 140/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1040 - val_loss: 0.1311\n",
      "Epoch 141/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1038 - val_loss: 0.1311\n",
      "Epoch 142/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1033 - val_loss: 0.1316\n",
      "Epoch 143/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1037 - val_loss: 0.1318\n",
      "Epoch 144/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1035 - val_loss: 0.1312\n",
      "Epoch 145/150\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1035 - val_loss: 0.1315\n",
      "Final Training Loss: 0.11094673722982407\n",
      "Final Validation Loss: 0.1314915269613266\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Best hyperparameters found from tuning\n",
    "best_params = {\n",
    "    'units1': 64,\n",
    "    'units2': 64,\n",
    "    'recurrent_dropout': 0.1,\n",
    "    'optimizer': 'adam',\n",
    "    'loss_function': 'mean_squared_error',\n",
    "    'learning_rate_decay': 0,\n",
    "    'learning_rate': 0.001,\n",
    "    'l2_lambda':  0.0151867360016515,\n",
    "    'epochs': 150,\n",
    "    'dropout_rate': 0.4,\n",
    "    'clipnorm': 1.0,\n",
    "    'batch_size': 64\n",
    "}\n",
    "\n",
    "# Build the model using the best parameters\n",
    "def build_best_model(params):\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units1'], return_sequences=True, \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "                   kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Second LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units2'], return_sequences=False, kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Select the optimizer\n",
    "    if params['optimizer'] == 'adam':\n",
    "        optimizer = Adam(learning_rate=params['learning_rate'], decay=params['learning_rate_decay'], clipnorm=params['clipnorm'])\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizer '{params['optimizer']}' is not implemented in this setup.\")\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate the model with the best hyperparameters\n",
    "model = build_best_model(best_params)\n",
    "\n",
    "# Early stopping to stop training when validation loss stops improving\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq, \n",
    "    epochs=best_params['epochs'], \n",
    "    batch_size=best_params['batch_size'], \n",
    "    validation_data=(X_test_seq, y_test_seq),\n",
    "    shuffle=False,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Print the final training and validation loss\n",
    "print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "57d5dba5-3d26-4f65-b006-5171ffc7c87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "LSTM Model Performance:\n",
      "Training RMSE: 0.020429289257174195\n",
      "Test RMSE: 0.023033736753495335\n",
      "Training MAE: 0.014797507612470435\n",
      "Test MAE: 0.017420660276589818\n",
      "Directional Accuracy on Training Data: 56.831228473019515%\n",
      "Directional Accuracy on Test Data: 58.666666666666664%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict on both training and test sequences\n",
    "y_train_pred = model.predict(X_train_seq)\n",
    "y_test_pred = model.predict(X_test_seq)\n",
    "\n",
    "# Inverse scale predictions and actual values to original scale\n",
    "y_train_pred_rescaled = scaler_y.inverse_transform(y_train_pred)\n",
    "y_test_pred_rescaled = scaler_y.inverse_transform(y_test_pred)\n",
    "y_train_actual_rescaled = scaler_y.inverse_transform(y_train_seq)\n",
    "y_test_actual_rescaled = scaler_y.inverse_transform(y_test_seq)\n",
    "\n",
    "# Define a function for directional accuracy\n",
    "def directional_accuracy(y_true, y_pred):\n",
    "    return np.mean((np.sign(y_true[1:] - y_true[:-1]) == np.sign(y_pred[1:] - y_pred[:-1])).astype(int)) * 100\n",
    "\n",
    "# Calculate metrics.\n",
    "# Root Mean Squared Error (RMSE) for training set\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_actual_rescaled, y_train_pred_rescaled))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_actual_rescaled, y_test_pred_rescaled)) # RMSE for test set.\n",
    "train_mae = mean_absolute_error(y_train_actual_rescaled, y_train_pred_rescaled) # Mean Absolute Error (MAE) for training set.\n",
    "test_mae = mean_absolute_error(y_test_actual_rescaled, y_test_pred_rescaled) # MAE for test set.\n",
    "# Directional accuracy for training set.\n",
    "train_directional_accuracy = directional_accuracy(y_train_actual_rescaled.flatten(), y_train_pred_rescaled.flatten())\n",
    "# Directional accuracy for test set.\n",
    "test_directional_accuracy = directional_accuracy(y_test_actual_rescaled.flatten(), y_test_pred_rescaled.flatten())\n",
    "\n",
    "# Display score metrics to evaluate the model's performance.\n",
    "print(\"LSTM Model Performance:\")\n",
    "print(f\"Training RMSE: {train_rmse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Training MAE: {train_mae}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Directional Accuracy on Training Data: {train_directional_accuracy}%\")\n",
    "print(f\"Directional Accuracy on Test Data: {test_directional_accuracy}%\")\n",
    "\n",
    "# Plot training & validation loss values, it will help visualize how well the model fits the data over training epochs.\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6211ff62-d6d8-43cd-b98e-7b2f4ce7bfa9",
   "metadata": {},
   "source": [
    "### 3. Keras Tuner (Model 6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6604b2e1-32c2-46ad-8f1d-8fcae761ffee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 00m 06s]\n",
      "val_loss: 0.14044289290905\n",
      "\n",
      "Best val_loss So Far: 0.13999874889850616\n",
      "Total elapsed time: 00h 12m 45s\n",
      "Built model with params: dropout_rate=0.3, recurrent_dropout=0.2, l2_lambda=0.07913979537147964, learning_rate=0.001, learning_rate_decay=1e-05, clipnorm=5.0, units=32, num_layers=2, batch_size=64\n",
      "Best hyperparameters: {'dropout_rate': 0.3, 'recurrent_dropout': 0.2, 'l2_lambda': 0.07913979537147964, 'learning_rate': 0.001, 'learning_rate_decay': 1e-05, 'clipnorm': 5.0, 'units': 32, 'num_layers': 2, 'batch_size': 64}\n",
      "Best batch size: 64\n"
     ]
    }
   ],
   "source": [
    "# Set a global random seed for reproducibility.\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# Build model function with hyperparameter choices.\n",
    "def build_model(hp): #hp (kerastuner.HyperParameters) - Hyperparameter search space.\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Define hyperparameters using `hp` for various tuning options.\n",
    "    dropout_rate = hp.Choice(\"dropout_rate\", [0.2, 0.3, 0.4])  # Dropout rates to fight overfitting.\n",
    "    recurrent_dropout = hp.Choice(\"recurrent_dropout\", [0.1, 0.2])  # Recurrent dropout for LSTM layers.\n",
    "    l2_lambda = hp.Float(\"l2_lambda\", min_value=0.001, max_value=0.1, sampling=\"log\")  # L2 regularization factor.\n",
    "    learning_rate = hp.Choice(\"learning_rate\", [0.001, 0.0005, 0.0001])  # Learning rate choices.\n",
    "    learning_rate_decay = hp.Choice(\"learning_rate_decay\", [1e-5, 0.0])  # Learning rate decay for gradual reduction.\n",
    "    clipnorm = hp.Choice(\"clipnorm\", [1.0, 5.0])  # Gradient clipping norm to prevent exploding gradients.\n",
    "    units = hp.Choice(\"units\", [32, 64, 128])  # Number of units for LSTM layers.\n",
    "    num_layers = hp.Int(\"num_layers\", 1, 3)  # Number of LSTM layers.\n",
    "    batch_size = hp.Choice(\"batch_size\", [32, 64, 120, 256])  # Batch size choices.\n",
    "\n",
    "\n",
    "    # Add LSTM layers based on the number of layers selected.\n",
    "    for i in range(num_layers):\n",
    "        return_sequences = (i < num_layers - 1)\n",
    "        model.add(LSTM(units=units, return_sequences=return_sequences,\n",
    "                       input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]) if i == 0 else None,\n",
    "                       kernel_regularizer=l2(l2_lambda), recurrent_dropout=recurrent_dropout))\n",
    "        model.add(BatchNormalization()) # Add batch normalization to stabilize training.\n",
    "        model.add(Dropout(dropout_rate))  # Add dropout to help with overfitting.\n",
    "\n",
    "    model.add(Dense(1))  # Output layer for a single continuous value.\n",
    "    # Configure optimizer with learning rate, decay, and gradient clipping.\n",
    "    optimizer = Adam(learning_rate=learning_rate, decay=learning_rate_decay, clipnorm=clipnorm)\n",
    "     # Compile model with specified optimizer and mean squared error loss.\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    print(f\"Built model with params: dropout_rate={dropout_rate}, recurrent_dropout={recurrent_dropout}, \"\n",
    "          f\"l2_lambda={l2_lambda}, learning_rate={learning_rate}, learning_rate_decay={learning_rate_decay}, \"\n",
    "          f\"clipnorm={clipnorm}, units={units}, num_layers={num_layers}, batch_size={batch_size}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Set up Bayesian Optimization tuner to search for optimal hyperparameters.\n",
    "tuner = BayesianOptimization(\n",
    "    build_model,  # Model-building function.\n",
    "    objective=\"val_loss\",  # Target metric for optimization.\n",
    "    max_trials=30,  # Maximum number of trials to run.\n",
    "    executions_per_trial=1,  # Number of times to execute each trial.\n",
    "    directory=\"tuner_dir\",  # Directory to store tuning results.\n",
    "    project_name=\"lstm_tuning_capstone\",  # Tuning project name.\n",
    "    overwrite=True  # Overwrite existing tuner results if present.\n",
    ")\n",
    "\n",
    "# Define early stopping.\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=True)\n",
    "\n",
    "# Perform tuning with verbose logging.\n",
    "tuner.search(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_test_seq, y_test_seq),\n",
    "    epochs=200,\n",
    "    callbacks=[early_stopping],\n",
    "    shuffle=False,\n",
    "    verbose=1  # Ensure output of each trial.\n",
    ")\n",
    "\n",
    "# Retrieve the best model and hyperparameters.\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Best hyperparameters:\", best_hyperparameters.values)\n",
    "print(\"Best batch size:\", best_hyperparameters.get(\"batch_size\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "69d8abfa-138c-42c3-b1f0-f2d9b7c1fb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 278ms/step - loss: 7.2129 - val_loss: 4.5267\n",
      "Epoch 2/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 5.6316 - val_loss: 4.0535\n",
      "Epoch 3/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 5.0528 - val_loss: 3.6056\n",
      "Epoch 4/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 4.3184 - val_loss: 3.1942\n",
      "Epoch 5/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 3.7722 - val_loss: 2.8428\n",
      "Epoch 6/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.5323 - val_loss: 2.5371\n",
      "Epoch 7/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1224 - val_loss: 2.2734\n",
      "Epoch 8/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.8322 - val_loss: 2.0404\n",
      "Epoch 9/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 2.6262 - val_loss: 1.8427\n",
      "Epoch 10/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.3146 - val_loss: 1.6691\n",
      "Epoch 11/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.1390 - val_loss: 1.5154\n",
      "Epoch 12/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.8939 - val_loss: 1.3851\n",
      "Epoch 13/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.7133 - val_loss: 1.2707\n",
      "Epoch 14/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.6274 - val_loss: 1.1700\n",
      "Epoch 15/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.4900 - val_loss: 1.0805\n",
      "Epoch 16/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.4566 - val_loss: 1.0036\n",
      "Epoch 17/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 1.2876 - val_loss: 0.9355\n",
      "Epoch 18/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1.2055 - val_loss: 0.8749\n",
      "Epoch 19/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.1628 - val_loss: 0.8213\n",
      "Epoch 20/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.0377 - val_loss: 0.7737\n",
      "Epoch 21/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.9532 - val_loss: 0.7298\n",
      "Epoch 22/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.9062 - val_loss: 0.6898\n",
      "Epoch 23/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.8523 - val_loss: 0.6539\n",
      "Epoch 24/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.8163 - val_loss: 0.6224\n",
      "Epoch 25/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.7649 - val_loss: 0.5922\n",
      "Epoch 26/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.7776 - val_loss: 0.5644\n",
      "Epoch 27/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.7012 - val_loss: 0.5384\n",
      "Epoch 28/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.6840 - val_loss: 0.5152\n",
      "Epoch 29/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.6367 - val_loss: 0.4918\n",
      "Epoch 30/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.6045 - val_loss: 0.4716\n",
      "Epoch 31/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.5765 - val_loss: 0.4530\n",
      "Epoch 32/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.5304 - val_loss: 0.4348\n",
      "Epoch 33/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4922 - val_loss: 0.4179\n",
      "Epoch 34/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4948 - val_loss: 0.4030\n",
      "Epoch 35/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.4625 - val_loss: 0.3858\n",
      "Epoch 36/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.4619 - val_loss: 0.3792\n",
      "Epoch 37/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.4339 - val_loss: 0.3576\n",
      "Epoch 38/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3872 - val_loss: 0.3468\n",
      "Epoch 39/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3882 - val_loss: 0.3328\n",
      "Epoch 40/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3571 - val_loss: 0.3218\n",
      "Epoch 41/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3405 - val_loss: 0.3131\n",
      "Epoch 42/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3299 - val_loss: 0.3012\n",
      "Epoch 43/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3070 - val_loss: 0.2910\n",
      "Epoch 44/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2974 - val_loss: 0.2806\n",
      "Epoch 45/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2835 - val_loss: 0.2725\n",
      "Epoch 46/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2760 - val_loss: 0.2637\n",
      "Epoch 47/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.2629 - val_loss: 0.2567\n",
      "Epoch 48/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2456 - val_loss: 0.2498\n",
      "Epoch 49/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2439 - val_loss: 0.2419\n",
      "Epoch 50/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2278 - val_loss: 0.2356\n",
      "Epoch 51/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2266 - val_loss: 0.2292\n",
      "Epoch 52/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.2130 - val_loss: 0.2238\n",
      "Epoch 53/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2051 - val_loss: 0.2175\n",
      "Epoch 54/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1988 - val_loss: 0.2124\n",
      "Epoch 55/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1949 - val_loss: 0.2076\n",
      "Epoch 56/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1898 - val_loss: 0.2061\n",
      "Epoch 57/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1810 - val_loss: 0.2004\n",
      "Epoch 58/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1748 - val_loss: 0.1964\n",
      "Epoch 59/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1741 - val_loss: 0.1920\n",
      "Epoch 60/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1672 - val_loss: 0.1893\n",
      "Epoch 61/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1650 - val_loss: 0.1848\n",
      "Epoch 62/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1591 - val_loss: 0.1842\n",
      "Epoch 63/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1560 - val_loss: 0.1804\n",
      "Epoch 64/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1546 - val_loss: 0.1764\n",
      "Epoch 65/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1551 - val_loss: 0.1741\n",
      "Epoch 66/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1480 - val_loss: 0.1729\n",
      "Epoch 67/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1467 - val_loss: 0.1694\n",
      "Epoch 68/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1428 - val_loss: 0.1678\n",
      "Epoch 69/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.1412 - val_loss: 0.1661\n",
      "Epoch 70/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1376 - val_loss: 0.1637\n",
      "Epoch 71/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1367 - val_loss: 0.1614\n",
      "Epoch 72/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1351 - val_loss: 0.1595\n",
      "Epoch 73/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1309 - val_loss: 0.1589\n",
      "Epoch 74/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1313 - val_loss: 0.1575\n",
      "Epoch 75/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1297 - val_loss: 0.1559\n",
      "Epoch 76/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1261 - val_loss: 0.1554\n",
      "Epoch 77/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1258 - val_loss: 0.1530\n",
      "Epoch 78/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1240 - val_loss: 0.1510\n",
      "Epoch 79/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1231 - val_loss: 0.1503\n",
      "Epoch 80/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1234 - val_loss: 0.1487\n",
      "Epoch 81/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1207 - val_loss: 0.1467\n",
      "Epoch 82/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1192 - val_loss: 0.1463\n",
      "Epoch 83/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.1180 - val_loss: 0.1456\n",
      "Epoch 84/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1167 - val_loss: 0.1444\n",
      "Epoch 85/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1175 - val_loss: 0.1438\n",
      "Epoch 86/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1162 - val_loss: 0.1436\n",
      "Epoch 87/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1138 - val_loss: 0.1429\n",
      "Epoch 88/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1151 - val_loss: 0.1423\n",
      "Epoch 89/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1129 - val_loss: 0.1418\n",
      "Epoch 90/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1132 - val_loss: 0.1412\n",
      "Epoch 91/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1120 - val_loss: 0.1402\n",
      "Epoch 92/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1123 - val_loss: 0.1400\n",
      "Epoch 93/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1114 - val_loss: 0.1394\n",
      "Epoch 94/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1114 - val_loss: 0.1387\n",
      "Epoch 95/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1116 - val_loss: 0.1382\n",
      "Epoch 96/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1091 - val_loss: 0.1374\n",
      "Epoch 97/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1097 - val_loss: 0.1370\n",
      "Epoch 98/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1103 - val_loss: 0.1364\n",
      "Epoch 99/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1088 - val_loss: 0.1361\n",
      "Epoch 100/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.1069 - val_loss: 0.1357\n",
      "Epoch 101/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1078 - val_loss: 0.1355\n",
      "Epoch 102/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1075 - val_loss: 0.1352\n",
      "Epoch 103/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1074 - val_loss: 0.1349\n",
      "Epoch 104/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.1070 - val_loss: 0.1345\n",
      "Epoch 105/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1076 - val_loss: 0.1342\n",
      "Epoch 106/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1064 - val_loss: 0.1345\n",
      "Epoch 107/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1064 - val_loss: 0.1343\n",
      "Epoch 108/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1070 - val_loss: 0.1340\n",
      "Epoch 109/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1060 - val_loss: 0.1342\n",
      "Epoch 110/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1068 - val_loss: 0.1353\n",
      "Epoch 111/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1063 - val_loss: 0.1342\n",
      "Epoch 112/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1046 - val_loss: 0.1333\n",
      "Epoch 113/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1054 - val_loss: 0.1334\n",
      "Epoch 114/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1060 - val_loss: 0.1337\n",
      "Epoch 115/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1051 - val_loss: 0.1333\n",
      "Epoch 116/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1060 - val_loss: 0.1326\n",
      "Epoch 117/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1046 - val_loss: 0.1326\n",
      "Epoch 118/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1047 - val_loss: 0.1331\n",
      "Epoch 119/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1054 - val_loss: 0.1328\n",
      "Epoch 120/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1051 - val_loss: 0.1322\n",
      "Epoch 121/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1038 - val_loss: 0.1320\n",
      "Epoch 122/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1048 - val_loss: 0.1321\n",
      "Epoch 123/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1050 - val_loss: 0.1318\n",
      "Epoch 124/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1048 - val_loss: 0.1318\n",
      "Epoch 125/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1051 - val_loss: 0.1316\n",
      "Epoch 126/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1044 - val_loss: 0.1315\n",
      "Epoch 127/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1046 - val_loss: 0.1314\n",
      "Epoch 128/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1037 - val_loss: 0.1318\n",
      "Epoch 129/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1043 - val_loss: 0.1316\n",
      "Epoch 130/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1041 - val_loss: 0.1312\n",
      "Epoch 131/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1042 - val_loss: 0.1312\n",
      "Epoch 132/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1041 - val_loss: 0.1314\n",
      "Epoch 133/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1042 - val_loss: 0.1322\n",
      "Epoch 134/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1042 - val_loss: 0.1327\n",
      "Epoch 135/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1039 - val_loss: 0.1318\n",
      "Epoch 136/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1033 - val_loss: 0.1311\n",
      "Epoch 137/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1032 - val_loss: 0.1312\n",
      "Epoch 138/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1038 - val_loss: 0.1317\n",
      "Epoch 139/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1035 - val_loss: 0.1318\n",
      "Epoch 140/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1038 - val_loss: 0.1313\n",
      "Epoch 141/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1035 - val_loss: 0.1310\n",
      "Epoch 142/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1039 - val_loss: 0.1307\n",
      "Epoch 143/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1038 - val_loss: 0.1308\n",
      "Epoch 144/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1039 - val_loss: 0.1310\n",
      "Epoch 145/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1042 - val_loss: 0.1308\n",
      "Epoch 146/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1036 - val_loss: 0.1309\n",
      "Epoch 147/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1036 - val_loss: 0.1316\n",
      "Epoch 148/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1032 - val_loss: 0.1316\n",
      "Epoch 149/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1040 - val_loss: 0.1319\n",
      "Epoch 150/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1044 - val_loss: 0.1307\n",
      "Epoch 151/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1039 - val_loss: 0.1308\n",
      "Epoch 152/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1035 - val_loss: 0.1309\n",
      "Final Training Loss: 0.11169055849313736\n",
      "Final Validation Loss: 0.13089299201965332\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Best hyperparameters found from tuning\n",
    "best_params = {\n",
    "    'units1': 32,\n",
    "    'units2': 32,\n",
    "    'recurrent_dropout': 0.2,\n",
    "    'optimizer': 'adam',\n",
    "    'loss_function': 'mean_squared_error',\n",
    "    'learning_rate_decay': 1e-05,\n",
    "    'learning_rate': 0.001,\n",
    "    'l2_lambda': 0.07913979537147964,\n",
    "    'epochs': 200,\n",
    "    'dropout_rate': 0.3,\n",
    "    'clipnorm': 5.0,\n",
    "    'batch_size': 64\n",
    "}\n",
    "\n",
    "# Build the model using the best parameters\n",
    "def build_best_model(params):\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units1'], return_sequences=True, \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "                   kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Second LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units2'], return_sequences=False, kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Select the optimizer\n",
    "    if params['optimizer'] == 'adam':\n",
    "        optimizer = Adam(learning_rate=params['learning_rate'], decay=params['learning_rate_decay'], clipnorm=params['clipnorm'])\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizer '{params['optimizer']}' is not implemented in this setup.\")\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate the model with the best hyperparameters\n",
    "model = build_best_model(best_params)\n",
    "\n",
    "# Early stopping to stop training when validation loss stops improving\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq, \n",
    "    epochs=best_params['epochs'], \n",
    "    batch_size=best_params['batch_size'], \n",
    "    validation_data=(X_test_seq, y_test_seq), \n",
    "    callbacks=[early_stopping],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Print the final training and validation loss\n",
    "print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f866691c-d151-463e-bd6a-78edd9d91a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step  \n",
      "LSTM Model Performance:\n",
      "Training RMSE: 0.020419093498409226\n",
      "Test RMSE: 0.023013218380415243\n",
      "Training MAE: 0.014854729799560932\n",
      "Test MAE: 0.01746791887816428\n",
      "Directional Accuracy on Training Data: 56.48679678530425%\n",
      "Directional Accuracy on Test Data: 57.99999999999999%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict on both training and test sequences\n",
    "y_train_pred = model.predict(X_train_seq)\n",
    "y_test_pred = model.predict(X_test_seq)\n",
    "\n",
    "# Inverse scale predictions and actual values to original scale\n",
    "y_train_pred_rescaled = scaler_y.inverse_transform(y_train_pred)\n",
    "y_test_pred_rescaled = scaler_y.inverse_transform(y_test_pred)\n",
    "y_train_actual_rescaled = scaler_y.inverse_transform(y_train_seq)\n",
    "y_test_actual_rescaled = scaler_y.inverse_transform(y_test_seq)\n",
    "\n",
    "# Define a function for directional accuracy\n",
    "def directional_accuracy(y_true, y_pred):\n",
    "    return np.mean((np.sign(y_true[1:] - y_true[:-1]) == np.sign(y_pred[1:] - y_pred[:-1])).astype(int)) * 100\n",
    "\n",
    "# Calculate metrics.\n",
    "# Root Mean Squared Error (RMSE) for training set\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_actual_rescaled, y_train_pred_rescaled))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_actual_rescaled, y_test_pred_rescaled)) # RMSE for test set.\n",
    "train_mae = mean_absolute_error(y_train_actual_rescaled, y_train_pred_rescaled) # Mean Absolute Error (MAE) for training set.\n",
    "test_mae = mean_absolute_error(y_test_actual_rescaled, y_test_pred_rescaled) # MAE for test set.\n",
    "# Directional accuracy for training set.\n",
    "train_directional_accuracy = directional_accuracy(y_train_actual_rescaled.flatten(), y_train_pred_rescaled.flatten())\n",
    "# Directional accuracy for test set.\n",
    "test_directional_accuracy = directional_accuracy(y_test_actual_rescaled.flatten(), y_test_pred_rescaled.flatten())\n",
    "\n",
    "# Display score metrics to evaluate the model's performance.\n",
    "print(\"LSTM Model Performance:\")\n",
    "print(f\"Training RMSE: {train_rmse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Training MAE: {train_mae}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Directional Accuracy on Training Data: {train_directional_accuracy}%\")\n",
    "print(f\"Directional Accuracy on Test Data: {test_directional_accuracy}%\")\n",
    "\n",
    "# Plot training & validation loss values, it will help visualize how well the model fits the data over training epochs.\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a45263-7759-4354-a7a6-4f67ab15acc6",
   "metadata": {},
   "source": [
    "### Model Performance Metrics for Sequence Length = 4\n",
    "\n",
    "| Metric         | Model 1       | Model 2       | Model 3       | Model 4       | Model 5       | Model 6       |\n",
    "|-----------------|---------------|---------------|---------------|---------------|---------------|---------------|\n",
    "| **Train RMSE** | 0.020314455   | 0.019941765   | 0.02038309    | 0.020511882   | 0.020429289   | 0.020419093   |\n",
    "| **Test RMSE**  | 0.023123872   | 0.023330305   | 0.022973741   | 0.023059431   | 0.023033737   | 0.023013218   |\n",
    "| **Train MAE**  | 0.014739464   | 0.014341673   | 0.014790372   | 0.014811615   | 0.014797508   | 0.01485473    |\n",
    "| **Test MAE**   | 0.017616895   | 0.017620127   | 0.017618449   | 0.017435959   | 0.01742066    | 0.017467919   |\n",
    "| **Train Loss** | 0.337355286   | 0.118890353   | 0.109886877   | 0.110587768   | 0.110946737   | 0.111690558   |\n",
    "| **Val Loss**   | 0.340427399   | 0.157293305   | 0.159300119   | 0.143887952   | 0.131491527   | 0.130892992   |\n",
    "| **Train DA**   | 51%           | 59%           | 62%           | 55%           | 57%           | 56%           |\n",
    "| **Test DA**    | 52%           | 52%           | 51%           | 53%           | 59%           | 58%           |\n",
    "\n",
    "**Key:**\n",
    "- RMSE: Root Mean Squared Error\n",
    "- MAE: Mean Absolute Error\n",
    "- Loss: Training or Validation Loss\n",
    "- DA: Directional Accuracy\n",
    "\n",
    "\n",
    "**The best model for the sequence length is the model 6**, which presented the best overall balance between all score metrics (it has lowest validation loss). The loss plot also shows convergence, but the early stop approach can be improved.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca102d78-c931-4bbc-87c5-6f0a95b58459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting train and val loss:\n",
    "scaled_train_loss = history.history['loss'][-1]  # Final train loss\n",
    "scaled_val_loss = history.history['val_loss'][-1]  # Final validation loss\n",
    "\n",
    "# Descale train and validation loss\n",
    "descaled_train_loss = np.sqrt(scaled_train_loss) * scaler_y.scale_[0] + scaler_y.mean_[0]\n",
    "descaled_val_loss = np.sqrt(scaled_val_loss) * scaler_y.scale_[0] + scaler_y.mean_[0]\n",
    "\n",
    "print(f\"Descaled Training Loss: {descaled_train_loss}\")\n",
    "print(f\"Descaled Validation Loss: {descaled_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cce6230b-dc08-4c95-9e15-dcc39d221555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 111ms/step\n",
      "Fold 1 RMSE: 0.4631680777453844\n",
      "Fold 2\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Fold 2 RMSE: 0.34029729053570823\n",
      "Fold 3\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Fold 3 RMSE: 0.28589442666716053\n",
      "Fold 4\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Fold 4 RMSE: 0.3745123836050248\n",
      "Average RMSE from TSCV: 0.3659680446383195\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = {\n",
    "    'units1': 32,\n",
    "    'units2': 32,\n",
    "    'recurrent_dropout': 0.2,\n",
    "    'optimizer': 'adam',\n",
    "    'loss_function': 'mean_squared_error',\n",
    "    'learning_rate_decay': 1e-05,\n",
    "    'learning_rate': 0.001,\n",
    "    'l2_lambda': 0.07913979537147964,\n",
    "    'epochs': 200,\n",
    "    'dropout_rate': 0.3,\n",
    "    'clipnorm': 5.0,\n",
    "    'batch_size': 64\n",
    "}\n",
    "\n",
    "# Build the model using the best parameters\n",
    "def build_best_model(params):\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units1'], return_sequences=True, \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "                   kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Second LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units2'], return_sequences=False, kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Select the optimizer\n",
    "    optimizer = Adam(learning_rate=params['learning_rate'], \n",
    "                     decay=params['learning_rate_decay'], \n",
    "                     clipnorm=params['clipnorm'])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss=params['loss_function'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define the number of splits for Time Series Cross-Validation\n",
    "n_splits = 4\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Initialize the list to store RMSE scores\n",
    "tscv_rmse_scores = []\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Create the model once and save its initial weights\n",
    "model = build_best_model(best_params)\n",
    "initial_weights = model.get_weights()\n",
    "\n",
    "# Perform TSCV\n",
    "for fold, (train_index, test_index) in enumerate(tscv.split(X_train_scaled)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "    # Reset model to initial weights\n",
    "    model.set_weights(initial_weights)\n",
    "    \n",
    "    # Define train and test sets\n",
    "    train, test = X_train_scaled[train_index], X_train_scaled[test_index]\n",
    "    y_train_fold, y_test_fold = y_train_scaled[train_index], y_train_scaled[test_index]\n",
    "    \n",
    "    # Create sequences\n",
    "    X_train_seq, y_train_seq = [], []\n",
    "    for i in range(len(train) - sequence_length):\n",
    "        X_train_seq.append(train[i:i + sequence_length])\n",
    "        y_train_seq.append(y_train_fold[i + sequence_length])\n",
    "    \n",
    "    X_test_seq, y_test_seq = [], []\n",
    "    for i in range(len(test) - sequence_length):\n",
    "        X_test_seq.append(test[i:i + sequence_length])\n",
    "        y_test_seq.append(y_test_fold[i + sequence_length])\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_train_seq, y_train_seq = np.array(X_train_seq), np.array(y_train_seq)\n",
    "    X_test_seq, y_test_seq = np.array(X_test_seq), np.array(y_test_seq)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=best_params['epochs'],\n",
    "        batch_size=best_params['batch_size'],\n",
    "        validation_data=(X_test_seq, y_test_seq),\n",
    "        shuffle=False,\n",
    "        verbose=0,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = model.predict(X_test_seq)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_seq, y_pred))\n",
    "    tscv_rmse_scores.append(rmse)\n",
    "    print(f\"Fold {fold + 1} RMSE: {rmse}\")\n",
    "\n",
    "# Calculate the average RMSE across all folds\n",
    "avg_rmse = np.mean(tscv_rmse_scores)\n",
    "print(f\"Average RMSE from TSCV: {avg_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b67985da-6a6c-40f5-a562-c3ff5bbbecb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling window starting at index 0\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 551ms/step\n",
      "Rolling window RMSE: 0.43233682281389846\n",
      "Rolling window starting at index 50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 644ms/stepWARNING:tensorflow:5 out of the last 23 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000182578E4860> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 722ms/step\n",
      "Rolling window RMSE: 0.43825290430280806\n",
      "Rolling window starting at index 100\n",
      "WARNING:tensorflow:6 out of the last 24 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001824DF871A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 573ms/step\n",
      "Rolling window RMSE: 0.3338610150338942\n",
      "Rolling window starting at index 150\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 668ms/step\n",
      "Rolling window RMSE: 0.322371106879519\n",
      "Rolling window starting at index 200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 668ms/step\n",
      "Rolling window RMSE: 0.2903645218373225\n",
      "Rolling window starting at index 250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 613ms/step\n",
      "Rolling window RMSE: 0.3023809915135637\n",
      "Rolling window starting at index 300\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n",
      "Rolling window RMSE: 0.3096041760165966\n",
      "Rolling window starting at index 350\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 578ms/step\n",
      "Rolling window RMSE: 0.25625313410909284\n",
      "Rolling window starting at index 400\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 532ms/step\n",
      "Rolling window RMSE: 0.2936105590603675\n",
      "Rolling window starting at index 450\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 512ms/step\n",
      "Rolling window RMSE: 0.3808265308223538\n",
      "Rolling window starting at index 500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694ms/step\n",
      "Rolling window RMSE: 0.47406872043776144\n",
      "Average Rolling Window RMSE: 0.34853913480247073\n"
     ]
    }
   ],
   "source": [
    "# Parameters for FRWCV\n",
    "train_window = 300  # Training window size\n",
    "test_window = 50    # Test window size\n",
    "\n",
    "# Store RMSEs for each window\n",
    "rolling_rmse_scores = []\n",
    "\n",
    "for start in range(0, len(X_train_scaled) - train_window - test_window, test_window):\n",
    "    print(f\"Rolling window starting at index {start}\")\n",
    "    \n",
    "    # Define train and test sets for the window\n",
    "    train = X_train_scaled[start:start + train_window]\n",
    "    test = X_train_scaled[start + train_window:start + train_window + test_window]\n",
    "    y_train_fold = y_train_scaled[start:start + train_window]\n",
    "    y_test_fold = y_train_scaled[start + train_window:start + train_window + test_window]\n",
    "    \n",
    "    # Create sequences for train and test\n",
    "    X_train_seq, y_train_seq = [], []\n",
    "    for i in range(len(train) - sequence_length):\n",
    "        X_train_seq.append(train[i:i + sequence_length])\n",
    "        y_train_seq.append(y_train_fold[i + sequence_length])\n",
    "    \n",
    "    X_test_seq, y_test_seq = [], []\n",
    "    for i in range(len(test) - sequence_length):\n",
    "        X_test_seq.append(test[i:i + sequence_length])\n",
    "        y_test_seq.append(y_test_fold[i + sequence_length])\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_train_seq, y_train_seq = np.array(X_train_seq), np.array(y_train_seq)\n",
    "    X_test_seq, y_test_seq = np.array(X_test_seq), np.array(y_test_seq)\n",
    "    \n",
    "    # Build and train the model\n",
    "    model = build_best_model(best_params)\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq, \n",
    "        epochs=best_params['epochs'], \n",
    "        batch_size=best_params['batch_size'], \n",
    "        validation_data=(X_test_seq, y_test_seq), \n",
    "        shuffle=False,\n",
    "        verbose=0,  # Suppress training logs for brevity\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    y_pred = model.predict(X_test_seq)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_seq, y_pred))\n",
    "    rolling_rmse_scores.append(rmse)\n",
    "    print(f\"Rolling window RMSE: {rmse}\")\n",
    "\n",
    "# Calculate average RMSE across rolling windows\n",
    "avg_rolling_rmse = np.mean(rolling_rmse_scores)\n",
    "print(f\"Average Rolling Window RMSE: {avg_rolling_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfab31a-f897-4ce3-8d27-61c172c6b840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to descale RMSE\n",
    "def descale_rmse(scaled_rmse, scaler_y):\n",
    "    # Reshape RMSE value to fit scaler requirements\n",
    "    scaled_rmse_array = np.array(scaled_rmse).reshape(-1, 1)\n",
    "    # Inverse transform the RMSE\n",
    "    descaled_rmse_array = scaler_y.inverse_transform(scaled_rmse_array)\n",
    "    # Return the descaled RMSE as a list\n",
    "    return descaled_rmse_array.flatten().tolist()\n",
    "\n",
    "# Descale TSCV RMSE values\n",
    "descaled_tscv_rmse = descale_rmse(tscv_rmse_scores, scaler_y)\n",
    "print(\"Descaled TSCV RMSE Values:\", descaled_tscv_rmse)\n",
    "\n",
    "# Descale FRWCV RMSE values\n",
    "descaled_frwcv_rmse = descale_rmse(rolling_rmse_scores, scaler_y)\n",
    "print(\"Descaled FRWCV RMSE Values:\", descaled_frwcv_rmse)\n",
    "\n",
    "# Calculates average descaled RMSE\n",
    "average_tscv_rmse = np.mean(descaled_tscv_rmse)\n",
    "average_frwcv_rmse = np.mean(descaled_frwcv_rmse)\n",
    "print(f\"Average Descaled TSCV RMSE: {average_tscv_rmse}\")\n",
    "print(f\"Average Descaled FRWCV RMSE: {average_frwcv_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8a6ed1-811b-4ce6-8936-b88bbe6c4251",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sequence Length = 12 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "609ab9dd-c518-4e18-b58e-eb805be84183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped X_train_seq shape: (864, 12, 5)\n",
      "Reshaped y_train_seq shape: (864, 1)\n",
      "Reshaped X_test_seq shape: (143, 12, 5)\n",
      "Reshaped  y_test_seq shape: (143, 1)\n"
     ]
    }
   ],
   "source": [
    "# Set the sequence length for LSTM input, representing the number of time steps.\n",
    "sequence_length = 12  # Number of time steps used for target prediction, 12 weeks (Quarterly).\n",
    "\n",
    "# Reshape data into sequences for LSTM.\n",
    "# Initialize empty lists to hold the sequences for the training set.\n",
    "X_train_seq, y_train_seq = [], []\n",
    "# Loop through the training data to create sequences of features and corresponding target values.\n",
    "for i in range(len(X_train_scaled) - sequence_length):\n",
    "    # Append a sequence of features for the current window.\n",
    "    X_train_seq.append(X_train_scaled[i:i + sequence_length]) # Sequence of features.\n",
    "    # Append the target value that comes immediately after the sequence.\n",
    "    y_train_seq.append(y_train_scaled[i + sequence_length]) # Target value following the sequence.\n",
    "# Initialize empty lists to hold the sequences for the test set    \n",
    "X_test_seq, y_test_seq = [], []\n",
    "# Same process for the test sets.\n",
    "for i in range(len(X_test_scaled) - sequence_length):\n",
    "    # Append a sequence of features for the current window in the test set.\n",
    "    X_test_seq.append(X_test_scaled[i:i + sequence_length])\n",
    "    # Append the target value immediately after the sequence in the test set.\n",
    "    y_test_seq.append(y_test_scaled[i + sequence_length])\n",
    "\n",
    "# Convert lists to numpy arrays to use in Neural Networks.\n",
    "X_train_seq, y_train_seq = np.array(X_train_seq), np.array(y_train_seq)\n",
    "X_test_seq, y_test_seq = np.array(X_test_seq), np.array(y_test_seq)\n",
    "\n",
    "# Print the reshaped input data for LSTM.\n",
    "print(f\"Reshaped X_train_seq shape: {X_train_seq.shape}\")\n",
    "print(f\"Reshaped y_train_seq shape: {y_train_seq.shape}\")\n",
    "print(f\"Reshaped X_test_seq shape: {X_test_seq.shape}\")\n",
    "print(f\"Reshaped  y_test_seq shape: { y_test_seq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b18c230-3ecf-4ad5-8bba-d85c81631ee8",
   "metadata": {},
   "source": [
    "### 1. Random Search (Two Layers - Model 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ba37589-3c7f-4efe-a1ef-d7ce04ddb6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running combination 1/30: {'units2': 32, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 64}\n",
      "Final Training Loss: 0.14561107754707336\n",
      "Final Validation Loss: 0.17564596235752106\n",
      "Running combination 2/30: {'units2': 128, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 0.4619697630405426\n",
      "Final Validation Loss: 0.28401193022727966\n",
      "Running combination 3/30: {'units2': 64, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 64}\n",
      "Final Training Loss: 0.9323776364326477\n",
      "Final Validation Loss: 0.3091294467449188\n",
      "Running combination 4/30: {'units2': 64, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 50, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 32}\n",
      "Final Training Loss: 5.9093852043151855\n",
      "Final Validation Loss: 5.068639278411865\n",
      "Running combination 5/30: {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 5.0, 'batch_size': 64}\n",
      "Final Training Loss: 1.9531461000442505\n",
      "Final Validation Loss: 1.3231948614120483\n",
      "Running combination 6/30: {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 120}\n",
      "Final Training Loss: 0.7079055309295654\n",
      "Final Validation Loss: 0.1907939314842224\n",
      "Running combination 7/30: {'units2': 32, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 64}\n",
      "Final Training Loss: 9.113693237304688\n",
      "Final Validation Loss: 8.24172306060791\n",
      "Running combination 8/30: {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 50, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 0.9740474820137024\n",
      "Final Validation Loss: 0.6010956764221191\n",
      "Running combination 9/30: {'units2': 64, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 1.879317283630371\n",
      "Final Validation Loss: 1.228506326675415\n",
      "Running combination 10/30: {'units2': 32, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 0.42527034878730774\n",
      "Final Validation Loss: 0.22532005608081818\n",
      "Running combination 11/30: {'units2': 64, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 32}\n",
      "Final Training Loss: 5.466570854187012\n",
      "Final Validation Loss: 4.928784370422363\n",
      "Running combination 12/30: {'units2': 32, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 0.7566550374031067\n",
      "Final Validation Loss: 0.19439268112182617\n",
      "Running combination 13/30: {'units2': 128, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 3.2982325553894043\n",
      "Final Validation Loss: 2.4048163890838623\n",
      "Running combination 14/30: {'units2': 64, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 256}\n",
      "Final Training Loss: 0.7619476914405823\n",
      "Final Validation Loss: 0.21591731905937195\n",
      "Running combination 15/30: {'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 32}\n",
      "Final Training Loss: 0.7896448969841003\n",
      "Final Validation Loss: 0.3095945715904236\n",
      "Running combination 16/30: {'units2': 32, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 256}\n",
      "Final Training Loss: 1.6993787288665771\n",
      "Final Validation Loss: 1.042978286743164\n",
      "Running combination 17/30: {'units2': 128, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 5.0, 'batch_size': 120}\n",
      "Final Training Loss: 10.31450080871582\n",
      "Final Validation Loss: 8.837921142578125\n",
      "Running combination 18/30: {'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 200, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 13.013269424438477\n",
      "Final Validation Loss: 12.291603088378906\n",
      "Running combination 19/30: {'units2': 128, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 18.53643226623535\n",
      "Final Validation Loss: 17.872623443603516\n",
      "Running combination 20/30: {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 1.5862756967544556\n",
      "Final Validation Loss: 0.821032702922821\n",
      "Running combination 21/30: {'units2': 32, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.001, 'l2_lambda': 0.01, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 0.47930166125297546\n",
      "Final Validation Loss: 0.3593480587005615\n",
      "Running combination 22/30: {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.01, 'epochs': 50, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 1.170853853225708\n",
      "Final Validation Loss: 0.6115163564682007\n",
      "Running combination 23/30: {'units2': 32, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 50, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 6.098565101623535\n",
      "Final Validation Loss: 5.138681888580322\n",
      "Running combination 24/30: {'units2': 64, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 2.2295944690704346\n",
      "Final Validation Loss: 1.7960107326507568\n",
      "Running combination 25/30: {'units2': 32, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 256}\n",
      "Final Training Loss: 13.969255447387695\n",
      "Final Validation Loss: 13.124198913574219\n",
      "Running combination 26/30: {'units2': 32, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 64}\n",
      "Final Training Loss: 1.4126367568969727\n",
      "Final Validation Loss: 0.6712388396263123\n",
      "Running combination 27/30: {'units2': 64, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 1.667602777481079\n",
      "Final Validation Loss: 0.34695446491241455\n",
      "Running combination 28/30: {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 0.5088276267051697\n",
      "Final Validation Loss: 0.2204194962978363\n",
      "Running combination 29/30: {'units2': 128, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0001, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 256}\n",
      "Final Training Loss: 1.148106575012207\n",
      "Final Validation Loss: 0.2862793505191803\n",
      "Running combination 30/30: {'units2': 32, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 200, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 64}\n",
      "Final Training Loss: 4.477337837219238\n",
      "Final Validation Loss: 3.919060707092285\n",
      "Top results:\n",
      "{'params': {'units2': 32, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 64}, 'final_train_loss': 0.14561107754707336, 'final_val_loss': 0.17564596235752106}\n",
      "{'params': {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 120}, 'final_train_loss': 0.7079055309295654, 'final_val_loss': 0.1907939314842224}\n",
      "{'params': {'units2': 32, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 120}, 'final_train_loss': 0.7566550374031067, 'final_val_loss': 0.19439268112182617}\n",
      "{'params': {'units2': 64, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 256}, 'final_train_loss': 0.7619476914405823, 'final_val_loss': 0.21591731905937195}\n",
      "{'params': {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 32}, 'final_train_loss': 0.5088276267051697, 'final_val_loss': 0.2204194962978363}\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Define parameter grid for random hyperparameter search.\n",
    "param_grid = {\n",
    "    'dropout_rate': [0.2, 0.3, 0.4],               # Dropout rate for regularization.\n",
    "    'recurrent_dropout': [0.1, 0.2],               # Recurrent dropout within LSTM layers.\n",
    "    'l2_lambda': [0.001, 0.01, 0.1],               # L2 regularization strength.\n",
    "    'learning_rate': [0.001, 0.0005, 0.0001],      # Learning rate for optimizer.\n",
    "    'learning_rate_decay': [1e-6, 1e-5, 0],        # Decay rate for learning rate over time.\n",
    "    'units1': [32, 64, 128],                       # Number of units in the first LSTM layer.\n",
    "    'units2': [32, 64, 128],                       # Number of units in the second LSTM layer.\n",
    "    'batch_size': [32, 64, 120, 256],              # Batch size for training.\n",
    "    'epochs': [50, 100, 200],                      # Number of epochs to train.\n",
    "    'optimizer': ['adam'],                         # Optimizer to use.\n",
    "    'clipnorm': [1.0, 5.0]                         # Gradient clipping to avoid exploding gradients.\n",
    "}\n",
    "\n",
    "# Generate a list of random combinations of hyperparameters.\n",
    "n_iter_search = 30  # Number of random combinations to attempt.\n",
    "# Random state being applied for reproducibility. \n",
    "random_combinations = list(ParameterSampler(param_grid, n_iter=n_iter_search, random_state=42)) \n",
    "\n",
    "# Define a function to build the LSTM model with given parameters.\n",
    "def build_model(dropout_rate, recurrent_dropout, l2_lambda, learning_rate, learning_rate_decay, clipnorm, units1, units2, optimizer_name, loss_function):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First LSTM layer with BatchNorm, Dropout, and recurrent_dropout.\n",
    "    model.add(LSTM(units=units1, return_sequences=True, \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),  # Input shape based on sequence data.\n",
    "                   kernel_regularizer=l2(l2_lambda), recurrent_dropout=recurrent_dropout))\n",
    "    model.add(BatchNormalization())  # Batch normalization for stable training.\n",
    "    model.add(Dropout(dropout_rate))  # Dropout for regularization.\n",
    "    \n",
    "    # Second LSTM layer with BatchNorm, Dropout, and recurrent_dropout.\n",
    "    model.add(LSTM(units=units2, return_sequences=False, kernel_regularizer=l2(l2_lambda), recurrent_dropout=recurrent_dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer with a single unit for regression output.\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Select optimizer with learning rate decay and gradient clipping.\n",
    "    optimizer = Adam(learning_rate=learning_rate, decay=learning_rate_decay, clipnorm=clipnorm)\n",
    "    \n",
    "    # Compile the model with the chosen optimizer and loss function.\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define early stopping to stop training when validation loss does not improve.\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Placeholder to store results of each model configuration.\n",
    "results = []\n",
    "\n",
    "# Run random search\n",
    "for i, params in enumerate(random_combinations):\n",
    "    print(f\"Running combination {i+1}/{len(random_combinations)}: {params}\")\n",
    "    \n",
    "    # Build model with the current parameters\n",
    "    model = build_model(\n",
    "        dropout_rate=params['dropout_rate'],\n",
    "        recurrent_dropout=params['recurrent_dropout'],\n",
    "        l2_lambda=params['l2_lambda'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        learning_rate_decay=params['learning_rate_decay'],\n",
    "        clipnorm=params['clipnorm'],\n",
    "        units1=params['units1'],\n",
    "        units2=params['units2'],\n",
    "        optimizer_name=params['optimizer'],\n",
    "        loss_function='mean_squared_error'\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq, \n",
    "        epochs=params['epochs'], \n",
    "        batch_size=params['batch_size'], \n",
    "        validation_data=(X_test_seq, y_test_seq), \n",
    "        callbacks=[early_stopping],\n",
    "        shuffle=False,\n",
    "        verbose=0  # Set verbose=0 to reduce output\n",
    "    )\n",
    "    \n",
    "    # Get final validation loss and training loss\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    \n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'params': params,\n",
    "        'final_train_loss': final_train_loss,\n",
    "        'final_val_loss': final_val_loss\n",
    "    })\n",
    "    \n",
    "    print(f\"Final Training Loss: {final_train_loss}\")\n",
    "    print(f\"Final Validation Loss: {final_val_loss}\")\n",
    "\n",
    "results = sorted(results, key=lambda x: x['final_val_loss'])\n",
    "print(\"Top results:\")\n",
    "for res in results[:5]:  # Show top 5 results\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4166cedc-145a-44a9-895b-e4db52df5c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 281ms/step - loss: 3.0575 - val_loss: 0.7773\n",
      "Epoch 2/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 2.2273 - val_loss: 0.7678\n",
      "Epoch 3/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.8906 - val_loss: 0.7495\n",
      "Epoch 4/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.8049 - val_loss: 0.7394\n",
      "Epoch 5/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.5532 - val_loss: 0.7305\n",
      "Epoch 6/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.7807 - val_loss: 0.7223\n",
      "Epoch 7/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 1.5988 - val_loss: 0.7145\n",
      "Epoch 8/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.3658 - val_loss: 0.7062\n",
      "Epoch 9/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 1.4922 - val_loss: 0.6979\n",
      "Epoch 10/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.4402 - val_loss: 0.6930\n",
      "Epoch 11/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 1.3451 - val_loss: 0.6834\n",
      "Epoch 12/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 1.3051 - val_loss: 0.6758\n",
      "Epoch 13/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 1.2483 - val_loss: 0.6693\n",
      "Epoch 14/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 1.2219 - val_loss: 0.6659\n",
      "Epoch 15/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 1.1584 - val_loss: 0.6585\n",
      "Epoch 16/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 1.2063 - val_loss: 0.6507\n",
      "Epoch 17/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1.2283 - val_loss: 0.6423\n",
      "Epoch 18/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.2106 - val_loss: 0.6349\n",
      "Epoch 19/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.1166 - val_loss: 0.6279\n",
      "Epoch 20/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.0998 - val_loss: 0.6236\n",
      "Epoch 21/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.0706 - val_loss: 0.6236\n",
      "Epoch 22/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.0395 - val_loss: 0.6260\n",
      "Epoch 23/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.9901 - val_loss: 0.6180\n",
      "Epoch 24/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.9537 - val_loss: 0.6043\n",
      "Epoch 25/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.0567 - val_loss: 0.5911\n",
      "Epoch 26/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.9904 - val_loss: 0.5825\n",
      "Epoch 27/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.9652 - val_loss: 0.5791\n",
      "Epoch 28/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.8960 - val_loss: 0.5735\n",
      "Epoch 29/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.9372 - val_loss: 0.5661\n",
      "Epoch 30/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.8417 - val_loss: 0.5592\n",
      "Epoch 31/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.9223 - val_loss: 0.5556\n",
      "Epoch 32/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.8651 - val_loss: 0.5483\n",
      "Epoch 33/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.8287 - val_loss: 0.5462\n",
      "Epoch 34/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.8179 - val_loss: 0.5389\n",
      "Epoch 35/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.8022 - val_loss: 0.5325\n",
      "Epoch 36/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.7540 - val_loss: 0.5262\n",
      "Epoch 37/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.7872 - val_loss: 0.5200\n",
      "Epoch 38/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.7428 - val_loss: 0.5130\n",
      "Epoch 39/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.7637 - val_loss: 0.5067\n",
      "Epoch 40/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.6778 - val_loss: 0.5027\n",
      "Epoch 41/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.7198 - val_loss: 0.5058\n",
      "Epoch 42/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.7101 - val_loss: 0.4946\n",
      "Epoch 43/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.7119 - val_loss: 0.4907\n",
      "Epoch 44/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.6873 - val_loss: 0.4893\n",
      "Epoch 45/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.6571 - val_loss: 0.4880\n",
      "Epoch 46/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.6486 - val_loss: 0.4879\n",
      "Epoch 47/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.6929 - val_loss: 0.4849\n",
      "Epoch 48/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.6348 - val_loss: 0.4817\n",
      "Epoch 49/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.5957 - val_loss: 0.4779\n",
      "Epoch 50/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.6310 - val_loss: 0.4767\n",
      "Epoch 51/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.5908 - val_loss: 0.4760\n",
      "Epoch 52/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.5837 - val_loss: 0.4694\n",
      "Epoch 53/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.5505 - val_loss: 0.4666\n",
      "Epoch 54/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.5573 - val_loss: 0.4590\n",
      "Epoch 55/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.5482 - val_loss: 0.4540\n",
      "Epoch 56/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.5367 - val_loss: 0.4408\n",
      "Epoch 57/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.5198 - val_loss: 0.4334\n",
      "Epoch 58/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.5534 - val_loss: 0.4285\n",
      "Epoch 59/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.4926 - val_loss: 0.4251\n",
      "Epoch 60/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.4904 - val_loss: 0.4227\n",
      "Epoch 61/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.4805 - val_loss: 0.4160\n",
      "Epoch 62/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.4897 - val_loss: 0.4113\n",
      "Epoch 63/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.4611 - val_loss: 0.4083\n",
      "Epoch 64/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.4619 - val_loss: 0.4058\n",
      "Epoch 65/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.4622 - val_loss: 0.4019\n",
      "Epoch 66/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.4517 - val_loss: 0.3963\n",
      "Epoch 67/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.4509 - val_loss: 0.3901\n",
      "Epoch 68/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.4531 - val_loss: 0.3888\n",
      "Epoch 69/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.4616 - val_loss: 0.3842\n",
      "Epoch 70/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.4324 - val_loss: 0.3782\n",
      "Epoch 71/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.4077 - val_loss: 0.3734\n",
      "Epoch 72/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.4300 - val_loss: 0.3720\n",
      "Epoch 73/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.4078 - val_loss: 0.3725\n",
      "Epoch 74/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3946 - val_loss: 0.3718\n",
      "Epoch 75/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3813 - val_loss: 0.3674\n",
      "Epoch 76/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3955 - val_loss: 0.3629\n",
      "Epoch 77/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3804 - val_loss: 0.3602\n",
      "Epoch 78/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3669 - val_loss: 0.3584\n",
      "Epoch 79/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3635 - val_loss: 0.3538\n",
      "Epoch 80/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.3722 - val_loss: 0.3504\n",
      "Epoch 81/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.3593 - val_loss: 0.3472\n",
      "Epoch 82/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.3546 - val_loss: 0.3441\n",
      "Epoch 83/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3423 - val_loss: 0.3429\n",
      "Epoch 84/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3442 - val_loss: 0.3393\n",
      "Epoch 85/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.3552 - val_loss: 0.3349\n",
      "Epoch 86/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.3589 - val_loss: 0.3333\n",
      "Epoch 87/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3343 - val_loss: 0.3318\n",
      "Epoch 88/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3130 - val_loss: 0.3292\n",
      "Epoch 89/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3258 - val_loss: 0.3263\n",
      "Epoch 90/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.3009 - val_loss: 0.3206\n",
      "Epoch 91/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.3193 - val_loss: 0.3156\n",
      "Epoch 92/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.3086 - val_loss: 0.3141\n",
      "Epoch 93/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.3087 - val_loss: 0.3107\n",
      "Epoch 94/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.2980 - val_loss: 0.3101\n",
      "Epoch 95/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.3064 - val_loss: 0.3084\n",
      "Epoch 96/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2974 - val_loss: 0.3060\n",
      "Epoch 97/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2979 - val_loss: 0.3034\n",
      "Epoch 98/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2935 - val_loss: 0.3026\n",
      "Epoch 99/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2850 - val_loss: 0.2996\n",
      "Epoch 100/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2797 - val_loss: 0.2968\n",
      "Epoch 101/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2694 - val_loss: 0.2955\n",
      "Epoch 102/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.2734 - val_loss: 0.2922\n",
      "Epoch 103/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2682 - val_loss: 0.2884\n",
      "Epoch 104/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2564 - val_loss: 0.2866\n",
      "Epoch 105/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2603 - val_loss: 0.2870\n",
      "Epoch 106/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2568 - val_loss: 0.2844\n",
      "Epoch 107/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2547 - val_loss: 0.2823\n",
      "Epoch 108/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.2473 - val_loss: 0.2798\n",
      "Epoch 109/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2467 - val_loss: 0.2768\n",
      "Epoch 110/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2475 - val_loss: 0.2743\n",
      "Epoch 111/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2496 - val_loss: 0.2737\n",
      "Epoch 112/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.2496 - val_loss: 0.2716\n",
      "Epoch 113/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2419 - val_loss: 0.2690\n",
      "Epoch 114/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.2390 - val_loss: 0.2666\n",
      "Epoch 115/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2398 - val_loss: 0.2660\n",
      "Epoch 116/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2345 - val_loss: 0.2648\n",
      "Epoch 117/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2316 - val_loss: 0.2624\n",
      "Epoch 118/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2256 - val_loss: 0.2609\n",
      "Epoch 119/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2237 - val_loss: 0.2580\n",
      "Epoch 120/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2296 - val_loss: 0.2533\n",
      "Epoch 121/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2217 - val_loss: 0.2523\n",
      "Epoch 122/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.2154 - val_loss: 0.2531\n",
      "Epoch 123/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2158 - val_loss: 0.2528\n",
      "Epoch 124/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.2235 - val_loss: 0.2507\n",
      "Epoch 125/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2106 - val_loss: 0.2486\n",
      "Epoch 126/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.2093 - val_loss: 0.2465\n",
      "Epoch 127/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2118 - val_loss: 0.2450\n",
      "Epoch 128/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.2127 - val_loss: 0.2426\n",
      "Epoch 129/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.2076 - val_loss: 0.2406\n",
      "Epoch 130/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.2122 - val_loss: 0.2393\n",
      "Epoch 131/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2008 - val_loss: 0.2374\n",
      "Epoch 132/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1997 - val_loss: 0.2357\n",
      "Epoch 133/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.1960 - val_loss: 0.2332\n",
      "Epoch 134/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2025 - val_loss: 0.2310\n",
      "Epoch 135/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1971 - val_loss: 0.2296\n",
      "Epoch 136/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1952 - val_loss: 0.2286\n",
      "Epoch 137/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1920 - val_loss: 0.2264\n",
      "Epoch 138/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1927 - val_loss: 0.2254\n",
      "Epoch 139/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.1860 - val_loss: 0.2263\n",
      "Epoch 140/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.1922 - val_loss: 0.2262\n",
      "Epoch 141/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1874 - val_loss: 0.2261\n",
      "Epoch 142/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.1869 - val_loss: 0.2243\n",
      "Epoch 143/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1865 - val_loss: 0.2235\n",
      "Epoch 144/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1860 - val_loss: 0.2210\n",
      "Epoch 145/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1804 - val_loss: 0.2199\n",
      "Epoch 146/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1813 - val_loss: 0.2178\n",
      "Epoch 147/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1822 - val_loss: 0.2166\n",
      "Epoch 148/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1757 - val_loss: 0.2161\n",
      "Epoch 149/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1766 - val_loss: 0.2154\n",
      "Epoch 150/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1758 - val_loss: 0.2140\n",
      "Epoch 151/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1738 - val_loss: 0.2117\n",
      "Epoch 152/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.1751 - val_loss: 0.2107\n",
      "Epoch 153/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1771 - val_loss: 0.2098\n",
      "Epoch 154/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1740 - val_loss: 0.2094\n",
      "Epoch 155/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1713 - val_loss: 0.2095\n",
      "Epoch 156/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.1697 - val_loss: 0.2085\n",
      "Epoch 157/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.1691 - val_loss: 0.2071\n",
      "Epoch 158/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1683 - val_loss: 0.2056\n",
      "Epoch 159/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1674 - val_loss: 0.2046\n",
      "Epoch 160/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.1665 - val_loss: 0.2030\n",
      "Epoch 161/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1632 - val_loss: 0.2013\n",
      "Epoch 162/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1627 - val_loss: 0.2002\n",
      "Epoch 163/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.1639 - val_loss: 0.1994\n",
      "Epoch 164/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1598 - val_loss: 0.1988\n",
      "Epoch 165/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1618 - val_loss: 0.1990\n",
      "Epoch 166/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1604 - val_loss: 0.1985\n",
      "Epoch 167/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1598 - val_loss: 0.1967\n",
      "Epoch 168/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.1603 - val_loss: 0.1960\n",
      "Epoch 169/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1567 - val_loss: 0.1959\n",
      "Epoch 170/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1562 - val_loss: 0.1944\n",
      "Epoch 171/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1562 - val_loss: 0.1931\n",
      "Epoch 172/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1537 - val_loss: 0.1914\n",
      "Epoch 173/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1537 - val_loss: 0.1906\n",
      "Epoch 174/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1510 - val_loss: 0.1919\n",
      "Epoch 175/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1544 - val_loss: 0.1916\n",
      "Epoch 176/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1496 - val_loss: 0.1913\n",
      "Epoch 177/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1517 - val_loss: 0.1915\n",
      "Epoch 178/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1486 - val_loss: 0.1912\n",
      "Epoch 179/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1496 - val_loss: 0.1887\n",
      "Epoch 180/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.1506 - val_loss: 0.1884\n",
      "Epoch 181/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1478 - val_loss: 0.1887\n",
      "Epoch 182/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1486 - val_loss: 0.1869\n",
      "Epoch 183/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1476 - val_loss: 0.1858\n",
      "Epoch 184/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1483 - val_loss: 0.1853\n",
      "Epoch 185/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1470 - val_loss: 0.1850\n",
      "Epoch 186/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1456 - val_loss: 0.1842\n",
      "Epoch 187/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1435 - val_loss: 0.1834\n",
      "Epoch 188/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1452 - val_loss: 0.1824\n",
      "Epoch 189/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1463 - val_loss: 0.1806\n",
      "Epoch 190/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1436 - val_loss: 0.1797\n",
      "Epoch 191/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1430 - val_loss: 0.1797\n",
      "Epoch 192/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1426 - val_loss: 0.1803\n",
      "Epoch 193/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1393 - val_loss: 0.1805\n",
      "Epoch 194/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1402 - val_loss: 0.1812\n",
      "Epoch 195/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1395 - val_loss: 0.1795\n",
      "Epoch 196/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1392 - val_loss: 0.1769\n",
      "Epoch 197/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1394 - val_loss: 0.1768\n",
      "Epoch 198/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1407 - val_loss: 0.1773\n",
      "Epoch 199/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1390 - val_loss: 0.1764\n",
      "Epoch 200/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1371 - val_loss: 0.1756\n",
      "Final Training Loss: 0.14561107754707336\n",
      "Final Validation Loss: 0.17564596235752106\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Best hyperparameters found from tuning\n",
    "best_params = {\n",
    "    'units1': 32,\n",
    "    'units2': 32,\n",
    "    'recurrent_dropout': 0.2,\n",
    "    'optimizer': 'adam',\n",
    "    'loss_function': 'mean_squared_error',\n",
    "    'learning_rate_decay': 1e-05,\n",
    "    'learning_rate': 0.0005,\n",
    "    'l2_lambda': 0.01,\n",
    "    'epochs': 200,\n",
    "    'dropout_rate': 0.3,\n",
    "    'clipnorm': 5.0,\n",
    "    'batch_size': 64\n",
    "}\n",
    "\n",
    "# Build the model using the best parameters\n",
    "def build_best_model(params):\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units1'], return_sequences=True, \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "                   kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Second LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units2'], return_sequences=False, kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Select the optimizer\n",
    "    if params['optimizer'] == 'adam':\n",
    "        optimizer = Adam(learning_rate=params['learning_rate'], decay=params['learning_rate_decay'], clipnorm=params['clipnorm'])\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizer '{params['optimizer']}' is not implemented in this setup.\")\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate the model with the best hyperparameters\n",
    "model = build_best_model(best_params)\n",
    "\n",
    "# Early stopping to stop training when validation loss stops improving\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq, \n",
    "    epochs=best_params['epochs'], \n",
    "    batch_size=best_params['batch_size'], \n",
    "    validation_data=(X_test_seq, y_test_seq),\n",
    "    shuffle=False,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Print the final training and validation loss\n",
    "print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4708d6a8-cfcf-449f-bab2-239b050caa69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 492ms/step\n",
      "LSTM Model Performance:\n",
      "Training RMSE: 0.020650653082081912\n",
      "Test RMSE: 0.02398720453603505\n",
      "Training MAE: 0.014889153505638218\n",
      "Test MAE: 0.018062194723975394\n",
      "Directional Accuracy on Training Data: 59.09617612977984%\n",
      "Directional Accuracy on Test Data: 58.45070422535211%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict on both training and test sequences\n",
    "y_train_pred = model.predict(X_train_seq)\n",
    "y_test_pred = model.predict(X_test_seq)\n",
    "\n",
    "# Inverse scale predictions and actual values to original scale\n",
    "y_train_pred_rescaled = scaler_y.inverse_transform(y_train_pred)\n",
    "y_test_pred_rescaled = scaler_y.inverse_transform(y_test_pred)\n",
    "y_train_actual_rescaled = scaler_y.inverse_transform(y_train_seq)\n",
    "y_test_actual_rescaled = scaler_y.inverse_transform(y_test_seq)\n",
    "\n",
    "# Define a function for directional accuracy\n",
    "def directional_accuracy(y_true, y_pred):\n",
    "    return np.mean((np.sign(y_true[1:] - y_true[:-1]) == np.sign(y_pred[1:] - y_pred[:-1])).astype(int)) * 100\n",
    "\n",
    "# Calculate metrics.\n",
    "# Root Mean Squared Error (RMSE) for training set\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_actual_rescaled, y_train_pred_rescaled))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_actual_rescaled, y_test_pred_rescaled)) # RMSE for test set.\n",
    "train_mae = mean_absolute_error(y_train_actual_rescaled, y_train_pred_rescaled) # Mean Absolute Error (MAE) for training set.\n",
    "test_mae = mean_absolute_error(y_test_actual_rescaled, y_test_pred_rescaled) # MAE for test set.\n",
    "# Directional accuracy for training set.\n",
    "train_directional_accuracy = directional_accuracy(y_train_actual_rescaled.flatten(), y_train_pred_rescaled.flatten())\n",
    "# Directional accuracy for test set.\n",
    "test_directional_accuracy = directional_accuracy(y_test_actual_rescaled.flatten(), y_test_pred_rescaled.flatten())\n",
    "\n",
    "# Display score metrics to evaluate the model's performance.\n",
    "print(\"LSTM Model Performance:\")\n",
    "print(f\"Training RMSE: {train_rmse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Training MAE: {train_mae}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Directional Accuracy on Training Data: {train_directional_accuracy}%\")\n",
    "print(f\"Directional Accuracy on Test Data: {test_directional_accuracy}%\")\n",
    "\n",
    "# Plot training & validation loss values, it will help visualize how well the model fits the data over training epochs.\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5cbab7-1f83-48a9-9c93-57187f26003a",
   "metadata": {},
   "source": [
    "### Three Layers (Model 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0438877e-9d9a-4a2e-a251-10d79763d62e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running combination 1/30: {'units3': 32, 'units2': 32, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 32}\n",
      "Final Training Loss: 0.20096047222614288\n",
      "Final Validation Loss: 0.35382428765296936\n",
      "Running combination 2/30: {'units3': 128, 'units2': 64, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 3.2474517822265625\n",
      "Final Validation Loss: 2.83465576171875\n",
      "Running combination 3/30: {'units3': 128, 'units2': 64, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.4, 'clipnorm': 5.0, 'batch_size': 256}\n",
      "Final Training Loss: 1.6059890985488892\n",
      "Final Validation Loss: 0.37392526865005493\n",
      "Running combination 4/30: {'units3': 128, 'units2': 64, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 5.0, 'batch_size': 120}\n",
      "Final Training Loss: 6.582381248474121\n",
      "Final Validation Loss: 5.431582450866699\n",
      "Running combination 5/30: {'units3': 64, 'units2': 64, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 3.4679322242736816\n",
      "Final Validation Loss: 2.8858859539031982\n",
      "Running combination 6/30: {'units3': 64, 'units2': 32, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 1.3604249954223633\n",
      "Final Validation Loss: 0.29776692390441895\n",
      "Running combination 7/30: {'units3': 32, 'units2': 32, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 256}\n",
      "Final Training Loss: 2.040759563446045\n",
      "Final Validation Loss: 1.5112055540084839\n",
      "Running combination 8/30: {'units3': 32, 'units2': 128, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 64}\n",
      "Final Training Loss: 3.4746267795562744\n",
      "Final Validation Loss: 2.35593843460083\n",
      "Running combination 9/30: {'units3': 32, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 256}\n",
      "Final Training Loss: 1.34364914894104\n",
      "Final Validation Loss: 0.33425429463386536\n",
      "Running combination 10/30: {'units3': 64, 'units2': 64, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0001, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 64}\n",
      "Final Training Loss: 1.3359824419021606\n",
      "Final Validation Loss: 0.34818336367607117\n",
      "Running combination 11/30: {'units3': 128, 'units2': 128, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 1.7883899211883545\n",
      "Final Validation Loss: 0.5498001575469971\n",
      "Running combination 12/30: {'units3': 32, 'units2': 64, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 32}\n",
      "Final Training Loss: 15.18642520904541\n",
      "Final Validation Loss: 14.283575057983398\n",
      "Running combination 13/30: {'units3': 128, 'units2': 64, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 64}\n",
      "Final Training Loss: 0.7737407088279724\n",
      "Final Validation Loss: 0.42240771651268005\n",
      "Running combination 14/30: {'units3': 32, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 120}\n",
      "Final Training Loss: 0.6273808479309082\n",
      "Final Validation Loss: 0.30607253313064575\n",
      "Running combination 15/30: {'units3': 128, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 3.1775288581848145\n",
      "Final Validation Loss: 1.9161657094955444\n",
      "Running combination 16/30: {'units3': 64, 'units2': 64, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 1.8712633848190308\n",
      "Final Validation Loss: 1.603703498840332\n",
      "Running combination 17/30: {'units3': 128, 'units2': 32, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 14.699010848999023\n",
      "Final Validation Loss: 13.87338924407959\n",
      "Running combination 18/30: {'units3': 32, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 1.7712621688842773\n",
      "Final Validation Loss: 0.3136436939239502\n",
      "Running combination 19/30: {'units3': 128, 'units2': 128, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 4.149754047393799\n",
      "Final Validation Loss: 3.008101224899292\n",
      "Running combination 20/30: {'units3': 64, 'units2': 32, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 120}\n",
      "Final Training Loss: 2.4996368885040283\n",
      "Final Validation Loss: 1.6426831483840942\n",
      "Running combination 21/30: {'units3': 64, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 50, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 2.787357807159424\n",
      "Final Validation Loss: 1.97591233253479\n",
      "Running combination 22/30: {'units3': 128, 'units2': 128, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 5.0, 'batch_size': 256}\n",
      "Final Training Loss: 30.851482391357422\n",
      "Final Validation Loss: 28.520416259765625\n",
      "Running combination 23/30: {'units3': 32, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 200, 'dropout_rate': 0.4, 'clipnorm': 5.0, 'batch_size': 256}\n",
      "Final Training Loss: 16.669498443603516\n",
      "Final Validation Loss: 15.002388000488281\n",
      "Running combination 24/30: {'units3': 64, 'units2': 128, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 200, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 256}\n",
      "Final Training Loss: 36.67507553100586\n",
      "Final Validation Loss: 35.62533187866211\n",
      "Running combination 25/30: {'units3': 64, 'units2': 64, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 10.24256420135498\n",
      "Final Validation Loss: 9.280488967895508\n",
      "Running combination 26/30: {'units3': 32, 'units2': 64, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 100, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 256}\n",
      "Final Training Loss: 3.003917694091797\n",
      "Final Validation Loss: 2.0012974739074707\n",
      "Running combination 27/30: {'units3': 64, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 64}\n",
      "Final Training Loss: 1.2809783220291138\n",
      "Final Validation Loss: 0.8826572895050049\n",
      "Running combination 28/30: {'units3': 64, 'units2': 64, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.01, 'epochs': 100, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 120}\n",
      "Final Training Loss: 2.1084251403808594\n",
      "Final Validation Loss: 1.496870517730713\n",
      "Running combination 29/30: {'units3': 64, 'units2': 32, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 256}\n",
      "Final Training Loss: 10.398048400878906\n",
      "Final Validation Loss: 9.65783977508545\n",
      "Running combination 30/30: {'units3': 128, 'units2': 128, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.4, 'clipnorm': 5.0, 'batch_size': 32}\n",
      "Final Training Loss: 4.230669975280762\n",
      "Final Validation Loss: 2.8136255741119385\n",
      "Top results:\n",
      "{'params': {'units3': 64, 'units2': 32, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 32}, 'final_train_loss': 1.3604249954223633, 'final_val_loss': 0.29776692390441895}\n",
      "{'params': {'units3': 32, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 120}, 'final_train_loss': 0.6273808479309082, 'final_val_loss': 0.30607253313064575}\n",
      "{'params': {'units3': 32, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 120}, 'final_train_loss': 1.7712621688842773, 'final_val_loss': 0.3136436939239502}\n",
      "{'params': {'units3': 32, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 256}, 'final_train_loss': 1.34364914894104, 'final_val_loss': 0.33425429463386536}\n",
      "{'params': {'units3': 64, 'units2': 64, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0001, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 64}, 'final_train_loss': 1.3359824419021606, 'final_val_loss': 0.34818336367607117}\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'dropout_rate': [0.2, 0.3, 0.4],\n",
    "    'recurrent_dropout': [0.1, 0.2],\n",
    "    'l2_lambda': [0.001, 0.01, 0.1],\n",
    "    'learning_rate': [0.001, 0.0005, 0.0001],\n",
    "    'learning_rate_decay': [1e-6, 1e-5, 0],\n",
    "    'units1': [32, 64, 128],\n",
    "    'units2': [32, 64, 128],\n",
    "    'units3': [32, 64, 128],\n",
    "    'batch_size': [32, 64, 120, 256],  \n",
    "    'epochs': [50, 100, 200],\n",
    "    'optimizer': ['adam'],\n",
    "    'clipnorm': [1.0, 5.0]\n",
    "}\n",
    "\n",
    "# Generate a list of random combinations with ParameterSampler\n",
    "n_iter_search = 30\n",
    "random_combinations = list(ParameterSampler(param_grid, n_iter=n_iter_search, random_state=42))\n",
    "\n",
    "# Define the function to build the model with variable parameters\n",
    "def build_model(dropout_rate, recurrent_dropout, l2_lambda, learning_rate, learning_rate_decay, clipnorm, units1, units2, units3, optimizer_name, loss_function):\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=units1, return_sequences=True, \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "                   kernel_regularizer=l2(l2_lambda), recurrent_dropout=recurrent_dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Second LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=units2, return_sequences=True, kernel_regularizer=l2(l2_lambda), recurrent_dropout=recurrent_dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Third LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=units3, return_sequences=False, kernel_regularizer=l2(l2_lambda), recurrent_dropout=recurrent_dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer.\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Select optimizer with decay and clipping.\n",
    "    optimizer = Adam(learning_rate=learning_rate, decay=learning_rate_decay, clipnorm=clipnorm)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Placeholder to keep track of results\n",
    "results = []\n",
    "\n",
    "# Run random search\n",
    "for i, params in enumerate(random_combinations):\n",
    "    print(f\"Running combination {i+1}/{len(random_combinations)}: {params}\")\n",
    "    \n",
    "    # Build model with the current parameters\n",
    "    model = build_model(\n",
    "        dropout_rate=params['dropout_rate'],\n",
    "        recurrent_dropout=params['recurrent_dropout'],\n",
    "        l2_lambda=params['l2_lambda'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        learning_rate_decay=params['learning_rate_decay'],\n",
    "        clipnorm=params['clipnorm'],\n",
    "        units1=params['units1'],\n",
    "        units2=params['units2'],\n",
    "        units3=params['units3'],\n",
    "        optimizer_name=params['optimizer'],\n",
    "        loss_function='mean_squared_error'\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq, \n",
    "        epochs=params['epochs'], \n",
    "        batch_size=params['batch_size'], \n",
    "        validation_data=(X_test_seq, y_test_seq), \n",
    "        callbacks=[early_stopping],\n",
    "        shuffle=False,\n",
    "        verbose=0  # Set verbose=0 to reduce output\n",
    "    )\n",
    "    \n",
    "    # Get final validation loss and training loss\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    \n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'params': params,\n",
    "        'final_train_loss': final_train_loss,\n",
    "        'final_val_loss': final_val_loss\n",
    "    })\n",
    "    \n",
    "    print(f\"Final Training Loss: {final_train_loss}\")\n",
    "    print(f\"Final Validation Loss: {final_val_loss}\")\n",
    "\n",
    "results = sorted(results, key=lambda x: x['final_val_loss'])\n",
    "print(\"Top results:\")\n",
    "for res in results[:5]:  # Show top 5 results\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fe6cf581-ab25-41ad-a0dd-d80518747002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 580ms/step - loss: 2.1614 - val_loss: 0.3189\n",
      "Epoch 2/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 1.1942 - val_loss: 0.3102\n",
      "Epoch 3/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.9990 - val_loss: 0.3110\n",
      "Epoch 4/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 1.0028 - val_loss: 0.3147\n",
      "Epoch 5/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.9405 - val_loss: 0.3175\n",
      "Epoch 6/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.8650 - val_loss: 0.3199\n",
      "Epoch 7/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.8744 - val_loss: 0.3190\n",
      "Epoch 8/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.8281 - val_loss: 0.3182\n",
      "Epoch 9/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.8611 - val_loss: 0.3219\n",
      "Epoch 10/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.8374 - val_loss: 0.3221\n",
      "Epoch 11/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.8146 - val_loss: 0.3198\n",
      "Epoch 12/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - loss: 0.7330 - val_loss: 0.3242\n",
      "Final Training Loss: 0.7334739565849304\n",
      "Final Validation Loss: 0.32415932416915894\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Best hyperparameters found from tuning\n",
    "best_params = {\n",
    "    'units1': 128,\n",
    "    'units2': 32,\n",
    "    'units3': 32,\n",
    "    'recurrent_dropout': 0.2,\n",
    "    'optimizer': 'adam',\n",
    "    'loss_function': 'mean_squared_error',\n",
    "    'learning_rate_decay': 0,\n",
    "    'learning_rate': 0.001,\n",
    "    'l2_lambda': 0.001,\n",
    "    'epochs': 100,\n",
    "    'dropout_rate': 0.2,\n",
    "    'clipnorm': 5.0,\n",
    "    'batch_size': 120\n",
    "}\n",
    "\n",
    "# Build the model using the best parameters\n",
    "def build_best_model(params):\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units1'], return_sequences=True, \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "                   kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Second LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units2'], return_sequences=True, kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Third LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units3'], return_sequences=False, kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Select the optimizer\n",
    "    if params['optimizer'] == 'adam':\n",
    "        optimizer = Adam(learning_rate=params['learning_rate'], decay=params['learning_rate_decay'], clipnorm=params['clipnorm'])\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizer '{params['optimizer']}' is not implemented in this setup.\")\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate the model with the best hyperparameters\n",
    "model = build_best_model(best_params)\n",
    "\n",
    "# Early stopping to stop training when validation loss stops improving\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq, \n",
    "    epochs=best_params['epochs'], \n",
    "    batch_size=best_params['batch_size'], \n",
    "    validation_data=(X_test_seq, y_test_seq),\n",
    "    shuffle=False,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Print the final training and validation loss\n",
    "print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f7e20b5f-1c05-4427-ace3-fcc9aa1a5946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 982ms/step\n",
      "LSTM Model Performance:\n",
      "Training RMSE: 0.020370913039071313\n",
      "Test RMSE: 0.023233765016554893\n",
      "Training MAE: 0.014775606984168\n",
      "Test MAE: 0.017562496871755574\n",
      "Directional Accuracy on Training Data: 48.319814600231744%\n",
      "Directional Accuracy on Test Data: 51.40845070422535%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict on both training and test sequences\n",
    "y_train_pred = model.predict(X_train_seq)\n",
    "y_test_pred = model.predict(X_test_seq)\n",
    "\n",
    "# Inverse scale predictions and actual values to original scale\n",
    "y_train_pred_rescaled = scaler_y.inverse_transform(y_train_pred)\n",
    "y_test_pred_rescaled = scaler_y.inverse_transform(y_test_pred)\n",
    "y_train_actual_rescaled = scaler_y.inverse_transform(y_train_seq)\n",
    "y_test_actual_rescaled = scaler_y.inverse_transform(y_test_seq)\n",
    "\n",
    "# Define a function for directional accuracy\n",
    "def directional_accuracy(y_true, y_pred):\n",
    "    return np.mean((np.sign(y_true[1:] - y_true[:-1]) == np.sign(y_pred[1:] - y_pred[:-1])).astype(int)) * 100\n",
    "\n",
    "# Calculate metrics.\n",
    "# Root Mean Squared Error (RMSE) for training set\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_actual_rescaled, y_train_pred_rescaled))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_actual_rescaled, y_test_pred_rescaled)) # RMSE for test set.\n",
    "train_mae = mean_absolute_error(y_train_actual_rescaled, y_train_pred_rescaled) # Mean Absolute Error (MAE) for training set.\n",
    "test_mae = mean_absolute_error(y_test_actual_rescaled, y_test_pred_rescaled) # MAE for test set.\n",
    "# Directional accuracy for training set.\n",
    "train_directional_accuracy = directional_accuracy(y_train_actual_rescaled.flatten(), y_train_pred_rescaled.flatten())\n",
    "# Directional accuracy for test set.\n",
    "test_directional_accuracy = directional_accuracy(y_test_actual_rescaled.flatten(), y_test_pred_rescaled.flatten())\n",
    "\n",
    "# Display score metrics to evaluate the model's performance.\n",
    "print(\"LSTM Model Performance:\")\n",
    "print(f\"Training RMSE: {train_rmse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Training MAE: {train_mae}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Directional Accuracy on Training Data: {train_directional_accuracy}%\")\n",
    "print(f\"Directional Accuracy on Test Data: {test_directional_accuracy}%\")\n",
    "\n",
    "# Plot training & validation loss values, it will help visualize how well the model fits the data over training epochs.\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a0dff9-123a-44a2-b0ce-039fb237b5a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6736d890-41ab-40ae-a9a4-f48add0d0feb",
   "metadata": {},
   "source": [
    "### 2. Optuna (Model 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "38c20756-919a-480f-b563-8fa51f448380",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 22:32:14,447] A new study created in memory with name: no-name-c9d044eb-ca59-40ef-8769-266c8cf0d699\n",
      "[I 2024-11-13 22:32:22,921] Trial 0 finished with value: 0.5691806674003601 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.2, 'l2_lambda': 0.0037550037026405905, 'learning_rate': 0.0001, 'learning_rate_decay': 1e-05, 'clipnorm': 1.0, 'units': 32, 'num_layers': 3, 'epochs': 100, 'batch_size': 64}. Best is trial 0 with value: 0.5691806674003601.\n",
      "[I 2024-11-13 22:32:30,036] Trial 1 finished with value: 2.5420398712158203 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.2, 'l2_lambda': 0.04641587067816149, 'learning_rate': 0.0001, 'learning_rate_decay': 1e-05, 'clipnorm': 1.0, 'units': 32, 'num_layers': 2, 'epochs': 50, 'batch_size': 256}. Best is trial 0 with value: 0.5691806674003601.\n",
      "[I 2024-11-13 22:32:52,254] Trial 2 finished with value: 2.4158737659454346 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.2, 'l2_lambda': 0.02754390996680696, 'learning_rate': 0.0001, 'learning_rate_decay': 1e-05, 'clipnorm': 1.0, 'units': 128, 'num_layers': 2, 'epochs': 100, 'batch_size': 120}. Best is trial 0 with value: 0.5691806674003601.\n",
      "[I 2024-11-13 22:33:07,190] Trial 3 finished with value: 0.3193957209587097 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.2, 'l2_lambda': 0.06953241045943279, 'learning_rate': 0.0001, 'learning_rate_decay': 1e-05, 'clipnorm': 1.0, 'units': 128, 'num_layers': 1, 'epochs': 150, 'batch_size': 256}. Best is trial 3 with value: 0.3193957209587097.\n",
      "[I 2024-11-13 22:33:13,698] Trial 4 finished with value: 0.1484442949295044 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.1, 'l2_lambda': 0.07320157289972352, 'learning_rate': 0.0005, 'learning_rate_decay': 1e-05, 'clipnorm': 1.0, 'units': 64, 'num_layers': 1, 'epochs': 150, 'batch_size': 32}. Best is trial 4 with value: 0.1484442949295044.\n",
      "[I 2024-11-13 22:33:31,320] Trial 5 finished with value: 0.23264658451080322 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.2, 'l2_lambda': 0.06621901977279396, 'learning_rate': 0.0005, 'learning_rate_decay': 0, 'clipnorm': 1.0, 'units': 64, 'num_layers': 2, 'epochs': 150, 'batch_size': 256}. Best is trial 4 with value: 0.1484442949295044.\n",
      "[I 2024-11-13 22:33:45,573] Trial 6 finished with value: 1.7703280448913574 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0038300508438275606, 'learning_rate': 0.0001, 'learning_rate_decay': 0, 'clipnorm': 5.0, 'units': 128, 'num_layers': 3, 'epochs': 50, 'batch_size': 256}. Best is trial 4 with value: 0.1484442949295044.\n",
      "[I 2024-11-13 22:33:52,779] Trial 7 finished with value: 0.14473307132720947 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0019223481607694784, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 150, 'batch_size': 64}. Best is trial 7 with value: 0.14473307132720947.\n",
      "[I 2024-11-13 22:34:04,801] Trial 8 finished with value: 0.4912290573120117 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0017384974140603195, 'learning_rate': 0.0001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 64, 'num_layers': 3, 'epochs': 150, 'batch_size': 120}. Best is trial 7 with value: 0.14473307132720947.\n",
      "[I 2024-11-13 22:34:11,411] Trial 9 finished with value: 0.1530628800392151 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.1, 'l2_lambda': 0.028980866560207673, 'learning_rate': 0.0005, 'learning_rate_decay': 1e-06, 'clipnorm': 1.0, 'units': 64, 'num_layers': 1, 'epochs': 200, 'batch_size': 120}. Best is trial 7 with value: 0.14473307132720947.\n",
      "[I 2024-11-13 22:34:15,310] Trial 10 finished with value: 0.145683154463768 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0010012325682668814, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 200, 'batch_size': 64}. Best is trial 7 with value: 0.14473307132720947.\n",
      "[I 2024-11-13 22:34:21,238] Trial 11 finished with value: 0.1470104306936264 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0011454828358668492, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 200, 'batch_size': 64}. Best is trial 7 with value: 0.14473307132720947.\n",
      "[I 2024-11-13 22:34:27,845] Trial 12 finished with value: 0.1453746259212494 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0028873014180725233, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 200, 'batch_size': 64}. Best is trial 7 with value: 0.14473307132720947.\n",
      "[I 2024-11-13 22:34:35,022] Trial 13 finished with value: 0.1432897299528122 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.005928495353726078, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 200, 'batch_size': 64}. Best is trial 13 with value: 0.1432897299528122.\n",
      "[I 2024-11-13 22:34:57,137] Trial 14 finished with value: 0.18365783989429474 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.008559378029651834, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 2, 'epochs': 100, 'batch_size': 64}. Best is trial 13 with value: 0.1432897299528122.\n",
      "[I 2024-11-13 22:35:02,840] Trial 15 finished with value: 0.15606708824634552 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.008980574547389344, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 200, 'batch_size': 32}. Best is trial 13 with value: 0.1432897299528122.\n",
      "[I 2024-11-13 22:35:20,866] Trial 16 finished with value: 0.16231994330883026 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0057758586332834535, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 32, 'num_layers': 2, 'epochs': 150, 'batch_size': 64}. Best is trial 13 with value: 0.1432897299528122.\n",
      "[I 2024-11-13 22:35:27,309] Trial 17 finished with value: 0.14659617841243744 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0021211361641394807, 'learning_rate': 0.001, 'learning_rate_decay': 0, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 200, 'batch_size': 64}. Best is trial 13 with value: 0.1432897299528122.\n",
      "[I 2024-11-13 22:35:54,069] Trial 18 finished with value: 0.1473255604505539 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.1, 'l2_lambda': 0.013305283788443754, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 2, 'epochs': 150, 'batch_size': 64}. Best is trial 13 with value: 0.1432897299528122.\n",
      "[I 2024-11-13 22:35:57,013] Trial 19 finished with value: 0.19085519015789032 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.005761413882956333, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 32, 'num_layers': 1, 'epochs': 100, 'batch_size': 32}. Best is trial 13 with value: 0.1432897299528122.\n",
      "[I 2024-11-13 22:36:31,002] Trial 20 finished with value: 0.13798832893371582 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.1, 'l2_lambda': 0.017551598258192868, 'learning_rate': 0.001, 'learning_rate_decay': 0, 'clipnorm': 5.0, 'units': 128, 'num_layers': 2, 'epochs': 200, 'batch_size': 64}. Best is trial 20 with value: 0.13798832893371582.\n",
      "[I 2024-11-13 22:37:04,925] Trial 21 finished with value: 0.13825955986976624 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.1, 'l2_lambda': 0.015142023386174807, 'learning_rate': 0.001, 'learning_rate_decay': 0, 'clipnorm': 5.0, 'units': 128, 'num_layers': 2, 'epochs': 200, 'batch_size': 64}. Best is trial 20 with value: 0.13798832893371582.\n",
      "[I 2024-11-13 22:37:31,501] Trial 22 finished with value: 0.14838065207004547 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.1, 'l2_lambda': 0.01650776426932335, 'learning_rate': 0.001, 'learning_rate_decay': 0, 'clipnorm': 5.0, 'units': 128, 'num_layers': 2, 'epochs': 200, 'batch_size': 64}. Best is trial 20 with value: 0.13798832893371582.\n",
      "[I 2024-11-13 22:38:14,749] Trial 23 finished with value: 0.1384434551000595 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.1, 'l2_lambda': 0.020066237605848726, 'learning_rate': 0.001, 'learning_rate_decay': 0, 'clipnorm': 5.0, 'units': 128, 'num_layers': 3, 'epochs': 200, 'batch_size': 64}. Best is trial 20 with value: 0.13798832893371582.\n",
      "[I 2024-11-13 22:39:02,660] Trial 24 finished with value: 0.13738267123699188 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.1, 'l2_lambda': 0.021127823563600365, 'learning_rate': 0.001, 'learning_rate_decay': 0, 'clipnorm': 5.0, 'units': 128, 'num_layers': 3, 'epochs': 200, 'batch_size': 64}. Best is trial 24 with value: 0.13738267123699188.\n",
      "[I 2024-11-13 22:39:55,942] Trial 25 finished with value: 0.14105987548828125 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.1, 'l2_lambda': 0.03217673718930627, 'learning_rate': 0.0005, 'learning_rate_decay': 0, 'clipnorm': 5.0, 'units': 128, 'num_layers': 3, 'epochs': 200, 'batch_size': 64}. Best is trial 24 with value: 0.13738267123699188.\n",
      "[I 2024-11-13 22:40:23,164] Trial 26 finished with value: 0.14995311200618744 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.2, 'l2_lambda': 0.011827045328884301, 'learning_rate': 0.001, 'learning_rate_decay': 0, 'clipnorm': 5.0, 'units': 128, 'num_layers': 2, 'epochs': 200, 'batch_size': 64}. Best is trial 24 with value: 0.13738267123699188.\n",
      "[I 2024-11-13 22:40:47,312] Trial 27 finished with value: 0.1460806429386139 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.1, 'l2_lambda': 0.02146348063364359, 'learning_rate': 0.001, 'learning_rate_decay': 0, 'clipnorm': 5.0, 'units': 128, 'num_layers': 2, 'epochs': 150, 'batch_size': 32}. Best is trial 24 with value: 0.13738267123699188.\n",
      "[I 2024-11-13 22:41:12,390] Trial 28 finished with value: 0.12486433982849121 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.1, 'l2_lambda': 0.042448918017048726, 'learning_rate': 0.001, 'learning_rate_decay': 0, 'clipnorm': 5.0, 'units': 32, 'num_layers': 3, 'epochs': 200, 'batch_size': 120}. Best is trial 28 with value: 0.12486433982849121.\n",
      "[I 2024-11-13 22:41:37,201] Trial 29 finished with value: 0.2024923712015152 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.2, 'l2_lambda': 0.04372010965288467, 'learning_rate': 0.0005, 'learning_rate_decay': 0, 'clipnorm': 5.0, 'units': 32, 'num_layers': 3, 'epochs': 200, 'batch_size': 120}. Best is trial 28 with value: 0.12486433982849121.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.1, 'l2_lambda': 0.042448918017048726, 'learning_rate': 0.001, 'learning_rate_decay': 0, 'clipnorm': 5.0, 'units': 32, 'num_layers': 3, 'epochs': 200, 'batch_size': 120}\n",
      "Best validation loss: 0.12486433982849121\n"
     ]
    }
   ],
   "source": [
    "# Set global random state for reproducibility.\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# Optuna's TPESampler with a fixed random seed for reproducibility in parameter search.\n",
    "sampler = TPESampler(seed=random_seed)\n",
    "\n",
    "# Create an Optuna study with direction \"minimize\" to minimize the validation loss.\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "\n",
    "# Define the objective function for hyperparameter optimization.\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna to optimize hyperparameters of an LSTM model.\n",
    "    Takes a trial object from Optuna and returns the validation loss of the model with given parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define hyperparameters to tune.\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.2, 0.4, step=0.1)  # Dropout rate for LSTM layers.\n",
    "    recurrent_dropout = trial.suggest_categorical(\"recurrent_dropout\", [0.1, 0.2])  # Recurrent dropout rate for LSTM layers.\n",
    "    l2_lambda = trial.suggest_loguniform(\"l2_lambda\", 1e-3, 1e-1)  # L2 regularization factor.\n",
    "    learning_rate = trial.suggest_categorical(\"learning_rate\", [0.001, 0.0005, 0.0001])  # Learning rate for the optimizer.\n",
    "    learning_rate_decay = trial.suggest_categorical(\"learning_rate_decay\", [1e-6, 1e-5, 0])  # Learning rate decay.\n",
    "    clipnorm = trial.suggest_categorical(\"clipnorm\", [1.0, 5.0])  # Gradient clipping norm.\n",
    "    units = trial.suggest_categorical(\"units\", [32, 64, 128])  # Number of units in LSTM layers.\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)  # Number of LSTM layers.\n",
    "    epochs = trial.suggest_int(\"epochs\", 50, 200, step=50)  # Number of epochs.\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 120, 256])  # Batch size.\n",
    "\n",
    "    # Initialize the Sequential model.\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add LSTM layers based on num_layers.\n",
    "    for i in range(num_layers):\n",
    "        # Set return_sequences=True for all but the last LSTM layer.\n",
    "        return_sequences = (i < num_layers - 1)\n",
    "        \n",
    "        # Add LSTM layer with specified hyperparameters.\n",
    "        model.add(LSTM(\n",
    "            units=units,\n",
    "            return_sequences=return_sequences,\n",
    "            input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]) if i == 0 else None,  # Set input shape only for the first layer.\n",
    "            kernel_regularizer=l2(l2_lambda),\n",
    "            recurrent_dropout=recurrent_dropout\n",
    "        ))\n",
    "\n",
    "        # Add BatchNormalization and Dropout after each LSTM layer.\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Add the output layer with a single unit (regression).\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Initialize the optimizer with learning rate, decay, and gradient clipping norm.\n",
    "    optimizer = Adam(learning_rate=learning_rate, decay=learning_rate_decay, clipnorm=clipnorm)\n",
    "\n",
    "    # Compile the model with mean squared error loss function.\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Early stopping callback to avoid overfitting.\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model with given hyperparameters.\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test_seq, y_test_seq),\n",
    "        callbacks=[early_stopping],\n",
    "        shuffle=False,\n",
    "        verbose=0  # Set verbose=0 to suppress training logs for faster experimentation.\n",
    "    )\n",
    "\n",
    "    # Retrieve the minimum validation loss from the training history.\n",
    "    val_loss = min(history.history['val_loss'])\n",
    "    \n",
    "    # Return the validation loss to be minimized by Optuna.\n",
    "    return val_loss\n",
    "\n",
    "# Run the Optuna study for a given number of trials.\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Print the best parameters found by the study.\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "# Print the best validation loss achieved with the optimal parameters.\n",
    "print(\"Best validation loss:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "807c6afb-5963-4f21-b22d-e506de1bf8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 790ms/step - loss: 8.4994 - val_loss: 4.7814\n",
      "Epoch 2/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - loss: 7.2147 - val_loss: 4.6490\n",
      "Epoch 3/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6616 - val_loss: 4.5129\n",
      "Epoch 4/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 6.1670 - val_loss: 4.3669\n",
      "Epoch 5/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 5.9231 - val_loss: 4.2174\n",
      "Epoch 6/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 5.6068 - val_loss: 4.0675\n",
      "Epoch 7/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 5.5508 - val_loss: 3.9194\n",
      "Epoch 8/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 5.2652 - val_loss: 3.7776\n",
      "Epoch 9/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 4.9824 - val_loss: 3.6398\n",
      "Epoch 10/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 4.7655 - val_loss: 3.5031\n",
      "Epoch 11/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 4.5901 - val_loss: 3.3702\n",
      "Epoch 12/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 4.4098 - val_loss: 3.2444\n",
      "Epoch 13/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 4.2366 - val_loss: 3.1239\n",
      "Epoch 14/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 4.0982 - val_loss: 3.0064\n",
      "Epoch 15/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 3.8819 - val_loss: 2.8943\n",
      "Epoch 16/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 3.7076 - val_loss: 2.7852\n",
      "Epoch 17/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 3.5705 - val_loss: 2.6803\n",
      "Epoch 18/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 3.4085 - val_loss: 2.5811\n",
      "Epoch 19/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 3.3127 - val_loss: 2.4833\n",
      "Epoch 20/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 3.3125 - val_loss: 2.3901\n",
      "Epoch 21/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 3.0749 - val_loss: 2.3018\n",
      "Epoch 22/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 2.9628 - val_loss: 2.2181\n",
      "Epoch 23/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 2.7465 - val_loss: 2.1357\n",
      "Epoch 24/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 2.7123 - val_loss: 2.0564\n",
      "Epoch 25/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 2.6261 - val_loss: 1.9808\n",
      "Epoch 26/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 2.4575 - val_loss: 1.9084\n",
      "Epoch 27/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 2.3704 - val_loss: 1.8392\n",
      "Epoch 28/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 2.3522 - val_loss: 1.7720\n",
      "Epoch 29/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 2.1987 - val_loss: 1.7088\n",
      "Epoch 30/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 2.1332 - val_loss: 1.6486\n",
      "Epoch 31/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 2.0346 - val_loss: 1.5910\n",
      "Epoch 32/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.9865 - val_loss: 1.5369\n",
      "Epoch 33/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 1.9447 - val_loss: 1.4851\n",
      "Epoch 34/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.8642 - val_loss: 1.4358\n",
      "Epoch 35/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.8459 - val_loss: 1.3885\n",
      "Epoch 36/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 1.7485 - val_loss: 1.3429\n",
      "Epoch 37/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 1.6965 - val_loss: 1.2998\n",
      "Epoch 38/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.5655 - val_loss: 1.2583\n",
      "Epoch 39/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 1.5443 - val_loss: 1.2171\n",
      "Epoch 40/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 1.5436 - val_loss: 1.1778\n",
      "Epoch 41/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 1.4710 - val_loss: 1.1394\n",
      "Epoch 42/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 1.3814 - val_loss: 1.1035\n",
      "Epoch 43/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 1.3946 - val_loss: 1.0704\n",
      "Epoch 44/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.2939 - val_loss: 1.0394\n",
      "Epoch 45/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.2679 - val_loss: 1.0107\n",
      "Epoch 46/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 1.2515 - val_loss: 0.9814\n",
      "Epoch 47/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 1.1951 - val_loss: 0.9515\n",
      "Epoch 48/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 1.1933 - val_loss: 0.9240\n",
      "Epoch 49/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 1.0827 - val_loss: 0.8984\n",
      "Epoch 50/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.0782 - val_loss: 0.8729\n",
      "Epoch 51/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.0745 - val_loss: 0.8482\n",
      "Epoch 52/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.0203 - val_loss: 0.8248\n",
      "Epoch 53/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.0114 - val_loss: 0.8024\n",
      "Epoch 54/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.9549 - val_loss: 0.7804\n",
      "Epoch 55/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.8903 - val_loss: 0.7601\n",
      "Epoch 56/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.9192 - val_loss: 0.7401\n",
      "Epoch 57/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.8557 - val_loss: 0.7213\n",
      "Epoch 58/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.8362 - val_loss: 0.7031\n",
      "Epoch 59/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.7943 - val_loss: 0.6858\n",
      "Epoch 60/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.7989 - val_loss: 0.6689\n",
      "Epoch 61/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.7631 - val_loss: 0.6519\n",
      "Epoch 62/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.7504 - val_loss: 0.6360\n",
      "Epoch 63/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.7151 - val_loss: 0.6208\n",
      "Epoch 64/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.7241 - val_loss: 0.6052\n",
      "Epoch 65/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.7046 - val_loss: 0.5903\n",
      "Epoch 66/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.6880 - val_loss: 0.5773\n",
      "Epoch 67/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.6477 - val_loss: 0.5647\n",
      "Epoch 68/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.6354 - val_loss: 0.5520\n",
      "Epoch 69/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.6267 - val_loss: 0.5396\n",
      "Epoch 70/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.6224 - val_loss: 0.5288\n",
      "Epoch 71/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.5937 - val_loss: 0.5173\n",
      "Epoch 72/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.5784 - val_loss: 0.5051\n",
      "Epoch 73/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.5719 - val_loss: 0.4939\n",
      "Epoch 74/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.5627 - val_loss: 0.4834\n",
      "Epoch 75/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.5353 - val_loss: 0.4738\n",
      "Epoch 76/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.5061 - val_loss: 0.4650\n",
      "Epoch 77/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.4938 - val_loss: 0.4552\n",
      "Epoch 78/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.4987 - val_loss: 0.4458\n",
      "Epoch 79/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.4780 - val_loss: 0.4360\n",
      "Epoch 80/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.4796 - val_loss: 0.4267\n",
      "Epoch 81/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.4638 - val_loss: 0.4187\n",
      "Epoch 82/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.4607 - val_loss: 0.4113\n",
      "Epoch 83/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.4551 - val_loss: 0.4034\n",
      "Epoch 84/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.4352 - val_loss: 0.3956\n",
      "Epoch 85/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.4263 - val_loss: 0.3882\n",
      "Epoch 86/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.4136 - val_loss: 0.3810\n",
      "Epoch 87/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.4179 - val_loss: 0.3730\n",
      "Epoch 88/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.3963 - val_loss: 0.3659\n",
      "Epoch 89/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.3992 - val_loss: 0.3583\n",
      "Epoch 90/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.3774 - val_loss: 0.3516\n",
      "Epoch 91/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.3780 - val_loss: 0.3453\n",
      "Epoch 92/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.3633 - val_loss: 0.3393\n",
      "Epoch 93/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.3542 - val_loss: 0.3331\n",
      "Epoch 94/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.3580 - val_loss: 0.3272\n",
      "Epoch 95/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.3328 - val_loss: 0.3216\n",
      "Epoch 96/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.3263 - val_loss: 0.3164\n",
      "Epoch 97/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.3306 - val_loss: 0.3120\n",
      "Epoch 98/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.3211 - val_loss: 0.3070\n",
      "Epoch 99/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.3192 - val_loss: 0.3021\n",
      "Epoch 100/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.3179 - val_loss: 0.2974\n",
      "Epoch 101/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.3066 - val_loss: 0.2937\n",
      "Epoch 102/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.3049 - val_loss: 0.2885\n",
      "Epoch 103/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.2948 - val_loss: 0.2825\n",
      "Epoch 104/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.2958 - val_loss: 0.2782\n",
      "Epoch 105/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2866 - val_loss: 0.2736\n",
      "Epoch 106/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.2815 - val_loss: 0.2690\n",
      "Epoch 107/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2679 - val_loss: 0.2650\n",
      "Epoch 108/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2762 - val_loss: 0.2612\n",
      "Epoch 109/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.2702 - val_loss: 0.2572\n",
      "Epoch 110/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.2561 - val_loss: 0.2533\n",
      "Epoch 111/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2509 - val_loss: 0.2496\n",
      "Epoch 112/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.2514 - val_loss: 0.2461\n",
      "Epoch 113/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2378 - val_loss: 0.2425\n",
      "Epoch 114/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2411 - val_loss: 0.2391\n",
      "Epoch 115/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.2442 - val_loss: 0.2367\n",
      "Epoch 116/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2365 - val_loss: 0.2343\n",
      "Epoch 117/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.2300 - val_loss: 0.2305\n",
      "Epoch 118/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2222 - val_loss: 0.2264\n",
      "Epoch 119/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2237 - val_loss: 0.2232\n",
      "Epoch 120/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.2215 - val_loss: 0.2206\n",
      "Epoch 121/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2211 - val_loss: 0.2178\n",
      "Epoch 122/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2153 - val_loss: 0.2153\n",
      "Epoch 123/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2087 - val_loss: 0.2129\n",
      "Epoch 124/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.2062 - val_loss: 0.2101\n",
      "Epoch 125/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1997 - val_loss: 0.2070\n",
      "Epoch 126/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.2065 - val_loss: 0.2047\n",
      "Epoch 127/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1981 - val_loss: 0.2028\n",
      "Epoch 128/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.1960 - val_loss: 0.2006\n",
      "Epoch 129/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1932 - val_loss: 0.1982\n",
      "Epoch 130/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1929 - val_loss: 0.1961\n",
      "Epoch 131/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1885 - val_loss: 0.1944\n",
      "Epoch 132/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1893 - val_loss: 0.1924\n",
      "Epoch 133/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1854 - val_loss: 0.1900\n",
      "Epoch 134/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.1829 - val_loss: 0.1880\n",
      "Epoch 135/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.1804 - val_loss: 0.1862\n",
      "Epoch 136/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.1776 - val_loss: 0.1840\n",
      "Epoch 137/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1750 - val_loss: 0.1816\n",
      "Epoch 138/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1713 - val_loss: 0.1795\n",
      "Epoch 139/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1705 - val_loss: 0.1777\n",
      "Epoch 140/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1672 - val_loss: 0.1759\n",
      "Epoch 141/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1695 - val_loss: 0.1746\n",
      "Epoch 142/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1662 - val_loss: 0.1730\n",
      "Epoch 143/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1628 - val_loss: 0.1715\n",
      "Epoch 144/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1593 - val_loss: 0.1702\n",
      "Epoch 145/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.1630 - val_loss: 0.1689\n",
      "Epoch 146/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1606 - val_loss: 0.1679\n",
      "Epoch 147/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1587 - val_loss: 0.1666\n",
      "Epoch 148/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1565 - val_loss: 0.1654\n",
      "Epoch 149/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.1554 - val_loss: 0.1642\n",
      "Epoch 150/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.1533 - val_loss: 0.1627\n",
      "Epoch 151/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.1538 - val_loss: 0.1613\n",
      "Epoch 152/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1493 - val_loss: 0.1598\n",
      "Epoch 153/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1508 - val_loss: 0.1590\n",
      "Epoch 154/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.1490 - val_loss: 0.1585\n",
      "Epoch 155/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1474 - val_loss: 0.1574\n",
      "Epoch 156/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1432 - val_loss: 0.1561\n",
      "Epoch 157/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1444 - val_loss: 0.1550\n",
      "Epoch 158/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1447 - val_loss: 0.1539\n",
      "Epoch 159/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.1411 - val_loss: 0.1528\n",
      "Epoch 160/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1420 - val_loss: 0.1524\n",
      "Epoch 161/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.1397 - val_loss: 0.1519\n",
      "Epoch 162/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.1402 - val_loss: 0.1515\n",
      "Epoch 163/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1388 - val_loss: 0.1502\n",
      "Epoch 164/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1383 - val_loss: 0.1489\n",
      "Epoch 165/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1365 - val_loss: 0.1477\n",
      "Epoch 166/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1355 - val_loss: 0.1467\n",
      "Epoch 167/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.1350 - val_loss: 0.1463\n",
      "Epoch 168/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.1335 - val_loss: 0.1459\n",
      "Epoch 169/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1337 - val_loss: 0.1454\n",
      "Epoch 170/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.1319 - val_loss: 0.1443\n",
      "Epoch 171/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.1319 - val_loss: 0.1436\n",
      "Epoch 172/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1323 - val_loss: 0.1427\n",
      "Epoch 173/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.1304 - val_loss: 0.1420\n",
      "Epoch 174/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.1306 - val_loss: 0.1414\n",
      "Epoch 175/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.1300 - val_loss: 0.1411\n",
      "Epoch 176/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.1284 - val_loss: 0.1405\n",
      "Epoch 177/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1279 - val_loss: 0.1398\n",
      "Epoch 178/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.1265 - val_loss: 0.1392\n",
      "Epoch 179/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.1258 - val_loss: 0.1389\n",
      "Epoch 180/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.1269 - val_loss: 0.1387\n",
      "Epoch 181/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.1257 - val_loss: 0.1385\n",
      "Epoch 182/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.1256 - val_loss: 0.1370\n",
      "Epoch 183/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.1249 - val_loss: 0.1359\n",
      "Epoch 184/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.1237 - val_loss: 0.1357\n",
      "Epoch 185/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1235 - val_loss: 0.1358\n",
      "Epoch 186/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1232 - val_loss: 0.1353\n",
      "Epoch 187/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.1217 - val_loss: 0.1347\n",
      "Epoch 188/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1221 - val_loss: 0.1343\n",
      "Epoch 189/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.1216 - val_loss: 0.1338\n",
      "Epoch 190/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1204 - val_loss: 0.1330\n",
      "Epoch 191/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.1215 - val_loss: 0.1323\n",
      "Epoch 192/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.1214 - val_loss: 0.1322\n",
      "Epoch 193/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.1206 - val_loss: 0.1324\n",
      "Epoch 194/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.1195 - val_loss: 0.1323\n",
      "Epoch 195/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1197 - val_loss: 0.1319\n",
      "Epoch 196/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1195 - val_loss: 0.1317\n",
      "Epoch 197/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1190 - val_loss: 0.1315\n",
      "Epoch 198/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1189 - val_loss: 0.1315\n",
      "Epoch 199/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.1189 - val_loss: 0.1315\n",
      "Epoch 200/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1179 - val_loss: 0.1310\n",
      "Final Training Loss: 0.12786121666431427\n",
      "Final Validation Loss: 0.1310335248708725\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Best hyperparameters found from tuning\n",
    "best_params = {\n",
    "    'units1': 32,\n",
    "    'units2': 32,\n",
    "    'units3': 32,\n",
    "    'recurrent_dropout': 0.1,\n",
    "    'optimizer': 'adam',\n",
    "    'loss_function': 'mean_squared_error',\n",
    "    'learning_rate_decay': 0,\n",
    "    'learning_rate': 0.001,\n",
    "    'l2_lambda': 0.042448918017048726,\n",
    "    'epochs': 200,\n",
    "    'dropout_rate': 0.4,\n",
    "    'clipnorm': 5.0,\n",
    "    'batch_size': 120\n",
    "}\n",
    "\n",
    "# Build the model using the best parameters\n",
    "def build_best_model(params):\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units1'], return_sequences=True, \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "                   kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Second LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units2'], return_sequences=True, kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Third LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units3'], return_sequences=False, kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Select the optimizer\n",
    "    if params['optimizer'] == 'adam':\n",
    "        optimizer = Adam(learning_rate=params['learning_rate'], decay=params['learning_rate_decay'], clipnorm=params['clipnorm'])\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizer '{params['optimizer']}' is not implemented in this setup.\")\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate the model with the best hyperparameters\n",
    "model = build_best_model(best_params)\n",
    "\n",
    "# Early stopping to stop training when validation loss stops improving\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq, \n",
    "    epochs=best_params['epochs'], \n",
    "    batch_size=best_params['batch_size'], \n",
    "    validation_data=(X_test_seq, y_test_seq),\n",
    "    shuffle=False,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Print the final training and validation loss\n",
    "print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "90ff2e4c-63cc-4c54-a031-0c82df2b51a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 950ms/step\n",
      "LSTM Model Performance:\n",
      "Training RMSE: 0.020398293380544646\n",
      "Test RMSE: 0.023338731087789263\n",
      "Training MAE: 0.014705354746982995\n",
      "Test MAE: 0.017482875505769775\n",
      "Directional Accuracy on Training Data: 52.02780996523755%\n",
      "Directional Accuracy on Test Data: 51.40845070422535%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict on both training and test sequences\n",
    "y_train_pred = model.predict(X_train_seq)\n",
    "y_test_pred = model.predict(X_test_seq)\n",
    "\n",
    "# Inverse scale predictions and actual values to original scale\n",
    "y_train_pred_rescaled = scaler_y.inverse_transform(y_train_pred)\n",
    "y_test_pred_rescaled = scaler_y.inverse_transform(y_test_pred)\n",
    "y_train_actual_rescaled = scaler_y.inverse_transform(y_train_seq)\n",
    "y_test_actual_rescaled = scaler_y.inverse_transform(y_test_seq)\n",
    "\n",
    "# Define a function for directional accuracy\n",
    "def directional_accuracy(y_true, y_pred):\n",
    "    return np.mean((np.sign(y_true[1:] - y_true[:-1]) == np.sign(y_pred[1:] - y_pred[:-1])).astype(int)) * 100\n",
    "\n",
    "# Calculate metrics.\n",
    "# Root Mean Squared Error (RMSE) for training set\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_actual_rescaled, y_train_pred_rescaled))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_actual_rescaled, y_test_pred_rescaled)) # RMSE for test set.\n",
    "train_mae = mean_absolute_error(y_train_actual_rescaled, y_train_pred_rescaled) # Mean Absolute Error (MAE) for training set.\n",
    "test_mae = mean_absolute_error(y_test_actual_rescaled, y_test_pred_rescaled) # MAE for test set.\n",
    "# Directional accuracy for training set.\n",
    "train_directional_accuracy = directional_accuracy(y_train_actual_rescaled.flatten(), y_train_pred_rescaled.flatten())\n",
    "# Directional accuracy for test set.\n",
    "test_directional_accuracy = directional_accuracy(y_test_actual_rescaled.flatten(), y_test_pred_rescaled.flatten())\n",
    "\n",
    "# Display score metrics to evaluate the model's performance.\n",
    "print(\"LSTM Model Performance:\")\n",
    "print(f\"Training RMSE: {train_rmse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Training MAE: {train_mae}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Directional Accuracy on Training Data: {train_directional_accuracy}%\")\n",
    "print(f\"Directional Accuracy on Test Data: {test_directional_accuracy}%\")\n",
    "\n",
    "# Plot training & validation loss values, it will help visualize how well the model fits the data over training epochs.\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4a1068-c110-4fe4-a1b3-0440c9a4041a",
   "metadata": {},
   "source": [
    "### 3. Keras Tuner (Model 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fa120630-9118-4e7e-9225-151d8d4ca4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 00m 24s]\n",
      "val_loss: 0.17077887058258057\n",
      "\n",
      "Best val_loss So Far: 0.1424890011548996\n",
      "Total elapsed time: 00h 10m 50s\n",
      "Built model with params: dropout_rate=0.4, recurrent_dropout=0.2, l2_lambda=0.025064206402151706, learning_rate=0.0005, learning_rate_decay=1e-05, clipnorm=1.0, units=32, num_layers=3, batch_size=120\n",
      "Best hyperparameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.2, 'l2_lambda': 0.025064206402151706, 'learning_rate': 0.0005, 'learning_rate_decay': 1e-05, 'clipnorm': 1.0, 'units': 32, 'num_layers': 3, 'batch_size': 120}\n",
      "Best batch size: 120\n"
     ]
    }
   ],
   "source": [
    "# Set a global random seed for reproducibility.\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# Build model function with hyperparameter choices.\n",
    "def build_model(hp): #hp (kerastuner.HyperParameters) - Hyperparameter search space.\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Define hyperparameters using `hp` for various tuning options.\n",
    "    dropout_rate = hp.Choice(\"dropout_rate\", [0.2, 0.3, 0.4])  # Dropout rates to fight overfitting.\n",
    "    recurrent_dropout = hp.Choice(\"recurrent_dropout\", [0.1, 0.2])  # Recurrent dropout for LSTM layers.\n",
    "    l2_lambda = hp.Float(\"l2_lambda\", min_value=0.001, max_value=0.1, sampling=\"log\")  # L2 regularization factor.\n",
    "    learning_rate = hp.Choice(\"learning_rate\", [0.001, 0.0005, 0.0001])  # Learning rate choices.\n",
    "    learning_rate_decay = hp.Choice(\"learning_rate_decay\", [1e-5, 0.0])  # Learning rate decay for gradual reduction.\n",
    "    clipnorm = hp.Choice(\"clipnorm\", [1.0, 5.0])  # Gradient clipping norm to prevent exploding gradients.\n",
    "    units = hp.Choice(\"units\", [32, 64, 128])  # Number of units for LSTM layers.\n",
    "    num_layers = hp.Int(\"num_layers\", 1, 3)  # Number of LSTM layers.\n",
    "    batch_size = hp.Choice(\"batch_size\", [32, 64, 120, 256])  # Batch size choices.\n",
    "\n",
    "\n",
    "    # Add LSTM layers based on the number of layers selected.\n",
    "    for i in range(num_layers):\n",
    "        return_sequences = (i < num_layers - 1)\n",
    "        model.add(LSTM(units=units, return_sequences=return_sequences,\n",
    "                       input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]) if i == 0 else None,\n",
    "                       kernel_regularizer=l2(l2_lambda), recurrent_dropout=recurrent_dropout))\n",
    "        model.add(BatchNormalization()) # Add batch normalization to stabilize training.\n",
    "        model.add(Dropout(dropout_rate))  # Add dropout to help with overfitting.\n",
    "\n",
    "    model.add(Dense(1))  # Output layer for a single continuous value.\n",
    "    # Configure optimizer with learning rate, decay, and gradient clipping.\n",
    "    optimizer = Adam(learning_rate=learning_rate, decay=learning_rate_decay, clipnorm=clipnorm)\n",
    "     # Compile model with specified optimizer and mean squared error loss.\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    print(f\"Built model with params: dropout_rate={dropout_rate}, recurrent_dropout={recurrent_dropout}, \"\n",
    "          f\"l2_lambda={l2_lambda}, learning_rate={learning_rate}, learning_rate_decay={learning_rate_decay}, \"\n",
    "          f\"clipnorm={clipnorm}, units={units}, num_layers={num_layers}, batch_size={batch_size}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Set up Bayesian Optimization tuner to search for optimal hyperparameters.\n",
    "tuner = BayesianOptimization(\n",
    "    build_model,  # Model-building function.\n",
    "    objective=\"val_loss\",  # Target metric for optimization.\n",
    "    max_trials=30,  # Maximum number of trials to run.\n",
    "    executions_per_trial=1,  # Number of times to execute each trial.\n",
    "    directory=\"tuner_dir\",  # Directory to store tuning results.\n",
    "    project_name=\"lstm_tuning_capstone\",  # Tuning project name.\n",
    "    overwrite=True  # Overwrite existing tuner results if present.\n",
    ")\n",
    "\n",
    "# Define early stopping.\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=True)\n",
    "\n",
    "# Perform tuning with verbose logging.\n",
    "tuner.search(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_test_seq, y_test_seq),\n",
    "    epochs=200,\n",
    "    callbacks=[early_stopping],\n",
    "    shuffle=False,\n",
    "    verbose=1  # Ensure output of each trial.\n",
    ")\n",
    "\n",
    "# Retrieve the best model and hyperparameters.\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Best hyperparameters:\", best_hyperparameters.values)\n",
    "print(\"Best batch size:\", best_hyperparameters.get(\"batch_size\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e5f23364-cabc-468c-9870-f42ffeb72b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 501ms/step - loss: 6.9052 - val_loss: 2.9285\n",
      "Epoch 2/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 5.4525 - val_loss: 2.8964\n",
      "Epoch 3/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 5.1613 - val_loss: 2.8657\n",
      "Epoch 4/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 4.8672 - val_loss: 2.8357\n",
      "Epoch 5/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 4.6445 - val_loss: 2.8054\n",
      "Epoch 6/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 4.4816 - val_loss: 2.7751\n",
      "Epoch 7/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 4.6257 - val_loss: 2.7447\n",
      "Epoch 8/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 4.4580 - val_loss: 2.7144\n",
      "Epoch 9/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 4.3182 - val_loss: 2.6831\n",
      "Epoch 10/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 4.0937 - val_loss: 2.6509\n",
      "Epoch 11/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 4.0975 - val_loss: 2.6177\n",
      "Epoch 12/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 4.0798 - val_loss: 2.5872\n",
      "Epoch 13/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 3.9227 - val_loss: 2.5560\n",
      "Epoch 14/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 3.9737 - val_loss: 2.5250\n",
      "Epoch 15/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.8379 - val_loss: 2.4937\n",
      "Epoch 16/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 3.7693 - val_loss: 2.4579\n",
      "Epoch 17/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 3.6245 - val_loss: 2.4211\n",
      "Epoch 18/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 3.5403 - val_loss: 2.3848\n",
      "Epoch 19/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 3.4922 - val_loss: 2.3491\n",
      "Epoch 20/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 3.4302 - val_loss: 2.3137\n",
      "Epoch 21/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 3.3109 - val_loss: 2.2762\n",
      "Epoch 22/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 3.1962 - val_loss: 2.2378\n",
      "Epoch 23/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 2.9973 - val_loss: 2.1959\n",
      "Epoch 24/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 3.1884 - val_loss: 2.1552\n",
      "Epoch 25/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 3.1346 - val_loss: 2.1166\n",
      "Epoch 26/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 2.8784 - val_loss: 2.0744\n",
      "Epoch 27/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 2.7908 - val_loss: 2.0302\n",
      "Epoch 28/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 2.7724 - val_loss: 1.9842\n",
      "Epoch 29/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 2.6821 - val_loss: 1.9391\n",
      "Epoch 30/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 2.6639 - val_loss: 1.8947\n",
      "Epoch 31/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 2.5706 - val_loss: 1.8511\n",
      "Epoch 32/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 2.5867 - val_loss: 1.8073\n",
      "Epoch 33/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 2.3861 - val_loss: 1.7652\n",
      "Epoch 34/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 2.4141 - val_loss: 1.7258\n",
      "Epoch 35/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 2.3416 - val_loss: 1.6872\n",
      "Epoch 36/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 2.2821 - val_loss: 1.6501\n",
      "Epoch 37/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 2.2354 - val_loss: 1.6123\n",
      "Epoch 38/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 2.0941 - val_loss: 1.5715\n",
      "Epoch 39/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 2.0837 - val_loss: 1.5304\n",
      "Epoch 40/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 2.0927 - val_loss: 1.4915\n",
      "Epoch 41/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.9938 - val_loss: 1.4525\n",
      "Epoch 42/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 1.8849 - val_loss: 1.4122\n",
      "Epoch 43/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 1.9234 - val_loss: 1.3757\n",
      "Epoch 44/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.7893 - val_loss: 1.3398\n",
      "Epoch 45/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 1.7094 - val_loss: 1.3017\n",
      "Epoch 46/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.7373 - val_loss: 1.2642\n",
      "Epoch 47/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.6800 - val_loss: 1.2271\n",
      "Epoch 48/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 1.6689 - val_loss: 1.1923\n",
      "Epoch 49/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.5353 - val_loss: 1.1623\n",
      "Epoch 50/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.5349 - val_loss: 1.1306\n",
      "Epoch 51/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.5068 - val_loss: 1.0979\n",
      "Epoch 52/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.4499 - val_loss: 1.0655\n",
      "Epoch 53/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.4047 - val_loss: 1.0357\n",
      "Epoch 54/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.3168 - val_loss: 1.0032\n",
      "Epoch 55/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.2631 - val_loss: 0.9732\n",
      "Epoch 56/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 1.2849 - val_loss: 0.9462\n",
      "Epoch 57/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.2121 - val_loss: 0.9239\n",
      "Epoch 58/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 1.1587 - val_loss: 0.9036\n",
      "Epoch 59/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 1.1279 - val_loss: 0.8785\n",
      "Epoch 60/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.1122 - val_loss: 0.8531\n",
      "Epoch 61/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.0557 - val_loss: 0.8309\n",
      "Epoch 62/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.0323 - val_loss: 0.8106\n",
      "Epoch 63/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.9994 - val_loss: 0.7913\n",
      "Epoch 64/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.0416 - val_loss: 0.7709\n",
      "Epoch 65/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.9942 - val_loss: 0.7519\n",
      "Epoch 66/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.9416 - val_loss: 0.7356\n",
      "Epoch 67/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.9029 - val_loss: 0.7178\n",
      "Epoch 68/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.8925 - val_loss: 0.7027\n",
      "Epoch 69/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.8906 - val_loss: 0.6866\n",
      "Epoch 70/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 0.8671 - val_loss: 0.6726\n",
      "Epoch 71/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.8397 - val_loss: 0.6591\n",
      "Epoch 72/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.8104 - val_loss: 0.6425\n",
      "Epoch 73/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.7841 - val_loss: 0.6279\n",
      "Epoch 74/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.7760 - val_loss: 0.6150\n",
      "Epoch 75/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.7314 - val_loss: 0.6046\n",
      "Epoch 76/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.7188 - val_loss: 0.5941\n",
      "Epoch 77/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.6846 - val_loss: 0.5803\n",
      "Epoch 78/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.6836 - val_loss: 0.5678\n",
      "Epoch 79/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.6479 - val_loss: 0.5564\n",
      "Epoch 80/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.6667 - val_loss: 0.5461\n",
      "Epoch 81/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.6339 - val_loss: 0.5364\n",
      "Epoch 82/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.6343 - val_loss: 0.5259\n",
      "Epoch 83/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.6226 - val_loss: 0.5157\n",
      "Epoch 84/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.5968 - val_loss: 0.5064\n",
      "Epoch 85/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.5941 - val_loss: 0.4977\n",
      "Epoch 86/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.5726 - val_loss: 0.4881\n",
      "Epoch 87/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.5713 - val_loss: 0.4786\n",
      "Epoch 88/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.5427 - val_loss: 0.4704\n",
      "Epoch 89/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.5498 - val_loss: 0.4620\n",
      "Epoch 90/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.5195 - val_loss: 0.4542\n",
      "Epoch 91/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.5230 - val_loss: 0.4470\n",
      "Epoch 92/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.5090 - val_loss: 0.4395\n",
      "Epoch 93/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.4847 - val_loss: 0.4316\n",
      "Epoch 94/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.4913 - val_loss: 0.4238\n",
      "Epoch 95/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.4590 - val_loss: 0.4154\n",
      "Epoch 96/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.4512 - val_loss: 0.4088\n",
      "Epoch 97/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.4545 - val_loss: 0.4044\n",
      "Epoch 98/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.4398 - val_loss: 0.3976\n",
      "Epoch 99/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.4417 - val_loss: 0.3912\n",
      "Epoch 100/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.4360 - val_loss: 0.3856\n",
      "Epoch 101/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.4250 - val_loss: 0.3813\n",
      "Epoch 102/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.4173 - val_loss: 0.3749\n",
      "Epoch 103/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.4069 - val_loss: 0.3683\n",
      "Epoch 104/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.4099 - val_loss: 0.3632\n",
      "Epoch 105/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.4000 - val_loss: 0.3571\n",
      "Epoch 106/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.3908 - val_loss: 0.3513\n",
      "Epoch 107/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.3645 - val_loss: 0.3459\n",
      "Epoch 108/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.3747 - val_loss: 0.3408\n",
      "Epoch 109/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.3753 - val_loss: 0.3361\n",
      "Epoch 110/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.3557 - val_loss: 0.3319\n",
      "Epoch 111/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.3489 - val_loss: 0.3273\n",
      "Epoch 112/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.3527 - val_loss: 0.3231\n",
      "Epoch 113/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.3281 - val_loss: 0.3189\n",
      "Epoch 114/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.3350 - val_loss: 0.3143\n",
      "Epoch 115/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.3364 - val_loss: 0.3106\n",
      "Epoch 116/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.3318 - val_loss: 0.3073\n",
      "Epoch 117/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.3289 - val_loss: 0.3032\n",
      "Epoch 118/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.3082 - val_loss: 0.2987\n",
      "Epoch 119/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.3104 - val_loss: 0.2949\n",
      "Epoch 120/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.3086 - val_loss: 0.2918\n",
      "Epoch 121/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.3065 - val_loss: 0.2881\n",
      "Epoch 122/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.2978 - val_loss: 0.2847\n",
      "Epoch 123/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.2917 - val_loss: 0.2820\n",
      "Epoch 124/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.2865 - val_loss: 0.2783\n",
      "Epoch 125/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.2740 - val_loss: 0.2741\n",
      "Epoch 126/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.2832 - val_loss: 0.2709\n",
      "Epoch 127/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.2741 - val_loss: 0.2682\n",
      "Epoch 128/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.2713 - val_loss: 0.2653\n",
      "Epoch 129/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.2679 - val_loss: 0.2621\n",
      "Epoch 130/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.2659 - val_loss: 0.2593\n",
      "Epoch 131/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2603 - val_loss: 0.2564\n",
      "Epoch 132/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2589 - val_loss: 0.2534\n",
      "Epoch 133/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2548 - val_loss: 0.2502\n",
      "Epoch 134/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.2525 - val_loss: 0.2476\n",
      "Epoch 135/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2502 - val_loss: 0.2454\n",
      "Epoch 136/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.2454 - val_loss: 0.2429\n",
      "Epoch 137/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.2411 - val_loss: 0.2397\n",
      "Epoch 138/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.2364 - val_loss: 0.2367\n",
      "Epoch 139/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.2339 - val_loss: 0.2344\n",
      "Epoch 140/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.2319 - val_loss: 0.2325\n",
      "Epoch 141/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.2340 - val_loss: 0.2309\n",
      "Epoch 142/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2308 - val_loss: 0.2289\n",
      "Epoch 143/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2248 - val_loss: 0.2265\n",
      "Epoch 144/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2155 - val_loss: 0.2240\n",
      "Epoch 145/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.2223 - val_loss: 0.2221\n",
      "Epoch 146/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.2181 - val_loss: 0.2203\n",
      "Epoch 147/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.2168 - val_loss: 0.2184\n",
      "Epoch 148/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.2130 - val_loss: 0.2169\n",
      "Epoch 149/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.2103 - val_loss: 0.2147\n",
      "Epoch 150/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.2058 - val_loss: 0.2127\n",
      "Epoch 151/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.2062 - val_loss: 0.2107\n",
      "Epoch 152/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.2015 - val_loss: 0.2084\n",
      "Epoch 153/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.2033 - val_loss: 0.2065\n",
      "Epoch 154/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.1970 - val_loss: 0.2048\n",
      "Epoch 155/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.1977 - val_loss: 0.2030\n",
      "Epoch 156/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.1917 - val_loss: 0.2016\n",
      "Epoch 157/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.1946 - val_loss: 0.2003\n",
      "Epoch 158/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.1936 - val_loss: 0.1982\n",
      "Epoch 159/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1889 - val_loss: 0.1963\n",
      "Epoch 160/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.1922 - val_loss: 0.1949\n",
      "Epoch 161/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.1855 - val_loss: 0.1937\n",
      "Epoch 162/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.1864 - val_loss: 0.1926\n",
      "Epoch 163/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1808 - val_loss: 0.1908\n",
      "Epoch 164/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1814 - val_loss: 0.1895\n",
      "Epoch 165/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.1791 - val_loss: 0.1879\n",
      "Epoch 166/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.1765 - val_loss: 0.1863\n",
      "Epoch 167/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1751 - val_loss: 0.1850\n",
      "Epoch 168/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.1750 - val_loss: 0.1837\n",
      "Epoch 169/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.1756 - val_loss: 0.1824\n",
      "Epoch 170/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.1721 - val_loss: 0.1807\n",
      "Epoch 171/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.1688 - val_loss: 0.1795\n",
      "Epoch 172/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1717 - val_loss: 0.1785\n",
      "Epoch 173/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1685 - val_loss: 0.1772\n",
      "Epoch 174/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1688 - val_loss: 0.1763\n",
      "Epoch 175/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1658 - val_loss: 0.1754\n",
      "Epoch 176/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1648 - val_loss: 0.1738\n",
      "Epoch 177/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.1637 - val_loss: 0.1723\n",
      "Epoch 178/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.1599 - val_loss: 0.1713\n",
      "Epoch 179/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1605 - val_loss: 0.1703\n",
      "Epoch 180/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.1612 - val_loss: 0.1693\n",
      "Epoch 181/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1589 - val_loss: 0.1686\n",
      "Epoch 182/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1572 - val_loss: 0.1672\n",
      "Epoch 183/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.1578 - val_loss: 0.1660\n",
      "Epoch 184/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.1543 - val_loss: 0.1654\n",
      "Epoch 185/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.1557 - val_loss: 0.1646\n",
      "Epoch 186/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.1546 - val_loss: 0.1634\n",
      "Epoch 187/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.1528 - val_loss: 0.1626\n",
      "Epoch 188/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.1525 - val_loss: 0.1620\n",
      "Epoch 189/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.1483 - val_loss: 0.1611\n",
      "Epoch 190/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.1490 - val_loss: 0.1597\n",
      "Epoch 191/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.1482 - val_loss: 0.1584\n",
      "Epoch 192/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1485 - val_loss: 0.1580\n",
      "Epoch 193/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.1462 - val_loss: 0.1576\n",
      "Epoch 194/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.1464 - val_loss: 0.1572\n",
      "Epoch 195/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.1470 - val_loss: 0.1562\n",
      "Epoch 196/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.1454 - val_loss: 0.1547\n",
      "Epoch 197/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.1446 - val_loss: 0.1535\n",
      "Epoch 198/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1444 - val_loss: 0.1531\n",
      "Epoch 199/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.1414 - val_loss: 0.1528\n",
      "Epoch 200/200\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.1407 - val_loss: 0.1525\n",
      "Final Training Loss: 0.15091842412948608\n",
      "Final Validation Loss: 0.15249133110046387\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Best hyperparameters found from tuning\n",
    "best_params = {\n",
    "    'units1': 32,\n",
    "    'units2': 32,\n",
    "    'units3': 32,\n",
    "    'recurrent_dropout': 0.2,\n",
    "    'optimizer': 'adam',\n",
    "    'loss_function': 'mean_squared_error',\n",
    "    'learning_rate_decay': 1e-05,\n",
    "    'learning_rate': 0.0005,\n",
    "    'l2_lambda': 0.0250642064021517066,\n",
    "    'epochs': 200,\n",
    "    'dropout_rate': 0.4,\n",
    "    'clipnorm': 1.0,\n",
    "    'batch_size': 120\n",
    "}\n",
    "\n",
    "# Build the model using the best parameters\n",
    "def build_best_model(params):\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units1'], return_sequences=True, \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "                   kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Second LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units2'], return_sequences=True, kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Third LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units3'], return_sequences=False, kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Select the optimizer\n",
    "    if params['optimizer'] == 'adam':\n",
    "        optimizer = Adam(learning_rate=params['learning_rate'], decay=params['learning_rate_decay'], clipnorm=params['clipnorm'])\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizer '{params['optimizer']}' is not implemented in this setup.\")\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate the model with the best hyperparameters\n",
    "model = build_best_model(best_params)\n",
    "\n",
    "# Early stopping to stop training when validation loss stops improving\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq, \n",
    "    epochs=best_params['epochs'], \n",
    "    batch_size=best_params['batch_size'], \n",
    "    validation_data=(X_test_seq, y_test_seq),\n",
    "    shuffle=False,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Print the final training and validation loss\n",
    "print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "115e32cf-93e1-4417-a9ed-1ca5722b7ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 844ms/step\n",
      "LSTM Model Performance:\n",
      "Training RMSE: 0.020354193093580174\n",
      "Test RMSE: 0.023208732397406222\n",
      "Training MAE: 0.014738026159912635\n",
      "Test MAE: 0.017424588932222252\n",
      "Directional Accuracy on Training Data: 51.680185399768256%\n",
      "Directional Accuracy on Test Data: 48.59154929577465%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict on both training and test sequences\n",
    "y_train_pred = model.predict(X_train_seq)\n",
    "y_test_pred = model.predict(X_test_seq)\n",
    "\n",
    "# Inverse scale predictions and actual values to original scale\n",
    "y_train_pred_rescaled = scaler_y.inverse_transform(y_train_pred)\n",
    "y_test_pred_rescaled = scaler_y.inverse_transform(y_test_pred)\n",
    "y_train_actual_rescaled = scaler_y.inverse_transform(y_train_seq)\n",
    "y_test_actual_rescaled = scaler_y.inverse_transform(y_test_seq)\n",
    "\n",
    "# Define a function for directional accuracy\n",
    "def directional_accuracy(y_true, y_pred):\n",
    "    return np.mean((np.sign(y_true[1:] - y_true[:-1]) == np.sign(y_pred[1:] - y_pred[:-1])).astype(int)) * 100\n",
    "\n",
    "# Calculate metrics.\n",
    "# Root Mean Squared Error (RMSE) for training set\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_actual_rescaled, y_train_pred_rescaled))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_actual_rescaled, y_test_pred_rescaled)) # RMSE for test set.\n",
    "train_mae = mean_absolute_error(y_train_actual_rescaled, y_train_pred_rescaled) # Mean Absolute Error (MAE) for training set.\n",
    "test_mae = mean_absolute_error(y_test_actual_rescaled, y_test_pred_rescaled) # MAE for test set.\n",
    "# Directional accuracy for training set.\n",
    "train_directional_accuracy = directional_accuracy(y_train_actual_rescaled.flatten(), y_train_pred_rescaled.flatten())\n",
    "# Directional accuracy for test set.\n",
    "test_directional_accuracy = directional_accuracy(y_test_actual_rescaled.flatten(), y_test_pred_rescaled.flatten())\n",
    "\n",
    "# Display score metrics to evaluate the model's performance.\n",
    "print(\"LSTM Model Performance:\")\n",
    "print(f\"Training RMSE: {train_rmse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Training MAE: {train_mae}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Directional Accuracy on Training Data: {train_directional_accuracy}%\")\n",
    "print(f\"Directional Accuracy on Test Data: {test_directional_accuracy}%\")\n",
    "\n",
    "# Plot training & validation loss values, it will help visualize how well the model fits the data over training epochs.\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417e932e-90c7-4c94-b629-a82300174c2a",
   "metadata": {},
   "source": [
    "### Model Performance Metrics for Sequence Length = 12\n",
    "\n",
    "| Metric         | Model 1       | Model 2       | Model 3       | Model 4       |\n",
    "|-----------------|---------------|---------------|---------------|---------------|\n",
    "| **Train RMSE** | 0.020650653   | 0.020370913   | 0.020398293   | 0.020354193   |\n",
    "| **Test RMSE**  | 0.023987205   | 0.023233765   | 0.023338731   | 0.023208732   |\n",
    "| **Train MAE**  | 0.014889154   | 0.014775607   | 0.014705355   | 0.014738026   |\n",
    "| **Test MAE**   | 0.018062195   | 0.017562497   | 0.017482876   | 0.017424589   |\n",
    "| **Train Loss** | 0.145611078   | 0.733473957   | 0.127861217   | 0.150918424   |\n",
    "| **Val Loss**   | 0.175645962   | 0.324159324   | 0.131033525   | 0.152491331   |\n",
    "| **Train DA**   | 59%           | 48%           | 52%           | 52%           |\n",
    "| **Test DA**    | 54%           | 51%           | 51%           | 49%           |\n",
    "\n",
    "The model 2, 3 and 4 have similiar RMSE and MAE metrics, but looking at the train and val loss model 3 has the lowest val loss and its curve decreases smoother than other models (2,4). **Thus, the optimal model for the sequence length of 12 is the model 3**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05494e65-ab6a-4ad7-85c7-f9535ef596ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 166ms/step\n",
      "Fold 1 RMSE: 0.46023922150572516\n",
      "Fold 2\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "Fold 2 RMSE: 0.3380093209030955\n",
      "Fold 3\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "Fold 3 RMSE: 0.2863619899364502\n",
      "Fold 4\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Fold 4 RMSE: 0.40359923142451254\n",
      "Average RMSE from TSCV: 0.37205244094244583\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# Best hyperparameters found from tuning\n",
    "best_params = {\n",
    "    'units1': 32,\n",
    "    'units2': 32,\n",
    "    'units3': 32,\n",
    "    'recurrent_dropout': 0.1,\n",
    "    'optimizer': 'adam',\n",
    "    'loss_function': 'mean_squared_error',\n",
    "    'learning_rate_decay': 0,\n",
    "    'learning_rate': 0.001,\n",
    "    'l2_lambda': 0.042448918017048726,\n",
    "    'epochs': 200,\n",
    "    'dropout_rate': 0.4,\n",
    "    'clipnorm': 5.0,\n",
    "    'batch_size': 120\n",
    "}\n",
    "\n",
    "# Build the model using the best parameters\n",
    "def build_best_model(params):\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units1'], return_sequences=True, \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "                   kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Second LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units2'], return_sequences=True, kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Third LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units3'], return_sequences=False, kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Select the optimizer\n",
    "    optimizer = Adam(learning_rate=params['learning_rate'], \n",
    "                     decay=params['learning_rate_decay'], \n",
    "                     clipnorm=params['clipnorm'])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss=params['loss_function'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define the number of splits for Time Series Cross-Validation\n",
    "n_splits = 4\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Initialize the list to store RMSE scores\n",
    "tscv_rmse_scores = []\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Create the model once and save its initial weights\n",
    "model = build_best_model(best_params)\n",
    "initial_weights = model.get_weights()\n",
    "\n",
    "# Perform TSCV\n",
    "for fold, (train_index, test_index) in enumerate(tscv.split(X_train_scaled)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "    # Reset model to initial weights\n",
    "    model.set_weights(initial_weights)\n",
    "    \n",
    "    # Define train and test sets\n",
    "    train, test = X_train_scaled[train_index], X_train_scaled[test_index]\n",
    "    y_train_fold, y_test_fold = y_train_scaled[train_index], y_train_scaled[test_index]\n",
    "    \n",
    "    # Create sequences\n",
    "    X_train_seq, y_train_seq = [], []\n",
    "    for i in range(len(train) - sequence_length):\n",
    "        X_train_seq.append(train[i:i + sequence_length])\n",
    "        y_train_seq.append(y_train_fold[i + sequence_length])\n",
    "    \n",
    "    X_test_seq, y_test_seq = [], []\n",
    "    for i in range(len(test) - sequence_length):\n",
    "        X_test_seq.append(test[i:i + sequence_length])\n",
    "        y_test_seq.append(y_test_fold[i + sequence_length])\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_train_seq, y_train_seq = np.array(X_train_seq), np.array(y_train_seq)\n",
    "    X_test_seq, y_test_seq = np.array(X_test_seq), np.array(y_test_seq)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=best_params['epochs'],\n",
    "        batch_size=best_params['batch_size'],\n",
    "        validation_data=(X_test_seq, y_test_seq),\n",
    "        shuffle=False,\n",
    "        verbose=0,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = model.predict(X_test_seq)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_seq, y_pred))\n",
    "    tscv_rmse_scores.append(rmse)\n",
    "    print(f\"Fold {fold + 1} RMSE: {rmse}\")\n",
    "\n",
    "# Calculate the average RMSE across all folds\n",
    "avg_rmse = np.mean(tscv_rmse_scores)\n",
    "print(f\"Average RMSE from TSCV: {avg_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1987ab48-72f6-48ea-87bd-4d39462c40f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling window starting at index 0\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 831ms/step\n",
      "Rolling window RMSE: 0.47248661545793624\n",
      "Rolling window starting at index 50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 985ms/step\n",
      "Rolling window RMSE: 0.459866164038996\n",
      "Rolling window starting at index 100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 950ms/step\n",
      "Rolling window RMSE: 0.31559642304483865\n",
      "Rolling window starting at index 150\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 827ms/step\n",
      "Rolling window RMSE: 0.2989637805595658\n",
      "Rolling window starting at index 200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step  \n",
      "Rolling window RMSE: 0.29269575369105516\n",
      "Rolling window starting at index 250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 839ms/step\n",
      "Rolling window RMSE: 0.3224644768210686\n",
      "Rolling window starting at index 300\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 934ms/step\n",
      "Rolling window RMSE: 0.3196362505415342\n",
      "Rolling window starting at index 350\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 835ms/step\n",
      "Rolling window RMSE: 0.22581472588850626\n",
      "Rolling window starting at index 400\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 843ms/step\n",
      "Rolling window RMSE: 0.2891084432353152\n",
      "Rolling window starting at index 450\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 832ms/step\n",
      "Rolling window RMSE: 0.3344826689104992\n",
      "Rolling window starting at index 500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step\n",
      "Rolling window RMSE: 0.5086132125722709\n",
      "Average Rolling Window RMSE: 0.34906622861468967\n"
     ]
    }
   ],
   "source": [
    "# Parameters for FRWCV\n",
    "train_window = 300  # Training window size\n",
    "test_window = 50    # Test window size\n",
    "\n",
    "# Store RMSEs for each window\n",
    "rolling_rmse_scores = []\n",
    "\n",
    "for start in range(0, len(X_train_scaled) - train_window - test_window, test_window):\n",
    "    print(f\"Rolling window starting at index {start}\")\n",
    "    \n",
    "    # Define train and test sets for the window\n",
    "    train = X_train_scaled[start:start + train_window]\n",
    "    test = X_train_scaled[start + train_window:start + train_window + test_window]\n",
    "    y_train_fold = y_train_scaled[start:start + train_window]\n",
    "    y_test_fold = y_train_scaled[start + train_window:start + train_window + test_window]\n",
    "    \n",
    "    # Create sequences for train and test\n",
    "    X_train_seq, y_train_seq = [], []\n",
    "    for i in range(len(train) - sequence_length):\n",
    "        X_train_seq.append(train[i:i + sequence_length])\n",
    "        y_train_seq.append(y_train_fold[i + sequence_length])\n",
    "    \n",
    "    X_test_seq, y_test_seq = [], []\n",
    "    for i in range(len(test) - sequence_length):\n",
    "        X_test_seq.append(test[i:i + sequence_length])\n",
    "        y_test_seq.append(y_test_fold[i + sequence_length])\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_train_seq, y_train_seq = np.array(X_train_seq), np.array(y_train_seq)\n",
    "    X_test_seq, y_test_seq = np.array(X_test_seq), np.array(y_test_seq)\n",
    "    \n",
    "    # Build and train the model\n",
    "    model = build_best_model(best_params)\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq, \n",
    "        epochs=best_params['epochs'], \n",
    "        batch_size=best_params['batch_size'], \n",
    "        validation_data=(X_test_seq, y_test_seq), \n",
    "        shuffle=False,\n",
    "        verbose=0,  # Suppress training logs for brevity\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    y_pred = model.predict(X_test_seq)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_seq, y_pred))\n",
    "    rolling_rmse_scores.append(rmse)\n",
    "    print(f\"Rolling window RMSE: {rmse}\")\n",
    "\n",
    "# Calculate average RMSE across rolling windows\n",
    "avg_rolling_rmse = np.mean(rolling_rmse_scores)\n",
    "print(f\"Average Rolling Window RMSE: {avg_rolling_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5356e9e6-e205-410f-a403-d7cd47827e2d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sequence Length = 26 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da6f0b5f-7190-4a79-ac7d-86b8b683a97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped X_train_seq shape: (850, 26, 5)\n",
      "Reshaped y_train_seq shape: (850, 1)\n",
      "Reshaped X_test_seq shape: (129, 26, 5)\n",
      "Reshaped  y_test_seq shape: (129, 1)\n"
     ]
    }
   ],
   "source": [
    "# Set the sequence length for LSTM input, representing the number of time steps.\n",
    "sequence_length = 26  # Number of time steps used for target prediction, 26 weeks (Six-Months).\n",
    "\n",
    "# Reshape data into sequences for LSTM.\n",
    "# Initialize empty lists to hold the sequences for the training set.\n",
    "X_train_seq, y_train_seq = [], []\n",
    "# Loop through the training data to create sequences of features and corresponding target values.\n",
    "for i in range(len(X_train_scaled) - sequence_length):\n",
    "    # Append a sequence of features for the current window.\n",
    "    X_train_seq.append(X_train_scaled[i:i + sequence_length]) # Sequence of features.\n",
    "    # Append the target value that comes immediately after the sequence.\n",
    "    y_train_seq.append(y_train_scaled[i + sequence_length]) # Target value following the sequence.\n",
    "# Initialize empty lists to hold the sequences for the test set    \n",
    "X_test_seq, y_test_seq = [], []\n",
    "# Same process for the test sets.\n",
    "for i in range(len(X_test_scaled) - sequence_length):\n",
    "    # Append a sequence of features for the current window in the test set.\n",
    "    X_test_seq.append(X_test_scaled[i:i + sequence_length])\n",
    "    # Append the target value immediately after the sequence in the test set.\n",
    "    y_test_seq.append(y_test_scaled[i + sequence_length])\n",
    "\n",
    "# Convert lists to numpy arrays to use in Neural Networks.\n",
    "X_train_seq, y_train_seq = np.array(X_train_seq), np.array(y_train_seq)\n",
    "X_test_seq, y_test_seq = np.array(X_test_seq), np.array(y_test_seq)\n",
    "\n",
    "# Print the reshaped input data for LSTM.\n",
    "print(f\"Reshaped X_train_seq shape: {X_train_seq.shape}\")\n",
    "print(f\"Reshaped y_train_seq shape: {y_train_seq.shape}\")\n",
    "print(f\"Reshaped X_test_seq shape: {X_test_seq.shape}\")\n",
    "print(f\"Reshaped  y_test_seq shape: { y_test_seq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c74c9e5-cb06-445f-a235-040d6490b95d",
   "metadata": {},
   "source": [
    "### 1. Random Search (Two Layers - Model 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cbb4b950-c734-479f-9775-8d1d3aad1442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running combination 1/30: {'units2': 32, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 64}\n",
      "Final Training Loss: 0.1448913812637329\n",
      "Final Validation Loss: 0.14419759809970856\n",
      "Running combination 2/30: {'units2': 128, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 0.6533531546592712\n",
      "Final Validation Loss: 0.378877729177475\n",
      "Running combination 3/30: {'units2': 64, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 64}\n",
      "Final Training Loss: 0.9585232138633728\n",
      "Final Validation Loss: 0.28127601742744446\n",
      "Running combination 4/30: {'units2': 64, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 50, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 32}\n",
      "Final Training Loss: 5.876496315002441\n",
      "Final Validation Loss: 5.0339555740356445\n",
      "Running combination 5/30: {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 5.0, 'batch_size': 64}\n",
      "Final Training Loss: 2.0496175289154053\n",
      "Final Validation Loss: 1.4198899269104004\n",
      "Running combination 6/30: {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 120}\n",
      "Final Training Loss: 0.6359370946884155\n",
      "Final Validation Loss: 0.20177783071994781\n",
      "Running combination 7/30: {'units2': 32, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 64}\n",
      "Final Training Loss: 9.166717529296875\n",
      "Final Validation Loss: 8.199868202209473\n",
      "Running combination 8/30: {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 50, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 0.9055909514427185\n",
      "Final Validation Loss: 0.6029587984085083\n",
      "Running combination 9/30: {'units2': 64, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 1.8151153326034546\n",
      "Final Validation Loss: 1.3292568922042847\n",
      "Running combination 10/30: {'units2': 32, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 0.4576721787452698\n",
      "Final Validation Loss: 0.2548781633377075\n",
      "Running combination 11/30: {'units2': 64, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 32}\n",
      "Final Training Loss: 5.516371726989746\n",
      "Final Validation Loss: 4.97318696975708\n",
      "Running combination 12/30: {'units2': 32, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 0.7018391489982605\n",
      "Final Validation Loss: 0.20769362151622772\n",
      "Running combination 13/30: {'units2': 128, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 3.137270450592041\n",
      "Final Validation Loss: 2.4358043670654297\n",
      "Running combination 14/30: {'units2': 64, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 256}\n",
      "Final Training Loss: 0.7803283929824829\n",
      "Final Validation Loss: 0.23003101348876953\n",
      "Running combination 15/30: {'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 32}\n",
      "Final Training Loss: 0.8243833184242249\n",
      "Final Validation Loss: 0.29483383893966675\n",
      "Running combination 16/30: {'units2': 32, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 256}\n",
      "Final Training Loss: 1.7201519012451172\n",
      "Final Validation Loss: 1.0568679571151733\n",
      "Running combination 17/30: {'units2': 128, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 5.0, 'batch_size': 120}\n",
      "Final Training Loss: 10.61550521850586\n",
      "Final Validation Loss: 9.124848365783691\n",
      "Running combination 18/30: {'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 200, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 13.044328689575195\n",
      "Final Validation Loss: 12.3508939743042\n",
      "Running combination 19/30: {'units2': 128, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 18.724903106689453\n",
      "Final Validation Loss: 17.961021423339844\n",
      "Running combination 20/30: {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 1.6812453269958496\n",
      "Final Validation Loss: 0.8214054107666016\n",
      "Running combination 21/30: {'units2': 32, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.001, 'l2_lambda': 0.01, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 0.4869779348373413\n",
      "Final Validation Loss: 0.3430718779563904\n",
      "Running combination 22/30: {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.01, 'epochs': 50, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 1.2450647354125977\n",
      "Final Validation Loss: 0.6309341192245483\n",
      "Running combination 23/30: {'units2': 32, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 50, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 6.128774166107178\n",
      "Final Validation Loss: 5.177249908447266\n",
      "Running combination 24/30: {'units2': 64, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 2.260653495788574\n",
      "Final Validation Loss: 1.7658694982528687\n",
      "Running combination 25/30: {'units2': 32, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 256}\n",
      "Final Training Loss: 14.095924377441406\n",
      "Final Validation Loss: 13.146766662597656\n",
      "Running combination 26/30: {'units2': 32, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 64}\n",
      "Final Training Loss: 1.5537630319595337\n",
      "Final Validation Loss: 0.644809901714325\n",
      "Running combination 27/30: {'units2': 64, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 1.5495131015777588\n",
      "Final Validation Loss: 0.3366524577140808\n",
      "Running combination 28/30: {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 0.4011531472206116\n",
      "Final Validation Loss: 0.2127165049314499\n",
      "Running combination 29/30: {'units2': 128, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0001, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 256}\n",
      "Final Training Loss: 0.8976736068725586\n",
      "Final Validation Loss: 0.29249364137649536\n",
      "Running combination 30/30: {'units2': 32, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 200, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 64}\n",
      "Final Training Loss: 4.478668212890625\n",
      "Final Validation Loss: 3.9197986125946045\n",
      "Top results:\n",
      "{'params': {'units2': 32, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 64}, 'final_train_loss': 0.1448913812637329, 'final_val_loss': 0.14419759809970856}\n",
      "{'params': {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 120}, 'final_train_loss': 0.6359370946884155, 'final_val_loss': 0.20177783071994781}\n",
      "{'params': {'units2': 32, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 120}, 'final_train_loss': 0.7018391489982605, 'final_val_loss': 0.20769362151622772}\n",
      "{'params': {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 32}, 'final_train_loss': 0.4011531472206116, 'final_val_loss': 0.2127165049314499}\n",
      "{'params': {'units2': 64, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 256}, 'final_train_loss': 0.7803283929824829, 'final_val_loss': 0.23003101348876953}\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Define parameter grid for random hyperparameter search.\n",
    "param_grid = {\n",
    "    'dropout_rate': [0.2, 0.3, 0.4],               # Dropout rate for regularization.\n",
    "    'recurrent_dropout': [0.1, 0.2],               # Recurrent dropout within LSTM layers.\n",
    "    'l2_lambda': [0.001, 0.01, 0.1],               # L2 regularization strength.\n",
    "    'learning_rate': [0.001, 0.0005, 0.0001],      # Learning rate for optimizer.\n",
    "    'learning_rate_decay': [1e-6, 1e-5, 0],        # Decay rate for learning rate over time.\n",
    "    'units1': [32, 64, 128],                       # Number of units in the first LSTM layer.\n",
    "    'units2': [32, 64, 128],                       # Number of units in the second LSTM layer.\n",
    "    'batch_size': [32, 64, 120, 256],              # Batch size for training.\n",
    "    'epochs': [50, 100, 200],                      # Number of epochs to train.\n",
    "    'optimizer': ['adam'],                         # Optimizer to use.\n",
    "    'clipnorm': [1.0, 5.0]                         # Gradient clipping to avoid exploding gradients.\n",
    "}\n",
    "\n",
    "# Generate a list of random combinations of hyperparameters.\n",
    "n_iter_search = 30  # Number of random combinations to attempt.\n",
    "# Random state being applied for reproducibility. \n",
    "random_combinations = list(ParameterSampler(param_grid, n_iter=n_iter_search, random_state=42)) \n",
    "\n",
    "# Define a function to build the LSTM model with given parameters.\n",
    "def build_model(dropout_rate, recurrent_dropout, l2_lambda, learning_rate, learning_rate_decay, clipnorm, units1, units2, optimizer_name, loss_function):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First LSTM layer with BatchNorm, Dropout, and recurrent_dropout.\n",
    "    model.add(LSTM(units=units1, return_sequences=True, \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),  # Input shape based on sequence data.\n",
    "                   kernel_regularizer=l2(l2_lambda), recurrent_dropout=recurrent_dropout))\n",
    "    model.add(BatchNormalization())  # Batch normalization for stable training.\n",
    "    model.add(Dropout(dropout_rate))  # Dropout for regularization.\n",
    "    \n",
    "    # Second LSTM layer with BatchNorm, Dropout, and recurrent_dropout.\n",
    "    model.add(LSTM(units=units2, return_sequences=False, kernel_regularizer=l2(l2_lambda), recurrent_dropout=recurrent_dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer with a single unit for regression output.\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Select optimizer with learning rate decay and gradient clipping.\n",
    "    optimizer = Adam(learning_rate=learning_rate, decay=learning_rate_decay, clipnorm=clipnorm)\n",
    "    \n",
    "    # Compile the model with the chosen optimizer and loss function.\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define early stopping to stop training when validation loss does not improve.\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Placeholder to store results of each model configuration.\n",
    "results = []\n",
    "\n",
    "# Run random search\n",
    "for i, params in enumerate(random_combinations):\n",
    "    print(f\"Running combination {i+1}/{len(random_combinations)}: {params}\")\n",
    "    \n",
    "    # Build model with the current parameters\n",
    "    model = build_model(\n",
    "        dropout_rate=params['dropout_rate'],\n",
    "        recurrent_dropout=params['recurrent_dropout'],\n",
    "        l2_lambda=params['l2_lambda'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        learning_rate_decay=params['learning_rate_decay'],\n",
    "        clipnorm=params['clipnorm'],\n",
    "        units1=params['units1'],\n",
    "        units2=params['units2'],\n",
    "        optimizer_name=params['optimizer'],\n",
    "        loss_function='mean_squared_error'\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq, \n",
    "        epochs=params['epochs'], \n",
    "        batch_size=params['batch_size'], \n",
    "        validation_data=(X_test_seq, y_test_seq), \n",
    "        callbacks=[early_stopping],\n",
    "        shuffle=False,\n",
    "        verbose=0  # Set verbose=0 to reduce output\n",
    "    )\n",
    "    \n",
    "    # Get final validation loss and training loss\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    \n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'params': params,\n",
    "        'final_train_loss': final_train_loss,\n",
    "        'final_val_loss': final_val_loss\n",
    "    })\n",
    "    \n",
    "    print(f\"Final Training Loss: {final_train_loss}\")\n",
    "    print(f\"Final Validation Loss: {final_val_loss}\")\n",
    "\n",
    "results = sorted(results, key=lambda x: x['final_val_loss'])\n",
    "print(\"Top results:\")\n",
    "for res in results[:5]:  # Show top 5 results\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c232eb43-61da-4791-8436-155f02b34149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 276ms/step - loss: 3.2324 - val_loss: 0.7203\n",
      "Epoch 2/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 2.1663 - val_loss: 0.7147\n",
      "Epoch 3/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 1.9079 - val_loss: 0.7077\n",
      "Epoch 4/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.6410 - val_loss: 0.6939\n",
      "Epoch 5/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 1.6786 - val_loss: 0.6808\n",
      "Epoch 6/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 1.5574 - val_loss: 0.6718\n",
      "Epoch 7/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.5120 - val_loss: 0.6641\n",
      "Epoch 8/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 1.4291 - val_loss: 0.6565\n",
      "Epoch 9/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 1.4371 - val_loss: 0.6502\n",
      "Epoch 10/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 1.5034 - val_loss: 0.6434\n",
      "Epoch 11/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 1.2741 - val_loss: 0.6395\n",
      "Epoch 12/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 1.2860 - val_loss: 0.6353\n",
      "Epoch 13/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 1.2807 - val_loss: 0.6309\n",
      "Epoch 14/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 1.2688 - val_loss: 0.6262\n",
      "Epoch 15/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 1.1460 - val_loss: 0.6215\n",
      "Epoch 16/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 1.2603 - val_loss: 0.6174\n",
      "Epoch 17/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 1.1272 - val_loss: 0.6158\n",
      "Epoch 18/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.1340 - val_loss: 0.6015\n",
      "Epoch 19/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 1.0813 - val_loss: 0.5890\n",
      "Epoch 20/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 1.1529 - val_loss: 0.5893\n",
      "Epoch 21/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.0279 - val_loss: 0.5895\n",
      "Epoch 22/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 1.0108 - val_loss: 0.5807\n",
      "Epoch 23/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.0071 - val_loss: 0.5740\n",
      "Epoch 24/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.9998 - val_loss: 0.5702\n",
      "Epoch 25/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.9569 - val_loss: 0.5719\n",
      "Epoch 26/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.9194 - val_loss: 0.5783\n",
      "Epoch 27/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.9851 - val_loss: 0.5766\n",
      "Epoch 28/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.8619 - val_loss: 0.5914\n",
      "Epoch 29/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.9192 - val_loss: 0.5894\n",
      "Epoch 30/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.8809 - val_loss: 0.5946\n",
      "Epoch 31/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.8814 - val_loss: 0.5756\n",
      "Epoch 32/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.8508 - val_loss: 0.5554\n",
      "Epoch 33/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.8104 - val_loss: 0.5420\n",
      "Epoch 34/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.8416 - val_loss: 0.5337\n",
      "Epoch 35/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.7778 - val_loss: 0.5248\n",
      "Epoch 36/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.7677 - val_loss: 0.5126\n",
      "Epoch 37/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.7917 - val_loss: 0.5074\n",
      "Epoch 38/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.7786 - val_loss: 0.5134\n",
      "Epoch 39/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.7116 - val_loss: 0.5116\n",
      "Epoch 40/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.6882 - val_loss: 0.4886\n",
      "Epoch 41/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.6854 - val_loss: 0.4807\n",
      "Epoch 42/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.6614 - val_loss: 0.4839\n",
      "Epoch 43/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.6485 - val_loss: 0.4745\n",
      "Epoch 44/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.6469 - val_loss: 0.4752\n",
      "Epoch 45/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.6307 - val_loss: 0.4797\n",
      "Epoch 46/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.6400 - val_loss: 0.4875\n",
      "Epoch 47/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.6363 - val_loss: 0.4860\n",
      "Epoch 48/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.6035 - val_loss: 0.4900\n",
      "Epoch 49/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.5893 - val_loss: 0.4868\n",
      "Epoch 50/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.5568 - val_loss: 0.4756\n",
      "Epoch 51/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.6004 - val_loss: 0.4701\n",
      "Epoch 52/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.5826 - val_loss: 0.4723\n",
      "Epoch 53/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.5653 - val_loss: 0.4611\n",
      "Epoch 54/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.5342 - val_loss: 0.4428\n",
      "Epoch 55/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.5193 - val_loss: 0.4368\n",
      "Epoch 56/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.5293 - val_loss: 0.4404\n",
      "Epoch 57/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.5244 - val_loss: 0.4435\n",
      "Epoch 58/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.5157 - val_loss: 0.4348\n",
      "Epoch 59/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.4899 - val_loss: 0.4372\n",
      "Epoch 60/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.4893 - val_loss: 0.4467\n",
      "Epoch 61/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.4882 - val_loss: 0.4283\n",
      "Epoch 62/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.4631 - val_loss: 0.4100\n",
      "Epoch 63/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.4806 - val_loss: 0.4065\n",
      "Epoch 64/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.4590 - val_loss: 0.4024\n",
      "Epoch 65/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.4398 - val_loss: 0.3953\n",
      "Epoch 66/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.4280 - val_loss: 0.3942\n",
      "Epoch 67/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.4211 - val_loss: 0.3922\n",
      "Epoch 68/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.4174 - val_loss: 0.3739\n",
      "Epoch 69/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.4144 - val_loss: 0.3704\n",
      "Epoch 70/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.3963 - val_loss: 0.3802\n",
      "Epoch 71/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.3999 - val_loss: 0.3857\n",
      "Epoch 72/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.3830 - val_loss: 0.3745\n",
      "Epoch 73/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.3840 - val_loss: 0.3662\n",
      "Epoch 74/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.4013 - val_loss: 0.3689\n",
      "Epoch 75/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.3794 - val_loss: 0.3699\n",
      "Epoch 76/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.3768 - val_loss: 0.3708\n",
      "Epoch 77/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.3754 - val_loss: 0.3714\n",
      "Epoch 78/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.3643 - val_loss: 0.3713\n",
      "Epoch 79/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.3511 - val_loss: 0.3637\n",
      "Epoch 80/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.3594 - val_loss: 0.3557\n",
      "Epoch 81/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.3512 - val_loss: 0.3553\n",
      "Epoch 82/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.3456 - val_loss: 0.3530\n",
      "Epoch 83/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.3422 - val_loss: 0.3528\n",
      "Epoch 84/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.3316 - val_loss: 0.3489\n",
      "Epoch 85/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.3399 - val_loss: 0.3470\n",
      "Epoch 86/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.3425 - val_loss: 0.3434\n",
      "Epoch 87/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.3233 - val_loss: 0.3385\n",
      "Epoch 88/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.3275 - val_loss: 0.3259\n",
      "Epoch 89/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.3193 - val_loss: 0.3123\n",
      "Epoch 90/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.3138 - val_loss: 0.3076\n",
      "Epoch 91/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.3098 - val_loss: 0.3049\n",
      "Epoch 92/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.3158 - val_loss: 0.3045\n",
      "Epoch 93/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2855 - val_loss: 0.2988\n",
      "Epoch 94/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2926 - val_loss: 0.2935\n",
      "Epoch 95/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2899 - val_loss: 0.2916\n",
      "Epoch 96/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2864 - val_loss: 0.2914\n",
      "Epoch 97/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.2768 - val_loss: 0.2868\n",
      "Epoch 98/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.2745 - val_loss: 0.2842\n",
      "Epoch 99/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.2767 - val_loss: 0.2811\n",
      "Epoch 100/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.2737 - val_loss: 0.2770\n",
      "Epoch 101/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.2692 - val_loss: 0.2730\n",
      "Epoch 102/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.2650 - val_loss: 0.2709\n",
      "Epoch 103/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2650 - val_loss: 0.2689\n",
      "Epoch 104/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.2644 - val_loss: 0.2666\n",
      "Epoch 105/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2615 - val_loss: 0.2684\n",
      "Epoch 106/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2552 - val_loss: 0.2621\n",
      "Epoch 107/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2507 - val_loss: 0.2556\n",
      "Epoch 108/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2479 - val_loss: 0.2518\n",
      "Epoch 109/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.2485 - val_loss: 0.2510\n",
      "Epoch 110/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2442 - val_loss: 0.2493\n",
      "Epoch 111/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.2378 - val_loss: 0.2466\n",
      "Epoch 112/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2404 - val_loss: 0.2453\n",
      "Epoch 113/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.2471 - val_loss: 0.2429\n",
      "Epoch 114/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2395 - val_loss: 0.2408\n",
      "Epoch 115/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.2334 - val_loss: 0.2393\n",
      "Epoch 116/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2341 - val_loss: 0.2383\n",
      "Epoch 117/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.2265 - val_loss: 0.2351\n",
      "Epoch 118/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2196 - val_loss: 0.2326\n",
      "Epoch 119/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2244 - val_loss: 0.2298\n",
      "Epoch 120/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.2187 - val_loss: 0.2279\n",
      "Epoch 121/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.2181 - val_loss: 0.2258\n",
      "Epoch 122/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2208 - val_loss: 0.2226\n",
      "Epoch 123/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2177 - val_loss: 0.2213\n",
      "Epoch 124/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2169 - val_loss: 0.2190\n",
      "Epoch 125/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.2107 - val_loss: 0.2166\n",
      "Epoch 126/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2148 - val_loss: 0.2144\n",
      "Epoch 127/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.2072 - val_loss: 0.2108\n",
      "Epoch 128/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.2103 - val_loss: 0.2077\n",
      "Epoch 129/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.2018 - val_loss: 0.2057\n",
      "Epoch 130/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.2005 - val_loss: 0.2057\n",
      "Epoch 131/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2038 - val_loss: 0.2058\n",
      "Epoch 132/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1991 - val_loss: 0.2036\n",
      "Epoch 133/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.2012 - val_loss: 0.2006\n",
      "Epoch 134/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.1968 - val_loss: 0.1981\n",
      "Epoch 135/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2006 - val_loss: 0.1970\n",
      "Epoch 136/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2009 - val_loss: 0.1952\n",
      "Epoch 137/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.1891 - val_loss: 0.1937\n",
      "Epoch 138/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1938 - val_loss: 0.1923\n",
      "Epoch 139/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1886 - val_loss: 0.1914\n",
      "Epoch 140/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1895 - val_loss: 0.1897\n",
      "Epoch 141/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.1861 - val_loss: 0.1899\n",
      "Epoch 142/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1877 - val_loss: 0.1900\n",
      "Epoch 143/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.1866 - val_loss: 0.1893\n",
      "Epoch 144/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1861 - val_loss: 0.1879\n",
      "Epoch 145/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1843 - val_loss: 0.1864\n",
      "Epoch 146/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1843 - val_loss: 0.1840\n",
      "Epoch 147/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1782 - val_loss: 0.1827\n",
      "Epoch 148/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1836 - val_loss: 0.1822\n",
      "Epoch 149/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.1775 - val_loss: 0.1835\n",
      "Epoch 150/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1763 - val_loss: 0.1823\n",
      "Epoch 151/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1737 - val_loss: 0.1800\n",
      "Epoch 152/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.1742 - val_loss: 0.1785\n",
      "Epoch 153/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.1762 - val_loss: 0.1780\n",
      "Epoch 154/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1716 - val_loss: 0.1766\n",
      "Epoch 155/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.1677 - val_loss: 0.1757\n",
      "Epoch 156/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1709 - val_loss: 0.1752\n",
      "Epoch 157/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1672 - val_loss: 0.1738\n",
      "Epoch 158/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1693 - val_loss: 0.1738\n",
      "Epoch 159/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1643 - val_loss: 0.1734\n",
      "Epoch 160/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.1636 - val_loss: 0.1706\n",
      "Epoch 161/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1634 - val_loss: 0.1687\n",
      "Epoch 162/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1634 - val_loss: 0.1681\n",
      "Epoch 163/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.1625 - val_loss: 0.1674\n",
      "Epoch 164/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.1623 - val_loss: 0.1674\n",
      "Epoch 165/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1616 - val_loss: 0.1672\n",
      "Epoch 166/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.1611 - val_loss: 0.1673\n",
      "Epoch 167/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.1590 - val_loss: 0.1663\n",
      "Epoch 168/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1625 - val_loss: 0.1655\n",
      "Epoch 169/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1562 - val_loss: 0.1656\n",
      "Epoch 170/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1581 - val_loss: 0.1650\n",
      "Epoch 171/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.1580 - val_loss: 0.1649\n",
      "Epoch 172/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.1570 - val_loss: 0.1629\n",
      "Epoch 173/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1553 - val_loss: 0.1618\n",
      "Epoch 174/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.1545 - val_loss: 0.1620\n",
      "Epoch 175/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.1552 - val_loss: 0.1626\n",
      "Epoch 176/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.1511 - val_loss: 0.1620\n",
      "Epoch 177/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1543 - val_loss: 0.1611\n",
      "Epoch 178/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1498 - val_loss: 0.1606\n",
      "Epoch 179/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1529 - val_loss: 0.1591\n",
      "Epoch 180/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1500 - val_loss: 0.1582\n",
      "Epoch 181/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1495 - val_loss: 0.1584\n",
      "Epoch 182/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1489 - val_loss: 0.1595\n",
      "Epoch 183/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.1477 - val_loss: 0.1601\n",
      "Epoch 184/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1468 - val_loss: 0.1589\n",
      "Epoch 185/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1450 - val_loss: 0.1594\n",
      "Epoch 186/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1476 - val_loss: 0.1599\n",
      "Epoch 187/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.1467 - val_loss: 0.1578\n",
      "Epoch 188/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1422 - val_loss: 0.1554\n",
      "Epoch 189/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1447 - val_loss: 0.1557\n",
      "Epoch 190/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1443 - val_loss: 0.1548\n",
      "Epoch 191/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1416 - val_loss: 0.1550\n",
      "Epoch 192/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1417 - val_loss: 0.1536\n",
      "Epoch 193/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.1427 - val_loss: 0.1540\n",
      "Epoch 194/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1410 - val_loss: 0.1536\n",
      "Epoch 195/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1400 - val_loss: 0.1515\n",
      "Epoch 196/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1413 - val_loss: 0.1526\n",
      "Epoch 197/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1400 - val_loss: 0.1541\n",
      "Epoch 198/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1413 - val_loss: 0.1555\n",
      "Epoch 199/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1401 - val_loss: 0.1534\n",
      "Epoch 200/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1376 - val_loss: 0.1511\n",
      "Final Training Loss: 0.14542058110237122\n",
      "Final Validation Loss: 0.15114916861057281\n"
     ]
    }
   ],
   "source": [
    "# Best hyperparameters found from tuning\n",
    "best_params = {\n",
    "    'units1': 32,\n",
    "    'units2': 32,\n",
    "    'recurrent_dropout': 0.2,\n",
    "    'optimizer': 'adam',\n",
    "    'loss_function': 'mean_squared_error',\n",
    "    'learning_rate_decay': 1e-05,\n",
    "    'learning_rate': 0.0005,\n",
    "    'l2_lambda': 0.01,\n",
    "    'epochs': 200,\n",
    "    'dropout_rate': 0.3,\n",
    "    'clipnorm': 5.0,\n",
    "    'batch_size': 64\n",
    "}\n",
    "\n",
    "# Build the model using the best parameters\n",
    "def build_best_model(params):\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units1'], return_sequences=True, \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "                   kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Second LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units2'], return_sequences=False, kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Select the optimizer\n",
    "    if params['optimizer'] == 'adam':\n",
    "        optimizer = Adam(learning_rate=params['learning_rate'], decay=params['learning_rate_decay'], clipnorm=params['clipnorm'])\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizer '{params['optimizer']}' is not implemented in this setup.\")\n",
    "    \n",
    "    # Compile the model with the specified optimizer and loss function\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Instantiate the model with the best hyperparameters\n",
    "model = build_best_model(best_params)\n",
    "\n",
    "# Early stopping to stop training when validation loss stops improving\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq, \n",
    "    epochs=best_params['epochs'], \n",
    "    batch_size=best_params['batch_size'], \n",
    "    validation_data=(X_test_seq, y_test_seq), \n",
    "    shuffle=False,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Print the final training and validation loss\n",
    "print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90b12448-0efa-4cbf-940b-7e0be3716b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 106ms/step\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "LSTM Model Performance:\n",
      "Training RMSE: 0.02048429984946356\n",
      "Test RMSE: 0.025404350065667373\n",
      "Training MAE: 0.01493472430189318\n",
      "Test MAE: 0.019465706883975704\n",
      "Directional Accuracy on Training Data: 60.541813898704355%\n",
      "Directional Accuracy on Test Data: 55.46875%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict on both training and test sequences\n",
    "y_train_pred = model.predict(X_train_seq)\n",
    "y_test_pred = model.predict(X_test_seq)\n",
    "\n",
    "# Inverse scale predictions and actual values to original scale\n",
    "y_train_pred_rescaled = scaler_y.inverse_transform(y_train_pred)\n",
    "y_test_pred_rescaled = scaler_y.inverse_transform(y_test_pred)\n",
    "y_train_actual_rescaled = scaler_y.inverse_transform(y_train_seq)\n",
    "y_test_actual_rescaled = scaler_y.inverse_transform(y_test_seq)\n",
    "\n",
    "# Define a function for directional accuracy\n",
    "def directional_accuracy(y_true, y_pred):\n",
    "    return np.mean((np.sign(y_true[1:] - y_true[:-1]) == np.sign(y_pred[1:] - y_pred[:-1])).astype(int)) * 100\n",
    "\n",
    "# Calculate metrics.\n",
    "# Root Mean Squared Error (RMSE) for training set\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_actual_rescaled, y_train_pred_rescaled))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_actual_rescaled, y_test_pred_rescaled)) # RMSE for test set.\n",
    "train_mae = mean_absolute_error(y_train_actual_rescaled, y_train_pred_rescaled) # Mean Absolute Error (MAE) for training set.\n",
    "test_mae = mean_absolute_error(y_test_actual_rescaled, y_test_pred_rescaled) # MAE for test set.\n",
    "# Directional accuracy for training set.\n",
    "train_directional_accuracy = directional_accuracy(y_train_actual_rescaled.flatten(), y_train_pred_rescaled.flatten())\n",
    "# Directional accuracy for test set.\n",
    "test_directional_accuracy = directional_accuracy(y_test_actual_rescaled.flatten(), y_test_pred_rescaled.flatten())\n",
    "\n",
    "# Display score metrics to evaluate the model's performance.\n",
    "print(\"LSTM Model Performance:\")\n",
    "print(f\"Training RMSE: {train_rmse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Training MAE: {train_mae}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Directional Accuracy on Training Data: {train_directional_accuracy}%\")\n",
    "print(f\"Directional Accuracy on Test Data: {test_directional_accuracy}%\")\n",
    "\n",
    "# Plot training & validation loss values, it will help visualize how well the model fits the data over training epochs.\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313f8a72-236d-489a-b55a-b69b83e96466",
   "metadata": {},
   "source": [
    "### Three Layers (Model 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c4ffb1ac-6060-4015-9b72-f868bb0c62d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running combination 1/30: {'units3': 32, 'units2': 32, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 32}\n",
      "Final Training Loss: 0.1156860738992691\n",
      "Final Validation Loss: 0.13551855087280273\n",
      "Running combination 2/30: {'units3': 128, 'units2': 64, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 3.3287806510925293\n",
      "Final Validation Loss: 2.814274311065674\n",
      "Running combination 3/30: {'units3': 128, 'units2': 64, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.4, 'clipnorm': 5.0, 'batch_size': 256}\n",
      "Final Training Loss: 1.662271499633789\n",
      "Final Validation Loss: 0.38742169737815857\n",
      "Running combination 4/30: {'units3': 128, 'units2': 64, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 5.0, 'batch_size': 120}\n",
      "Final Training Loss: 6.672164440155029\n",
      "Final Validation Loss: 5.596430778503418\n",
      "Running combination 5/30: {'units3': 64, 'units2': 64, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 3.416130781173706\n",
      "Final Validation Loss: 2.9644107818603516\n",
      "Running combination 6/30: {'units3': 64, 'units2': 32, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 1.4523475170135498\n",
      "Final Validation Loss: 0.2850722372531891\n",
      "Running combination 7/30: {'units3': 32, 'units2': 32, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 256}\n",
      "Final Training Loss: 2.095487594604492\n",
      "Final Validation Loss: 1.5315115451812744\n",
      "Running combination 8/30: {'units3': 32, 'units2': 128, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 64}\n",
      "Final Training Loss: 3.4789528846740723\n",
      "Final Validation Loss: 2.3740174770355225\n",
      "Running combination 9/30: {'units3': 32, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 256}\n",
      "Final Training Loss: 1.3240433931350708\n",
      "Final Validation Loss: 0.3502872884273529\n",
      "Running combination 10/30: {'units3': 64, 'units2': 64, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0001, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 64}\n",
      "Final Training Loss: 1.4169379472732544\n",
      "Final Validation Loss: 0.3513905107975006\n",
      "Running combination 11/30: {'units3': 128, 'units2': 128, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 1.9854663610458374\n",
      "Final Validation Loss: 0.5631262063980103\n",
      "Running combination 12/30: {'units3': 32, 'units2': 64, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 32}\n",
      "Final Training Loss: 15.400518417358398\n",
      "Final Validation Loss: 14.472439765930176\n",
      "Running combination 13/30: {'units3': 128, 'units2': 64, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 64}\n",
      "Final Training Loss: 0.8088030815124512\n",
      "Final Validation Loss: 0.40924346446990967\n",
      "Running combination 14/30: {'units3': 32, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 120}\n",
      "Final Training Loss: 0.581254780292511\n",
      "Final Validation Loss: 0.3175960183143616\n",
      "Running combination 15/30: {'units3': 128, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 3.0278987884521484\n",
      "Final Validation Loss: 1.9242029190063477\n",
      "Running combination 16/30: {'units3': 64, 'units2': 64, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 1.9285742044448853\n",
      "Final Validation Loss: 1.5978708267211914\n",
      "Running combination 17/30: {'units3': 128, 'units2': 32, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 14.899683952331543\n",
      "Final Validation Loss: 13.933755874633789\n",
      "Running combination 18/30: {'units3': 32, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 1.714542031288147\n",
      "Final Validation Loss: 0.3130777180194855\n",
      "Running combination 19/30: {'units3': 128, 'units2': 128, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 4.301121234893799\n",
      "Final Validation Loss: 3.069427013397217\n",
      "Running combination 20/30: {'units3': 64, 'units2': 32, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 120}\n",
      "Final Training Loss: 2.5860114097595215\n",
      "Final Validation Loss: 1.6702957153320312\n",
      "Running combination 21/30: {'units3': 64, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 50, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 2.8043930530548096\n",
      "Final Validation Loss: 2.009704113006592\n",
      "Running combination 22/30: {'units3': 128, 'units2': 128, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 5.0, 'batch_size': 256}\n",
      "Final Training Loss: 30.694496154785156\n",
      "Final Validation Loss: 28.499427795410156\n",
      "Running combination 23/30: {'units3': 32, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 200, 'dropout_rate': 0.4, 'clipnorm': 5.0, 'batch_size': 256}\n",
      "Final Training Loss: 16.647537231445312\n",
      "Final Validation Loss: 14.943626403808594\n",
      "Running combination 24/30: {'units3': 64, 'units2': 128, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 200, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 256}\n",
      "Final Training Loss: 36.538761138916016\n",
      "Final Validation Loss: 35.679203033447266\n",
      "Running combination 25/30: {'units3': 64, 'units2': 64, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 10.660041809082031\n",
      "Final Validation Loss: 9.684877395629883\n",
      "Running combination 26/30: {'units3': 32, 'units2': 64, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 100, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 256}\n",
      "Final Training Loss: 2.9572958946228027\n",
      "Final Validation Loss: 2.0137338638305664\n",
      "Running combination 27/30: {'units3': 64, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 64}\n",
      "Final Training Loss: 2.0999372005462646\n",
      "Final Validation Loss: 1.542700171470642\n",
      "Running combination 28/30: {'units3': 64, 'units2': 64, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.01, 'epochs': 100, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 120}\n",
      "Final Training Loss: 2.1835999488830566\n",
      "Final Validation Loss: 1.5304813385009766\n",
      "Running combination 29/30: {'units3': 64, 'units2': 32, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 256}\n",
      "Final Training Loss: 10.407037734985352\n",
      "Final Validation Loss: 9.747726440429688\n",
      "Running combination 30/30: {'units3': 128, 'units2': 128, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.4, 'clipnorm': 5.0, 'batch_size': 32}\n",
      "Final Training Loss: 4.215217113494873\n",
      "Final Validation Loss: 2.7886834144592285\n",
      "Top results:\n",
      "{'params': {'units3': 32, 'units2': 32, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 32}, 'final_train_loss': 0.1156860738992691, 'final_val_loss': 0.13551855087280273}\n",
      "{'params': {'units3': 64, 'units2': 32, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 32}, 'final_train_loss': 1.4523475170135498, 'final_val_loss': 0.2850722372531891}\n",
      "{'params': {'units3': 32, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 120}, 'final_train_loss': 1.714542031288147, 'final_val_loss': 0.3130777180194855}\n",
      "{'params': {'units3': 32, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 120}, 'final_train_loss': 0.581254780292511, 'final_val_loss': 0.3175960183143616}\n",
      "{'params': {'units3': 32, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 256}, 'final_train_loss': 1.3240433931350708, 'final_val_loss': 0.3502872884273529}\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'dropout_rate': [0.2, 0.3, 0.4],\n",
    "    'recurrent_dropout': [0.1, 0.2],\n",
    "    'l2_lambda': [0.001, 0.01, 0.1],\n",
    "    'learning_rate': [0.001, 0.0005, 0.0001],\n",
    "    'learning_rate_decay': [1e-6, 1e-5, 0],\n",
    "    'units1': [32, 64, 128],\n",
    "    'units2': [32, 64, 128],\n",
    "    'units3': [32, 64, 128],\n",
    "    'batch_size': [32, 64, 120, 256],  \n",
    "    'epochs': [50, 100, 200],\n",
    "    'optimizer': ['adam'],\n",
    "    'clipnorm': [1.0, 5.0]\n",
    "}\n",
    "\n",
    "# Generate a list of random combinations with ParameterSampler\n",
    "n_iter_search = 30\n",
    "random_combinations = list(ParameterSampler(param_grid, n_iter=n_iter_search, random_state=42))\n",
    "\n",
    "# Define the function to build the model with variable parameters\n",
    "def build_model(dropout_rate, recurrent_dropout, l2_lambda, learning_rate, learning_rate_decay, clipnorm, units1, units2, units3, optimizer_name, loss_function):\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=units1, return_sequences=True, \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "                   kernel_regularizer=l2(l2_lambda), recurrent_dropout=recurrent_dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Second LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=units2, return_sequences=True, kernel_regularizer=l2(l2_lambda), recurrent_dropout=recurrent_dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Third LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=units3, return_sequences=False, kernel_regularizer=l2(l2_lambda), recurrent_dropout=recurrent_dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer.\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Select optimizer with decay and clipping.\n",
    "    optimizer = Adam(learning_rate=learning_rate, decay=learning_rate_decay, clipnorm=clipnorm)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Placeholder to keep track of results\n",
    "results = []\n",
    "\n",
    "# Run random search\n",
    "for i, params in enumerate(random_combinations):\n",
    "    print(f\"Running combination {i+1}/{len(random_combinations)}: {params}\")\n",
    "    \n",
    "    # Build model with the current parameters\n",
    "    model = build_model(\n",
    "        dropout_rate=params['dropout_rate'],\n",
    "        recurrent_dropout=params['recurrent_dropout'],\n",
    "        l2_lambda=params['l2_lambda'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        learning_rate_decay=params['learning_rate_decay'],\n",
    "        clipnorm=params['clipnorm'],\n",
    "        units1=params['units1'],\n",
    "        units2=params['units2'],\n",
    "        units3=params['units3'],\n",
    "        optimizer_name=params['optimizer'],\n",
    "        loss_function='mean_squared_error'\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq, \n",
    "        epochs=params['epochs'], \n",
    "        batch_size=params['batch_size'], \n",
    "        validation_data=(X_test_seq, y_test_seq), \n",
    "        callbacks=[early_stopping],\n",
    "        shuffle=False,\n",
    "        verbose=0  # Set verbose=0 to reduce output\n",
    "    )\n",
    "    \n",
    "    # Get final validation loss and training loss\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    \n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'params': params,\n",
    "        'final_train_loss': final_train_loss,\n",
    "        'final_val_loss': final_val_loss\n",
    "    })\n",
    "    \n",
    "    print(f\"Final Training Loss: {final_train_loss}\")\n",
    "    print(f\"Final Validation Loss: {final_val_loss}\")\n",
    "\n",
    "results = sorted(results, key=lambda x: x['final_val_loss'])\n",
    "print(\"Top results:\")\n",
    "for res in results[:5]:  # Show top 5 results\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "600af3e9-a1dd-43d4-aa3f-abbc9f826756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 93ms/step - loss: 13.0333 - val_loss: 10.4712\n",
      "Epoch 2/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 11.2325 - val_loss: 9.3962\n",
      "Epoch 3/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 9.8324 - val_loss: 8.3156\n",
      "Epoch 4/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 8.7394 - val_loss: 7.3365\n",
      "Epoch 5/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 7.7628 - val_loss: 6.4972\n",
      "Epoch 6/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 6.9565 - val_loss: 5.7536\n",
      "Epoch 7/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 6.0973 - val_loss: 5.1039\n",
      "Epoch 8/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 5.3920 - val_loss: 4.5551\n",
      "Epoch 9/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 4.8919 - val_loss: 4.0645\n",
      "Epoch 10/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 4.3496 - val_loss: 3.6586\n",
      "Epoch 11/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 3.8843 - val_loss: 3.3154\n",
      "Epoch 12/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 3.5374 - val_loss: 2.9726\n",
      "Epoch 13/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 3.1655 - val_loss: 2.6963\n",
      "Epoch 14/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 2.9340 - val_loss: 2.4911\n",
      "Epoch 15/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 2.7029 - val_loss: 2.2668\n",
      "Epoch 16/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 2.4600 - val_loss: 2.0819\n",
      "Epoch 17/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 2.2792 - val_loss: 1.9370\n",
      "Epoch 18/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 2.0849 - val_loss: 1.7881\n",
      "Epoch 19/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 1.9200 - val_loss: 1.6108\n",
      "Epoch 20/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 1.7680 - val_loss: 1.4923\n",
      "Epoch 21/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 1.6945 - val_loss: 1.3826\n",
      "Epoch 22/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 1.5069 - val_loss: 1.3065\n",
      "Epoch 23/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 1.4368 - val_loss: 1.2440\n",
      "Epoch 24/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 1.3527 - val_loss: 1.1942\n",
      "Epoch 25/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 1.2935 - val_loss: 1.0637\n",
      "Epoch 26/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 1.1930 - val_loss: 1.0002\n",
      "Epoch 27/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 1.1359 - val_loss: 0.9649\n",
      "Epoch 28/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 1.0578 - val_loss: 0.8916\n",
      "Epoch 29/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.9809 - val_loss: 0.8442\n",
      "Epoch 30/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.9068 - val_loss: 0.7824\n",
      "Epoch 31/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.8877 - val_loss: 0.7567\n",
      "Epoch 32/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.8298 - val_loss: 0.7249\n",
      "Epoch 33/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.7935 - val_loss: 0.6435\n",
      "Epoch 34/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.7373 - val_loss: 0.6181\n",
      "Epoch 35/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.7011 - val_loss: 0.5855\n",
      "Epoch 36/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.6751 - val_loss: 0.5591\n",
      "Epoch 37/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.6660 - val_loss: 0.5338\n",
      "Epoch 38/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.6093 - val_loss: 0.5070\n",
      "Epoch 39/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.5788 - val_loss: 0.4850\n",
      "Epoch 40/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.5626 - val_loss: 0.5047\n",
      "Epoch 41/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.5240 - val_loss: 0.4646\n",
      "Epoch 42/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.4989 - val_loss: 0.4043\n",
      "Epoch 43/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.4673 - val_loss: 0.4482\n",
      "Epoch 44/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.4590 - val_loss: 0.3991\n",
      "Epoch 45/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.4503 - val_loss: 0.3755\n",
      "Epoch 46/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.3934 - val_loss: 0.3852\n",
      "Epoch 47/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.4001 - val_loss: 0.4607\n",
      "Epoch 48/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.3499 - val_loss: 0.3396\n",
      "Epoch 49/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.3372 - val_loss: 0.3750\n",
      "Epoch 50/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.3108 - val_loss: 0.4047\n",
      "Epoch 51/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.3095 - val_loss: 0.5382\n",
      "Epoch 52/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.2877 - val_loss: 0.4590\n",
      "Epoch 53/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.2705 - val_loss: 0.4415\n",
      "Epoch 54/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.2514 - val_loss: 0.3387\n",
      "Epoch 55/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.2440 - val_loss: 0.4337\n",
      "Epoch 56/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.2431 - val_loss: 0.3889\n",
      "Epoch 57/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.2290 - val_loss: 0.5079\n",
      "Epoch 58/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.2136 - val_loss: 0.4920\n",
      "Epoch 59/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.2132 - val_loss: 0.4998\n",
      "Epoch 60/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.2034 - val_loss: 0.5126\n",
      "Epoch 61/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.2009 - val_loss: 0.5848\n",
      "Epoch 62/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.1881 - val_loss: 0.4688\n",
      "Epoch 63/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.1842 - val_loss: 0.4135\n",
      "Epoch 64/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.1748 - val_loss: 0.3336\n",
      "Epoch 65/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.1711 - val_loss: 0.3241\n",
      "Epoch 66/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.1665 - val_loss: 0.3504\n",
      "Epoch 67/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.1575 - val_loss: 0.3699\n",
      "Epoch 68/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.1567 - val_loss: 0.2632\n",
      "Epoch 69/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.1556 - val_loss: 0.3258\n",
      "Epoch 70/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.1493 - val_loss: 0.2334\n",
      "Epoch 71/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.1471 - val_loss: 0.2277\n",
      "Epoch 72/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.1457 - val_loss: 0.1831\n",
      "Epoch 73/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.1432 - val_loss: 0.1883\n",
      "Epoch 74/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.1364 - val_loss: 0.1608\n",
      "Epoch 75/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.1375 - val_loss: 0.1765\n",
      "Epoch 76/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.1309 - val_loss: 0.1839\n",
      "Epoch 77/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.1299 - val_loss: 0.1493\n",
      "Epoch 78/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.1268 - val_loss: 0.1626\n",
      "Epoch 79/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.1221 - val_loss: 0.1708\n",
      "Epoch 80/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.1284 - val_loss: 0.1540\n",
      "Epoch 81/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.1240 - val_loss: 0.1412\n",
      "Epoch 82/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.1214 - val_loss: 0.1437\n",
      "Epoch 83/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.1202 - val_loss: 0.1612\n",
      "Epoch 84/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.1200 - val_loss: 0.1492\n",
      "Epoch 85/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.1178 - val_loss: 0.1428\n",
      "Epoch 86/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.1161 - val_loss: 0.1387\n",
      "Epoch 87/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.1125 - val_loss: 0.1520\n",
      "Epoch 88/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.1145 - val_loss: 0.1371\n",
      "Epoch 89/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.1151 - val_loss: 0.1373\n",
      "Epoch 90/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.1133 - val_loss: 0.1333\n",
      "Epoch 91/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.1105 - val_loss: 0.1317\n",
      "Epoch 92/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.1099 - val_loss: 0.1324\n",
      "Epoch 93/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.1135 - val_loss: 0.1335\n",
      "Epoch 94/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.1110 - val_loss: 0.1320\n",
      "Epoch 95/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.1096 - val_loss: 0.1369\n",
      "Epoch 96/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.1107 - val_loss: 0.1358\n",
      "Epoch 97/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.1089 - val_loss: 0.1312\n",
      "Epoch 98/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.1105 - val_loss: 0.1300\n",
      "Epoch 99/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.1085 - val_loss: 0.1284\n",
      "Epoch 100/100\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.1080 - val_loss: 0.1355\n",
      "Final Training Loss: 0.1156860738992691\n",
      "Final Validation Loss: 0.13551855087280273\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Best hyperparameters found from tuning\n",
    "best_params = {\n",
    "    'units1': 32,\n",
    "    'units2': 32,\n",
    "    'units3': 32,\n",
    "    'recurrent_dropout': 0.2,\n",
    "    'optimizer': 'adam',\n",
    "    'loss_function': 'mean_squared_error',\n",
    "    'learning_rate_decay': 1e-05,\n",
    "    'learning_rate': 0.0005,\n",
    "    'l2_lambda': 0.1,\n",
    "    'epochs': 100,\n",
    "    'dropout_rate': 0.2,\n",
    "    'clipnorm': 5.0,\n",
    "    'batch_size': 32\n",
    "}\n",
    "\n",
    "# Build the model using the best parameters\n",
    "def build_best_model(params):\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units1'], return_sequences=True, \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "                   kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Second LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units2'], return_sequences=True, kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Third LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units3'], return_sequences=False, kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Select the optimizer\n",
    "    if params['optimizer'] == 'adam':\n",
    "        optimizer = Adam(learning_rate=params['learning_rate'], decay=params['learning_rate_decay'], clipnorm=params['clipnorm'])\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizer '{params['optimizer']}' is not implemented in this setup.\")\n",
    "        \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate the model with the best hyperparameters\n",
    "model = build_best_model(best_params)\n",
    "\n",
    "# Early stopping to stop training when validation loss stops improving\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq, \n",
    "    epochs=best_params['epochs'], \n",
    "    batch_size=best_params['batch_size'], \n",
    "    validation_data=(X_test_seq, y_test_seq),\n",
    "    shuffle=False,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Print the final training and validation loss\n",
    "print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "884981af-407f-423e-a971-993ca407eb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 173ms/step\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step \n",
      "LSTM Model Performance:\n",
      "Training RMSE: 0.02054748361203847\n",
      "Test RMSE: 0.02399193388254631\n",
      "Training MAE: 0.01495094313670688\n",
      "Test MAE: 0.01831006405625682\n",
      "Directional Accuracy on Training Data: 52.65017667844523%\n",
      "Directional Accuracy on Test Data: 47.65625%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict on both training and test sequences\n",
    "y_train_pred = model.predict(X_train_seq)\n",
    "y_test_pred = model.predict(X_test_seq)\n",
    "\n",
    "# Inverse scale predictions and actual values to original scale\n",
    "y_train_pred_rescaled = scaler_y.inverse_transform(y_train_pred)\n",
    "y_test_pred_rescaled = scaler_y.inverse_transform(y_test_pred)\n",
    "y_train_actual_rescaled = scaler_y.inverse_transform(y_train_seq)\n",
    "y_test_actual_rescaled = scaler_y.inverse_transform(y_test_seq)\n",
    "\n",
    "# Define a function for directional accuracy\n",
    "def directional_accuracy(y_true, y_pred):\n",
    "    return np.mean((np.sign(y_true[1:] - y_true[:-1]) == np.sign(y_pred[1:] - y_pred[:-1])).astype(int)) * 100\n",
    "\n",
    "# Calculate metrics.\n",
    "# Root Mean Squared Error (RMSE) for training set\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_actual_rescaled, y_train_pred_rescaled))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_actual_rescaled, y_test_pred_rescaled)) # RMSE for test set.\n",
    "train_mae = mean_absolute_error(y_train_actual_rescaled, y_train_pred_rescaled) # Mean Absolute Error (MAE) for training set.\n",
    "test_mae = mean_absolute_error(y_test_actual_rescaled, y_test_pred_rescaled) # MAE for test set.\n",
    "# Directional accuracy for training set.\n",
    "train_directional_accuracy = directional_accuracy(y_train_actual_rescaled.flatten(), y_train_pred_rescaled.flatten())\n",
    "# Directional accuracy for test set.\n",
    "test_directional_accuracy = directional_accuracy(y_test_actual_rescaled.flatten(), y_test_pred_rescaled.flatten())\n",
    "\n",
    "# Display score metrics to evaluate the model's performance.\n",
    "print(\"LSTM Model Performance:\")\n",
    "print(f\"Training RMSE: {train_rmse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Training MAE: {train_mae}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Directional Accuracy on Training Data: {train_directional_accuracy}%\")\n",
    "print(f\"Directional Accuracy on Test Data: {test_directional_accuracy}%\")\n",
    "\n",
    "# Plot training & validation loss values, it will help visualize how well the model fits the data over training epochs.\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e624ac88-b2b7-4df6-9107-7c32dee9cf51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26bd8a91-4efa-4804-a89a-a67cd3f71815",
   "metadata": {},
   "source": [
    "### 2. Optuna (Model 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b5937598-7471-4315-830e-88affa10e989",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 23:01:13,341] A new study created in memory with name: no-name-972e1a98-4515-4098-82ff-c118eff63f03\n",
      "[I 2024-11-13 23:01:24,036] Trial 0 finished with value: 0.5328297019004822 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.2, 'l2_lambda': 0.0037550037026405905, 'learning_rate': 0.0001, 'learning_rate_decay': 1e-05, 'clipnorm': 1.0, 'units': 32, 'num_layers': 3, 'epochs': 100, 'batch_size': 64}. Best is trial 0 with value: 0.5328297019004822.\n",
      "[I 2024-11-13 23:01:32,786] Trial 1 finished with value: 2.5555622577667236 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.2, 'l2_lambda': 0.04641587067816149, 'learning_rate': 0.0001, 'learning_rate_decay': 1e-05, 'clipnorm': 1.0, 'units': 32, 'num_layers': 2, 'epochs': 50, 'batch_size': 256}. Best is trial 0 with value: 0.5328297019004822.\n",
      "[I 2024-11-13 23:02:11,419] Trial 2 finished with value: 2.443268299102783 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.2, 'l2_lambda': 0.02754390996680696, 'learning_rate': 0.0001, 'learning_rate_decay': 1e-05, 'clipnorm': 1.0, 'units': 128, 'num_layers': 2, 'epochs': 100, 'batch_size': 120}. Best is trial 0 with value: 0.5328297019004822.\n",
      "[I 2024-11-13 23:02:34,982] Trial 3 finished with value: 0.3527396023273468 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.2, 'l2_lambda': 0.06953241045943279, 'learning_rate': 0.0001, 'learning_rate_decay': 1e-05, 'clipnorm': 1.0, 'units': 128, 'num_layers': 1, 'epochs': 150, 'batch_size': 256}. Best is trial 3 with value: 0.3527396023273468.\n",
      "[I 2024-11-13 23:02:40,719] Trial 4 finished with value: 0.19055894017219543 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.1, 'l2_lambda': 0.07320157289972352, 'learning_rate': 0.0005, 'learning_rate_decay': 1e-05, 'clipnorm': 1.0, 'units': 64, 'num_layers': 1, 'epochs': 150, 'batch_size': 32}. Best is trial 4 with value: 0.19055894017219543.\n",
      "[I 2024-11-13 23:03:07,291] Trial 5 finished with value: 0.2398924082517624 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.2, 'l2_lambda': 0.06621901977279396, 'learning_rate': 0.0005, 'learning_rate_decay': 0, 'clipnorm': 1.0, 'units': 64, 'num_layers': 2, 'epochs': 150, 'batch_size': 256}. Best is trial 4 with value: 0.19055894017219543.\n",
      "[I 2024-11-13 23:03:32,531] Trial 6 finished with value: 1.7661077976226807 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0038300508438275606, 'learning_rate': 0.0001, 'learning_rate_decay': 0, 'clipnorm': 5.0, 'units': 128, 'num_layers': 3, 'epochs': 50, 'batch_size': 256}. Best is trial 4 with value: 0.19055894017219543.\n",
      "[I 2024-11-13 23:03:38,280] Trial 7 finished with value: 0.12112877517938614 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0019223481607694784, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 150, 'batch_size': 64}. Best is trial 7 with value: 0.12112877517938614.\n",
      "[I 2024-11-13 23:03:51,845] Trial 8 finished with value: 0.493658185005188 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0017384974140603195, 'learning_rate': 0.0001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 64, 'num_layers': 3, 'epochs': 150, 'batch_size': 120}. Best is trial 7 with value: 0.12112877517938614.\n",
      "[I 2024-11-13 23:03:58,367] Trial 9 finished with value: 0.18824273347854614 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.1, 'l2_lambda': 0.028980866560207673, 'learning_rate': 0.0005, 'learning_rate_decay': 1e-06, 'clipnorm': 1.0, 'units': 64, 'num_layers': 1, 'epochs': 200, 'batch_size': 120}. Best is trial 7 with value: 0.12112877517938614.\n",
      "[I 2024-11-13 23:04:07,748] Trial 10 finished with value: 0.11149580031633377 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0010012325682668814, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 200, 'batch_size': 64}. Best is trial 10 with value: 0.11149580031633377.\n",
      "[I 2024-11-13 23:04:16,020] Trial 11 finished with value: 0.11309441179037094 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0011454828358668492, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 200, 'batch_size': 64}. Best is trial 10 with value: 0.11149580031633377.\n",
      "[I 2024-11-13 23:04:25,905] Trial 12 finished with value: 0.11152374744415283 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0011554791595316783, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 200, 'batch_size': 64}. Best is trial 10 with value: 0.11149580031633377.\n",
      "[I 2024-11-13 23:04:31,822] Trial 13 finished with value: 0.1323969066143036 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.008097857902138784, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 200, 'batch_size': 64}. Best is trial 10 with value: 0.11149580031633377.\n",
      "[I 2024-11-13 23:05:10,692] Trial 14 finished with value: 0.22306078672409058 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0010494090688129005, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 2, 'epochs': 200, 'batch_size': 64}. Best is trial 10 with value: 0.11149580031633377.\n",
      "[I 2024-11-13 23:05:17,583] Trial 15 finished with value: 0.13991524279117584 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.003502431872516879, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 200, 'batch_size': 32}. Best is trial 10 with value: 0.11149580031633377.\n",
      "[I 2024-11-13 23:05:29,086] Trial 16 finished with value: 0.3509503901004791 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.009118985456676882, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 32, 'num_layers': 2, 'epochs': 200, 'batch_size': 64}. Best is trial 10 with value: 0.11149580031633377.\n",
      "[I 2024-11-13 23:05:35,911] Trial 17 finished with value: 0.12050185352563858 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.002085993900111682, 'learning_rate': 0.001, 'learning_rate_decay': 0, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 100, 'batch_size': 64}. Best is trial 10 with value: 0.11149580031633377.\n",
      "[I 2024-11-13 23:05:44,411] Trial 18 finished with value: 0.12269530445337296 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.005040684715226819, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 200, 'batch_size': 64}. Best is trial 10 with value: 0.11149580031633377.\n",
      "[I 2024-11-13 23:05:59,133] Trial 19 finished with value: 0.2252160757780075 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.021365105049329695, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 32, 'num_layers': 2, 'epochs': 150, 'batch_size': 32}. Best is trial 10 with value: 0.11149580031633377.\n",
      "[I 2024-11-13 23:06:41,684] Trial 20 finished with value: 0.12731695175170898 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.1, 'l2_lambda': 0.016369012009803452, 'learning_rate': 0.001, 'learning_rate_decay': 0, 'clipnorm': 5.0, 'units': 128, 'num_layers': 2, 'epochs': 200, 'batch_size': 64}. Best is trial 10 with value: 0.11149580031633377.\n",
      "[I 2024-11-13 23:06:48,499] Trial 21 finished with value: 0.11324367672204971 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0010046340599894252, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 200, 'batch_size': 64}. Best is trial 10 with value: 0.11149580031633377.\n",
      "[I 2024-11-13 23:06:55,185] Trial 22 finished with value: 0.1171073243021965 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0015394277020084359, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 200, 'batch_size': 64}. Best is trial 10 with value: 0.11149580031633377.\n",
      "[I 2024-11-13 23:07:04,267] Trial 23 finished with value: 0.11669421941041946 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0025012773532357905, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 200, 'batch_size': 64}. Best is trial 10 with value: 0.11149580031633377.\n",
      "[I 2024-11-13 23:07:10,971] Trial 24 finished with value: 0.11502334475517273 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0012923257479048517, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 150, 'batch_size': 64}. Best is trial 10 with value: 0.11149580031633377.\n",
      "[I 2024-11-13 23:07:19,106] Trial 25 finished with value: 0.12235390394926071 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0024728114134915766, 'learning_rate': 0.0005, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 200, 'batch_size': 64}. Best is trial 10 with value: 0.11149580031633377.\n",
      "[I 2024-11-13 23:07:27,674] Trial 26 finished with value: 0.1133522018790245 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.2, 'l2_lambda': 0.0013038048591806203, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 150, 'batch_size': 64}. Best is trial 10 with value: 0.11149580031633377.\n",
      "[I 2024-11-13 23:07:53,546] Trial 27 finished with value: 0.25839242339134216 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.005715936153835941, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 2, 'epochs': 200, 'batch_size': 32}. Best is trial 10 with value: 0.11149580031633377.\n",
      "[I 2024-11-13 23:07:56,896] Trial 28 finished with value: 0.14531852304935455 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0010428601193272703, 'learning_rate': 0.001, 'learning_rate_decay': 0, 'clipnorm': 5.0, 'units': 32, 'num_layers': 1, 'epochs': 100, 'batch_size': 120}. Best is trial 10 with value: 0.11149580031633377.\n",
      "[I 2024-11-13 23:08:55,394] Trial 29 finished with value: 0.19070331752300262 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.2, 'l2_lambda': 0.003120232515467871, 'learning_rate': 0.0005, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 64, 'num_layers': 3, 'epochs': 200, 'batch_size': 64}. Best is trial 10 with value: 0.11149580031633377.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0010012325682668814, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 200, 'batch_size': 64}\n",
      "Best validation loss: 0.11149580031633377\n"
     ]
    }
   ],
   "source": [
    "# Set global random state for reproducibility.\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# Optuna's TPESampler with a fixed random seed for reproducibility in parameter search.\n",
    "sampler = TPESampler(seed=random_seed)\n",
    "\n",
    "# Create an Optuna study with direction \"minimize\" to minimize the validation loss.\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "\n",
    "# Define the objective function for hyperparameter optimization.\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna to optimize hyperparameters of an LSTM model.\n",
    "    Takes a trial object from Optuna and returns the validation loss of the model with given parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define hyperparameters to tune.\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.2, 0.4, step=0.1)  # Dropout rate for LSTM layers.\n",
    "    recurrent_dropout = trial.suggest_categorical(\"recurrent_dropout\", [0.1, 0.2])  # Recurrent dropout rate for LSTM layers.\n",
    "    l2_lambda = trial.suggest_loguniform(\"l2_lambda\", 1e-3, 1e-1)  # L2 regularization factor.\n",
    "    learning_rate = trial.suggest_categorical(\"learning_rate\", [0.001, 0.0005, 0.0001])  # Learning rate for the optimizer.\n",
    "    learning_rate_decay = trial.suggest_categorical(\"learning_rate_decay\", [1e-6, 1e-5, 0])  # Learning rate decay.\n",
    "    clipnorm = trial.suggest_categorical(\"clipnorm\", [1.0, 5.0])  # Gradient clipping norm.\n",
    "    units = trial.suggest_categorical(\"units\", [32, 64, 128])  # Number of units in LSTM layers.\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)  # Number of LSTM layers.\n",
    "    epochs = trial.suggest_int(\"epochs\", 50, 200, step=50)  # Number of epochs.\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 120, 256])  # Batch size.\n",
    "\n",
    "    # Initialize the Sequential model.\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add LSTM layers based on num_layers.\n",
    "    for i in range(num_layers):\n",
    "        # Set return_sequences=True for all but the last LSTM layer.\n",
    "        return_sequences = (i < num_layers - 1)\n",
    "        \n",
    "        # Add LSTM layer with specified hyperparameters.\n",
    "        model.add(LSTM(\n",
    "            units=units,\n",
    "            return_sequences=return_sequences,\n",
    "            input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]) if i == 0 else None,  # Set input shape only for the first layer.\n",
    "            kernel_regularizer=l2(l2_lambda),\n",
    "            recurrent_dropout=recurrent_dropout\n",
    "        ))\n",
    "\n",
    "        # Add BatchNormalization and Dropout after each LSTM layer.\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Add the output layer with a single unit (regression).\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Initialize the optimizer with learning rate, decay, and gradient clipping norm.\n",
    "    optimizer = Adam(learning_rate=learning_rate, decay=learning_rate_decay, clipnorm=clipnorm)\n",
    "\n",
    "    # Compile the model with mean squared error loss function.\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Early stopping callback to avoid overfitting.\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model with given hyperparameters.\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test_seq, y_test_seq),\n",
    "        callbacks=[early_stopping],\n",
    "        shuffle=False,\n",
    "        verbose=0  # Set verbose=0 to suppress training logs for faster experimentation.\n",
    "    )\n",
    "\n",
    "    # Retrieve the minimum validation loss from the training history.\n",
    "    val_loss = min(history.history['val_loss'])\n",
    "    \n",
    "    # Return the validation loss to be minimized by Optuna.\n",
    "    return val_loss\n",
    "\n",
    "# Run the Optuna study for a given number of trials.\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Print the best parameters found by the study.\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "# Print the best validation loss achieved with the optimal parameters.\n",
    "print(\"Best validation loss:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "129d0f04-2f99-4957-9793-dd86892ba82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 72ms/step - loss: 0.2746 - val_loss: 0.1171\n",
      "Epoch 2/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.2142 - val_loss: 0.1199\n",
      "Epoch 3/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.1865 - val_loss: 0.1165\n",
      "Epoch 4/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.1751 - val_loss: 0.1161\n",
      "Epoch 5/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.1797 - val_loss: 0.1168\n",
      "Epoch 6/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.1681 - val_loss: 0.1162\n",
      "Epoch 7/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.2006 - val_loss: 0.1210\n",
      "Epoch 8/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.2087 - val_loss: 0.1202\n",
      "Epoch 9/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.1604 - val_loss: 0.1152\n",
      "Epoch 10/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.1358 - val_loss: 0.1151\n",
      "Epoch 11/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.1426 - val_loss: 0.1149\n",
      "Epoch 12/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.1359 - val_loss: 0.1151\n",
      "Epoch 13/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.1506 - val_loss: 0.1148\n",
      "Epoch 14/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.1296 - val_loss: 0.1143\n",
      "Epoch 15/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.1258 - val_loss: 0.1137\n",
      "Epoch 16/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.1360 - val_loss: 0.1148\n",
      "Epoch 17/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.1352 - val_loss: 0.1215\n",
      "Epoch 18/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.1244 - val_loss: 0.1225\n",
      "Epoch 19/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.1232 - val_loss: 0.1237\n",
      "Epoch 20/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.1228 - val_loss: 0.1237\n",
      "Epoch 21/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 0.1170 - val_loss: 0.1210\n",
      "Epoch 22/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.1195 - val_loss: 0.1266\n",
      "Epoch 23/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.1209 - val_loss: 0.1174\n",
      "Epoch 24/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.1185 - val_loss: 0.1186\n",
      "Epoch 25/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.1248 - val_loss: 0.1257\n",
      "Final Training Loss: 0.13347920775413513\n",
      "Final Validation Loss: 0.12573203444480896\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Best hyperparameters found from tuning\n",
    "best_params = {\n",
    "    'units1': 128,\n",
    "    'recurrent_dropout': 0.1,\n",
    "    'optimizer': 'adam',\n",
    "    'loss_function': 'mean_squared_error',\n",
    "    'learning_rate_decay': 1e-06,\n",
    "    'learning_rate': 0.001,\n",
    "    'l2_lambda': 0.0010012325682668814,\n",
    "    'epochs': 200,\n",
    "    'dropout_rate': 0.2,\n",
    "    'clipnorm': 5.0,\n",
    "    'batch_size': 64\n",
    "}\n",
    "\n",
    "# Build the model using the best parameters\n",
    "def build_best_model(params):\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units1'], return_sequences=False, \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "                   kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Select the optimizer\n",
    "    if params['optimizer'] == 'adam':\n",
    "        optimizer = Adam(learning_rate=params['learning_rate'], decay=params['learning_rate_decay'], clipnorm=params['clipnorm'])\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizer '{params['optimizer']}' is not implemented in this setup.\")\n",
    "\n",
    "    \n",
    "    # Compile the model with the specified optimizer and loss function\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Instantiate the model with the best hyperparameters\n",
    "model = build_best_model(best_params)\n",
    "\n",
    "# Early stopping to stop training when validation loss stops improving\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq, \n",
    "    epochs=best_params['epochs'], \n",
    "    batch_size=best_params['batch_size'], \n",
    "    validation_data=(X_test_seq, y_test_seq), \n",
    "    shuffle=False,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Print the final training and validation loss.\n",
    "print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e349b3b4-3941-4257-80ac-25dfc6a9cc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step  \n",
      "LSTM Model Performance:\n",
      "Training RMSE: 0.020486305608138695\n",
      "Test RMSE: 0.024209731104996263\n",
      "Training MAE: 0.01504348072861271\n",
      "Test MAE: 0.018667071929117787\n",
      "Directional Accuracy on Training Data: 65.48881036513545%\n",
      "Directional Accuracy on Test Data: 67.1875%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict on both training and test sequences\n",
    "y_train_pred = model.predict(X_train_seq)\n",
    "y_test_pred = model.predict(X_test_seq)\n",
    "\n",
    "# Inverse scale predictions and actual values to original scale\n",
    "y_train_pred_rescaled = scaler_y.inverse_transform(y_train_pred)\n",
    "y_test_pred_rescaled = scaler_y.inverse_transform(y_test_pred)\n",
    "y_train_actual_rescaled = scaler_y.inverse_transform(y_train_seq)\n",
    "y_test_actual_rescaled = scaler_y.inverse_transform(y_test_seq)\n",
    "\n",
    "# Define a function for directional accuracy\n",
    "def directional_accuracy(y_true, y_pred):\n",
    "    return np.mean((np.sign(y_true[1:] - y_true[:-1]) == np.sign(y_pred[1:] - y_pred[:-1])).astype(int)) * 100\n",
    "\n",
    "# Calculate metrics.\n",
    "# Root Mean Squared Error (RMSE) for training set\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_actual_rescaled, y_train_pred_rescaled))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_actual_rescaled, y_test_pred_rescaled)) # RMSE for test set.\n",
    "train_mae = mean_absolute_error(y_train_actual_rescaled, y_train_pred_rescaled) # Mean Absolute Error (MAE) for training set.\n",
    "test_mae = mean_absolute_error(y_test_actual_rescaled, y_test_pred_rescaled) # MAE for test set.\n",
    "# Directional accuracy for training set.\n",
    "train_directional_accuracy = directional_accuracy(y_train_actual_rescaled.flatten(), y_train_pred_rescaled.flatten())\n",
    "# Directional accuracy for test set.\n",
    "test_directional_accuracy = directional_accuracy(y_test_actual_rescaled.flatten(), y_test_pred_rescaled.flatten())\n",
    "\n",
    "# Display score metrics to evaluate the model's performance.\n",
    "print(\"LSTM Model Performance:\")\n",
    "print(f\"Training RMSE: {train_rmse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Training MAE: {train_mae}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Directional Accuracy on Training Data: {train_directional_accuracy}%\")\n",
    "print(f\"Directional Accuracy on Test Data: {test_directional_accuracy}%\")\n",
    "\n",
    "# Plot training & validation loss values, it will help visualize how well the model fits the data over training epochs.\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca41d53-d497-4c97-99ae-4fcfe1865ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b327bce1-eb9f-4e0d-b1dc-ab01bcc06ada",
   "metadata": {},
   "source": [
    "### 3. Keras Tuner (Model 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0f75d0b0-39e7-4871-928e-e4cfb3cf120c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 00m 34s]\n",
      "val_loss: 0.16396601498126984\n",
      "\n",
      "Best val_loss So Far: 0.1255941092967987\n",
      "Total elapsed time: 00h 15m 22s\n",
      "Built model with params: dropout_rate=0.3, recurrent_dropout=0.2, l2_lambda=0.07913979537147964, learning_rate=0.001, learning_rate_decay=1e-05, clipnorm=5.0, units=32, num_layers=2, batch_size=64\n",
      "Best hyperparameters: {'dropout_rate': 0.3, 'recurrent_dropout': 0.2, 'l2_lambda': 0.07913979537147964, 'learning_rate': 0.001, 'learning_rate_decay': 1e-05, 'clipnorm': 5.0, 'units': 32, 'num_layers': 2, 'batch_size': 64}\n",
      "Best batch size: 64\n"
     ]
    }
   ],
   "source": [
    "# Set a global random seed for reproducibility.\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# Build model function with hyperparameter choices.\n",
    "def build_model(hp): #hp (kerastuner.HyperParameters) - Hyperparameter search space.\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Define hyperparameters using `hp` for various tuning options.\n",
    "    dropout_rate = hp.Choice(\"dropout_rate\", [0.2, 0.3, 0.4])  # Dropout rates to fight overfitting.\n",
    "    recurrent_dropout = hp.Choice(\"recurrent_dropout\", [0.1, 0.2])  # Recurrent dropout for LSTM layers.\n",
    "    l2_lambda = hp.Float(\"l2_lambda\", min_value=0.001, max_value=0.1, sampling=\"log\")  # L2 regularization factor.\n",
    "    learning_rate = hp.Choice(\"learning_rate\", [0.001, 0.0005, 0.0001])  # Learning rate choices.\n",
    "    learning_rate_decay = hp.Choice(\"learning_rate_decay\", [1e-5, 0.0])  # Learning rate decay for gradual reduction.\n",
    "    clipnorm = hp.Choice(\"clipnorm\", [1.0, 5.0])  # Gradient clipping norm to prevent exploding gradients.\n",
    "    units = hp.Choice(\"units\", [32, 64, 128])  # Number of units for LSTM layers.\n",
    "    num_layers = hp.Int(\"num_layers\", 1, 3)  # Number of LSTM layers.\n",
    "    batch_size = hp.Choice(\"batch_size\", [32, 64, 120, 256])  # Batch size choices.\n",
    "\n",
    "\n",
    "    # Add LSTM layers based on the number of layers selected.\n",
    "    for i in range(num_layers):\n",
    "        return_sequences = (i < num_layers - 1)\n",
    "        model.add(LSTM(units=units, return_sequences=return_sequences,\n",
    "                       input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]) if i == 0 else None,\n",
    "                       kernel_regularizer=l2(l2_lambda), recurrent_dropout=recurrent_dropout))\n",
    "        model.add(BatchNormalization()) # Add batch normalization to stabilize training.\n",
    "        model.add(Dropout(dropout_rate))  # Add dropout to help with overfitting.\n",
    "\n",
    "    model.add(Dense(1))  # Output layer for a single continuous value.\n",
    "    # Configure optimizer with learning rate, decay, and gradient clipping.\n",
    "    optimizer = Adam(learning_rate=learning_rate, decay=learning_rate_decay, clipnorm=clipnorm)\n",
    "     # Compile model with specified optimizer and mean squared error loss.\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    print(f\"Built model with params: dropout_rate={dropout_rate}, recurrent_dropout={recurrent_dropout}, \"\n",
    "          f\"l2_lambda={l2_lambda}, learning_rate={learning_rate}, learning_rate_decay={learning_rate_decay}, \"\n",
    "          f\"clipnorm={clipnorm}, units={units}, num_layers={num_layers}, batch_size={batch_size}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Set up Bayesian Optimization tuner to search for optimal hyperparameters.\n",
    "tuner = BayesianOptimization(\n",
    "    build_model,  # Model-building function.\n",
    "    objective=\"val_loss\",  # Target metric for optimization.\n",
    "    max_trials=30,  # Maximum number of trials to run.\n",
    "    executions_per_trial=1,  # Number of times to execute each trial.\n",
    "    directory=\"tuner_dir\",  # Directory to store tuning results.\n",
    "    project_name=\"lstm_tuning_capstone\",  # Tuning project name.\n",
    "    overwrite=True  # Overwrite existing tuner results if present.\n",
    ")\n",
    "\n",
    "# Define early stopping.\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=True)\n",
    "\n",
    "# Perform tuning with verbose logging.\n",
    "tuner.search(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_test_seq, y_test_seq),\n",
    "    epochs=200,\n",
    "    callbacks=[early_stopping],\n",
    "    shuffle=False,\n",
    "    verbose=1  # Ensure output of each trial.\n",
    ")\n",
    "\n",
    "# Retrieve the best model and hyperparameters.\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Best hyperparameters:\", best_hyperparameters.values)\n",
    "print(\"Best batch size:\", best_hyperparameters.get(\"batch_size\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c92cc51-790e-4446-8f8d-3ee850801068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 290ms/step - loss: 7.4105 - val_loss: 4.5741\n",
      "Epoch 2/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - loss: 5.6578 - val_loss: 4.1127\n",
      "Epoch 3/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 5.1281 - val_loss: 3.6670\n",
      "Epoch 4/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 4.5297 - val_loss: 3.2571\n",
      "Epoch 5/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 3.9601 - val_loss: 2.8921\n",
      "Epoch 6/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 3.4544 - val_loss: 2.5766\n",
      "Epoch 7/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 3.1847 - val_loss: 2.3006\n",
      "Epoch 8/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 2.9277 - val_loss: 2.0604\n",
      "Epoch 9/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 2.5825 - val_loss: 1.8532\n",
      "Epoch 10/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.3310 - val_loss: 1.6720\n",
      "Epoch 11/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 2.1012 - val_loss: 1.5136\n",
      "Epoch 12/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 2.0080 - val_loss: 1.3742\n",
      "Epoch 13/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 1.7605 - val_loss: 1.2519\n",
      "Epoch 14/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 1.6341 - val_loss: 1.1443\n",
      "Epoch 15/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 1.5141 - val_loss: 1.0520\n",
      "Epoch 16/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 1.3913 - val_loss: 0.9675\n",
      "Epoch 17/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 1.2527 - val_loss: 0.8946\n",
      "Epoch 18/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 1.1935 - val_loss: 0.8346\n",
      "Epoch 19/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 1.0943 - val_loss: 0.7746\n",
      "Epoch 20/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 1.0761 - val_loss: 0.7238\n",
      "Epoch 21/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.9760 - val_loss: 0.6764\n",
      "Epoch 22/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.9162 - val_loss: 0.6362\n",
      "Epoch 23/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.9036 - val_loss: 0.6010\n",
      "Epoch 24/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.8207 - val_loss: 0.5702\n",
      "Epoch 25/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.7391 - val_loss: 0.5387\n",
      "Epoch 26/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.7238 - val_loss: 0.5081\n",
      "Epoch 27/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.6899 - val_loss: 0.4851\n",
      "Epoch 28/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.6645 - val_loss: 0.4538\n",
      "Epoch 29/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.5999 - val_loss: 0.4326\n",
      "Epoch 30/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.5525 - val_loss: 0.4129\n",
      "Epoch 31/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.5209 - val_loss: 0.3945\n",
      "Epoch 32/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.5058 - val_loss: 0.3798\n",
      "Epoch 33/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.4724 - val_loss: 0.3638\n",
      "Epoch 34/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.4449 - val_loss: 0.3454\n",
      "Epoch 35/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.4263 - val_loss: 0.3402\n",
      "Epoch 36/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.4234 - val_loss: 0.3185\n",
      "Epoch 37/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.3781 - val_loss: 0.3057\n",
      "Epoch 38/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.3776 - val_loss: 0.3014\n",
      "Epoch 39/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.3615 - val_loss: 0.2767\n",
      "Epoch 40/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.3340 - val_loss: 0.3111\n",
      "Epoch 41/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.3319 - val_loss: 0.2567\n",
      "Epoch 42/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.3043 - val_loss: 0.2654\n",
      "Epoch 43/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.2918 - val_loss: 0.2433\n",
      "Epoch 44/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.2709 - val_loss: 0.2488\n",
      "Epoch 45/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2670 - val_loss: 0.2222\n",
      "Epoch 46/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.2527 - val_loss: 0.2274\n",
      "Epoch 47/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.2424 - val_loss: 0.2089\n",
      "Epoch 48/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.2275 - val_loss: 0.2234\n",
      "Epoch 49/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2175 - val_loss: 0.1973\n",
      "Epoch 50/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2275 - val_loss: 0.1898\n",
      "Epoch 51/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.2036 - val_loss: 0.1904\n",
      "Epoch 52/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1986 - val_loss: 0.1889\n",
      "Epoch 53/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1950 - val_loss: 0.1764\n",
      "Epoch 54/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1823 - val_loss: 0.1881\n",
      "Epoch 55/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1824 - val_loss: 0.1667\n",
      "Epoch 56/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1739 - val_loss: 0.1958\n",
      "Epoch 57/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1714 - val_loss: 0.1718\n",
      "Epoch 58/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1710 - val_loss: 0.1660\n",
      "Epoch 59/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1668 - val_loss: 0.1614\n",
      "Epoch 60/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1618 - val_loss: 0.1516\n",
      "Epoch 61/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1558 - val_loss: 0.1565\n",
      "Epoch 62/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1545 - val_loss: 0.1482\n",
      "Epoch 63/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.1536 - val_loss: 0.1457\n",
      "Epoch 64/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1482 - val_loss: 0.1419\n",
      "Epoch 65/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1433 - val_loss: 0.1403\n",
      "Epoch 66/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1426 - val_loss: 0.1405\n",
      "Epoch 67/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1433 - val_loss: 0.1510\n",
      "Epoch 68/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1375 - val_loss: 0.1361\n",
      "Epoch 69/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1378 - val_loss: 0.1361\n",
      "Epoch 70/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.1369 - val_loss: 0.1358\n",
      "Epoch 71/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1329 - val_loss: 0.1326\n",
      "Epoch 72/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.1316 - val_loss: 0.1293\n",
      "Epoch 73/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.1292 - val_loss: 0.1279\n",
      "Epoch 74/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1296 - val_loss: 0.1312\n",
      "Epoch 75/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1245 - val_loss: 0.1259\n",
      "Epoch 76/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1255 - val_loss: 0.1255\n",
      "Epoch 77/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1236 - val_loss: 0.1229\n",
      "Epoch 78/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.1251 - val_loss: 0.1206\n",
      "Epoch 79/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1215 - val_loss: 0.1179\n",
      "Epoch 80/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1202 - val_loss: 0.1183\n",
      "Epoch 81/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1217 - val_loss: 0.1187\n",
      "Epoch 82/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1200 - val_loss: 0.1151\n",
      "Epoch 83/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1168 - val_loss: 0.1149\n",
      "Epoch 84/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1183 - val_loss: 0.1153\n",
      "Epoch 85/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1172 - val_loss: 0.1152\n",
      "Epoch 86/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1163 - val_loss: 0.1145\n",
      "Epoch 87/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1151 - val_loss: 0.1154\n",
      "Epoch 88/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1136 - val_loss: 0.1133\n",
      "Epoch 89/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1149 - val_loss: 0.1147\n",
      "Epoch 90/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1150 - val_loss: 0.1121\n",
      "Epoch 91/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1141 - val_loss: 0.1125\n",
      "Epoch 92/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1132 - val_loss: 0.1126\n",
      "Epoch 93/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1128 - val_loss: 0.1117\n",
      "Epoch 94/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1152 - val_loss: 0.1121\n",
      "Epoch 95/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1135 - val_loss: 0.1117\n",
      "Epoch 96/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1124 - val_loss: 0.1095\n",
      "Epoch 97/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1122 - val_loss: 0.1098\n",
      "Epoch 98/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1123 - val_loss: 0.1098\n",
      "Epoch 99/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1125 - val_loss: 0.1094\n",
      "Epoch 100/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1114 - val_loss: 0.1090\n",
      "Epoch 101/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1108 - val_loss: 0.1092\n",
      "Epoch 102/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.1104 - val_loss: 0.1093\n",
      "Epoch 103/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.1107 - val_loss: 0.1093\n",
      "Epoch 104/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.1106 - val_loss: 0.1094\n",
      "Epoch 105/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.1101 - val_loss: 0.1096\n",
      "Epoch 106/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.1100 - val_loss: 0.1114\n",
      "Epoch 107/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.1102 - val_loss: 0.1091\n",
      "Epoch 108/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.1087 - val_loss: 0.1103\n",
      "Epoch 109/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1107 - val_loss: 0.1114\n",
      "Epoch 110/200\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.1098 - val_loss: 0.1156\n",
      "Final Training Loss: 0.11804936081171036\n",
      "Final Validation Loss: 0.11559329181909561\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Best hyperparameters found from tuning\n",
    "best_params = {\n",
    "    'units1': 32,\n",
    "    'units2': 32,\n",
    "    'recurrent_dropout': 0.2,\n",
    "    'optimizer': 'adam',\n",
    "    'loss_function': 'mean_squared_error',\n",
    "    'learning_rate_decay': 1e-05,\n",
    "    'learning_rate': 0.001,\n",
    "    'l2_lambda': 0.07913979537147964,\n",
    "    'epochs': 200,\n",
    "    'dropout_rate': 0.3,\n",
    "    'clipnorm': 5.0,\n",
    "    'batch_size': 64\n",
    "}\n",
    "\n",
    "# Build the model using the best parameters\n",
    "def build_best_model(params):\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units1'], return_sequences=True, \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "                   kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Second LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units2'], return_sequences=False, kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Select the optimizer\n",
    "    if params['optimizer'] == 'adam':\n",
    "        optimizer = Adam(learning_rate=params['learning_rate'], decay=params['learning_rate_decay'], clipnorm=params['clipnorm'])\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizer '{params['optimizer']}' is not implemented in this setup.\")\n",
    "        \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate the model with the best hyperparameters\n",
    "model = build_best_model(best_params)\n",
    "\n",
    "# Early stopping to stop training when validation loss stops improving\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq, \n",
    "    epochs=best_params['epochs'], \n",
    "    batch_size=best_params['batch_size'], \n",
    "    validation_data=(X_test_seq, y_test_seq),\n",
    "    shuffle=False,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Print the final training and validation loss\n",
    "print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c7b7c337-0a8c-4c49-939d-714aefd20671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 97ms/step\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step  \n",
      "LSTM Model Performance:\n",
      "Training RMSE: 0.020533080287349854\n",
      "Test RMSE: 0.024094763223004478\n",
      "Training MAE: 0.015166463661876461\n",
      "Test MAE: 0.0185496999556951\n",
      "Directional Accuracy on Training Data: 60.65959952885748%\n",
      "Directional Accuracy on Test Data: 53.90625%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict on both training and test sequences\n",
    "y_train_pred = model.predict(X_train_seq)\n",
    "y_test_pred = model.predict(X_test_seq)\n",
    "\n",
    "# Inverse scale predictions and actual values to original scale\n",
    "y_train_pred_rescaled = scaler_y.inverse_transform(y_train_pred)\n",
    "y_test_pred_rescaled = scaler_y.inverse_transform(y_test_pred)\n",
    "y_train_actual_rescaled = scaler_y.inverse_transform(y_train_seq)\n",
    "y_test_actual_rescaled = scaler_y.inverse_transform(y_test_seq)\n",
    "\n",
    "# Define a function for directional accuracy\n",
    "def directional_accuracy(y_true, y_pred):\n",
    "    return np.mean((np.sign(y_true[1:] - y_true[:-1]) == np.sign(y_pred[1:] - y_pred[:-1])).astype(int)) * 100\n",
    "\n",
    "# Calculate metrics.\n",
    "# Root Mean Squared Error (RMSE) for training set\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_actual_rescaled, y_train_pred_rescaled))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_actual_rescaled, y_test_pred_rescaled)) # RMSE for test set.\n",
    "train_mae = mean_absolute_error(y_train_actual_rescaled, y_train_pred_rescaled) # Mean Absolute Error (MAE) for training set.\n",
    "test_mae = mean_absolute_error(y_test_actual_rescaled, y_test_pred_rescaled) # MAE for test set.\n",
    "# Directional accuracy for training set.\n",
    "train_directional_accuracy = directional_accuracy(y_train_actual_rescaled.flatten(), y_train_pred_rescaled.flatten())\n",
    "# Directional accuracy for test set.\n",
    "test_directional_accuracy = directional_accuracy(y_test_actual_rescaled.flatten(), y_test_pred_rescaled.flatten())\n",
    "\n",
    "# Display score metrics to evaluate the model's performance.\n",
    "print(\"LSTM Model Performance:\")\n",
    "print(f\"Training RMSE: {train_rmse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Training MAE: {train_mae}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Directional Accuracy on Training Data: {train_directional_accuracy}%\")\n",
    "print(f\"Directional Accuracy on Test Data: {test_directional_accuracy}%\")\n",
    "\n",
    "# Plot training & validation loss values, it will help visualize how well the model fits the data over training epochs.\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa4e0eb-d53e-43f5-9083-8a6259681c8d",
   "metadata": {},
   "source": [
    "### Model Performance Metrics for Sequence Length = 26\n",
    "\n",
    "| Metric         | Model 1       | Model 2       | Model 3       | Model 4       |\n",
    "|-----------------|---------------|---------------|---------------|---------------|\n",
    "| **Train RMSE** | 0.0204843     | 0.020547484   | 0.020486306   | 0.02053308    |\n",
    "| **Test RMSE**  | 0.02540435    | 0.023991934   | 0.024209731   | 0.024094763   |\n",
    "| **Train MAE**  | 0.014934724   | 0.014950943   | 0.015043481   | 0.015166464   |\n",
    "| **Test MAE**   | 0.019465707   | 0.018310064   | 0.018667072   | 0.0185497     |\n",
    "| **Train Loss** | 0.145420581   | 0.115686074   | 0.133479208   | 0.118049361   |\n",
    "| **Val Loss**   | 0.151149169   | 0.135518551   | 0.125732034   | 0.115593292   |\n",
    "| **Train DA**   | 61%           | 53%           | 65%           | 61%           |\n",
    "| **Test DA**    | 55%           | 48%           | 67%           | 54%           |\n",
    "\n",
    "The model for sequence length did not perfomed well enough, looking at the validation loss curve throught the different models, it is observable small signs of overfitting, the curve is unstable (can see more on the model 2 plot). Thus, **the model the performed better was the model 3**, having balanced metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5dfc56-9c1f-4f12-ae45-c5b0f6246ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting train and val loss:\n",
    "scaled_train_loss = history.history['loss'][-1]  # Final train loss\n",
    "scaled_val_loss = history.history['val_loss'][-1]  # Final validation loss\n",
    "\n",
    "# Descale train and validation loss\n",
    "descaled_train_loss = np.sqrt(scaled_train_loss) * scaler_y.scale_[0] + scaler_y.mean_[0]\n",
    "descaled_val_loss = np.sqrt(scaled_val_loss) * scaler_y.scale_[0] + scaler_y.mean_[0]\n",
    "\n",
    "print(f\"Descaled Training Loss: {descaled_train_loss}\")\n",
    "print(f\"Descaled Validation Loss: {descaled_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de4421e8-9532-4a1e-a86e-c11d49c6c39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step\n",
      "Fold 1 RMSE: 0.46833997979179554\n",
      "Fold 2\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Fold 2 RMSE: 0.3154315816533551\n",
      "Fold 3\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "Fold 3 RMSE: 0.2537473994401615\n",
      "Fold 4\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "Fold 4 RMSE: 0.38765997059678897\n",
      "Average RMSE from TSCV: 0.35629473287052527\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# Best hyperparameters found from tuning\n",
    "best_params = {\n",
    "    'units1': 128,\n",
    "    'recurrent_dropout': 0.1,\n",
    "    'optimizer': 'adam',\n",
    "    'loss_function': 'mean_squared_error',\n",
    "    'learning_rate_decay': 1e-06,\n",
    "    'learning_rate': 0.001,\n",
    "    'l2_lambda': 0.0010012325682668814,\n",
    "    'epochs': 200,\n",
    "    'dropout_rate': 0.2,\n",
    "    'clipnorm': 5.0,\n",
    "    'batch_size': 64\n",
    "}\n",
    "\n",
    "# Build the model using the best parameters\n",
    "def build_best_model(params):\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units1'], return_sequences=False, \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "                   kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Select the optimizer\n",
    "    optimizer = Adam(learning_rate=params['learning_rate'], \n",
    "                     decay=params['learning_rate_decay'], \n",
    "                     clipnorm=params['clipnorm'])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss=params['loss_function'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define the number of splits for Time Series Cross-Validation\n",
    "n_splits = 4\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Initialize the list to store RMSE scores\n",
    "tscv_rmse_scores = []\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Create the model once and save its initial weights\n",
    "model = build_best_model(best_params)\n",
    "initial_weights = model.get_weights()\n",
    "\n",
    "# Perform TSCV\n",
    "for fold, (train_index, test_index) in enumerate(tscv.split(X_train_scaled)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "    # Reset model to initial weights\n",
    "    model.set_weights(initial_weights)\n",
    "    \n",
    "    # Define train and test sets\n",
    "    train, test = X_train_scaled[train_index], X_train_scaled[test_index]\n",
    "    y_train_fold, y_test_fold = y_train_scaled[train_index], y_train_scaled[test_index]\n",
    "    \n",
    "    # Create sequences\n",
    "    X_train_seq, y_train_seq = [], []\n",
    "    for i in range(len(train) - sequence_length):\n",
    "        X_train_seq.append(train[i:i + sequence_length])\n",
    "        y_train_seq.append(y_train_fold[i + sequence_length])\n",
    "    \n",
    "    X_test_seq, y_test_seq = [], []\n",
    "    for i in range(len(test) - sequence_length):\n",
    "        X_test_seq.append(test[i:i + sequence_length])\n",
    "        y_test_seq.append(y_test_fold[i + sequence_length])\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_train_seq, y_train_seq = np.array(X_train_seq), np.array(y_train_seq)\n",
    "    X_test_seq, y_test_seq = np.array(X_test_seq), np.array(y_test_seq)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=best_params['epochs'],\n",
    "        batch_size=best_params['batch_size'],\n",
    "        validation_data=(X_test_seq, y_test_seq),\n",
    "        shuffle=False,\n",
    "        verbose=0,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = model.predict(X_test_seq)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_seq, y_pred))\n",
    "    tscv_rmse_scores.append(rmse)\n",
    "    print(f\"Fold {fold + 1} RMSE: {rmse}\")\n",
    "\n",
    "# Calculate the average RMSE across all folds\n",
    "avg_rmse = np.mean(tscv_rmse_scores)\n",
    "print(f\"Average RMSE from TSCV: {avg_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dccc560e-5433-4e92-82e1-81a7de58de3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling window starting at index 0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349ms/step\n",
      "Rolling window RMSE: 0.3991616858588019\n",
      "Rolling window starting at index 50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351ms/step\n",
      "Rolling window RMSE: 0.5098760372077791\n",
      "Rolling window starting at index 100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330ms/step\n",
      "Rolling window RMSE: 0.3327980303684711\n",
      "Rolling window starting at index 150\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403ms/step\n",
      "Rolling window RMSE: 0.2702229902037996\n",
      "Rolling window starting at index 200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372ms/step\n",
      "Rolling window RMSE: 0.2557481528404545\n",
      "Rolling window starting at index 250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377ms/step\n",
      "Rolling window RMSE: 0.31917286107168724\n",
      "Rolling window starting at index 300\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353ms/step\n",
      "Rolling window RMSE: 0.20000460366539183\n",
      "Rolling window starting at index 350\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step\n",
      "Rolling window RMSE: 0.1422835826850441\n",
      "Rolling window starting at index 400\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314ms/step\n",
      "Rolling window RMSE: 0.27033938890783776\n",
      "Rolling window starting at index 450\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299ms/step\n",
      "Rolling window RMSE: 0.2369460206656567\n",
      "Rolling window starting at index 500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 288ms/step\n",
      "Rolling window RMSE: 0.5802530175640092\n",
      "Average Rolling Window RMSE: 0.31970967009444845\n"
     ]
    }
   ],
   "source": [
    "# Parameters for FRWCV\n",
    "train_window = 300  # Training window size\n",
    "test_window = 50    # Test window size\n",
    "\n",
    "# Store RMSEs for each window\n",
    "rolling_rmse_scores = []\n",
    "\n",
    "for start in range(0, len(X_train_scaled) - train_window - test_window, test_window):\n",
    "    print(f\"Rolling window starting at index {start}\")\n",
    "    \n",
    "    # Define train and test sets for the window\n",
    "    train = X_train_scaled[start:start + train_window]\n",
    "    test = X_train_scaled[start + train_window:start + train_window + test_window]\n",
    "    y_train_fold = y_train_scaled[start:start + train_window]\n",
    "    y_test_fold = y_train_scaled[start + train_window:start + train_window + test_window]\n",
    "    \n",
    "    # Create sequences for train and test\n",
    "    X_train_seq, y_train_seq = [], []\n",
    "    for i in range(len(train) - sequence_length):\n",
    "        X_train_seq.append(train[i:i + sequence_length])\n",
    "        y_train_seq.append(y_train_fold[i + sequence_length])\n",
    "    \n",
    "    X_test_seq, y_test_seq = [], []\n",
    "    for i in range(len(test) - sequence_length):\n",
    "        X_test_seq.append(test[i:i + sequence_length])\n",
    "        y_test_seq.append(y_test_fold[i + sequence_length])\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_train_seq, y_train_seq = np.array(X_train_seq), np.array(y_train_seq)\n",
    "    X_test_seq, y_test_seq = np.array(X_test_seq), np.array(y_test_seq)\n",
    "    \n",
    "    # Build and train the model\n",
    "    model = build_best_model(best_params)\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq, \n",
    "        epochs=best_params['epochs'], \n",
    "        batch_size=best_params['batch_size'], \n",
    "        validation_data=(X_test_seq, y_test_seq), \n",
    "        shuffle=False,\n",
    "        verbose=0,  # Suppress training logs for brevity\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    y_pred = model.predict(X_test_seq)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_seq, y_pred))\n",
    "    rolling_rmse_scores.append(rmse)\n",
    "    print(f\"Rolling window RMSE: {rmse}\")\n",
    "\n",
    "# Calculate average RMSE across rolling windows\n",
    "avg_rolling_rmse = np.mean(rolling_rmse_scores)\n",
    "print(f\"Average Rolling Window RMSE: {avg_rolling_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b675ba-669e-4859-8b16-a93a5e422acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to descale RMSE\n",
    "def descale_rmse(scaled_rmse, scaler_y):\n",
    "    # Reshape RMSE value to fit scaler requirements\n",
    "    scaled_rmse_array = np.array(scaled_rmse).reshape(-1, 1)\n",
    "    # Inverse transform the RMSE\n",
    "    descaled_rmse_array = scaler_y.inverse_transform(scaled_rmse_array)\n",
    "    # Return the descaled RMSE as a list\n",
    "    return descaled_rmse_array.flatten().tolist()\n",
    "\n",
    "# Descale TSCV RMSE values\n",
    "descaled_tscv_rmse = descale_rmse(tscv_rmse_scores, scaler_y)\n",
    "print(\"Descaled TSCV RMSE Values:\", descaled_tscv_rmse)\n",
    "\n",
    "# Descale FRWCV RMSE values\n",
    "descaled_frwcv_rmse = descale_rmse(rolling_rmse_scores, scaler_y)\n",
    "print(\"Descaled FRWCV RMSE Values:\", descaled_frwcv_rmse)\n",
    "\n",
    "# Calculates average descaled RMSE\n",
    "average_tscv_rmse = np.mean(descaled_tscv_rmse)\n",
    "average_frwcv_rmse = np.mean(descaled_frwcv_rmse)\n",
    "print(f\"Average Descaled TSCV RMSE: {average_tscv_rmse}\")\n",
    "print(f\"Average Descaled FRWCV RMSE: {average_frwcv_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfa291f-e81c-4a83-9bf2-8ba41491065f",
   "metadata": {},
   "source": [
    "## Sequence Length = 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adfed385-7d8b-4064-8dc1-49e07e1c187d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped X_train_seq shape: (824, 52, 5)\n",
      "Reshaped y_train_seq shape: (824, 1)\n",
      "Reshaped X_test_seq shape: (103, 52, 5)\n",
      "Reshaped  y_test_seq shape: (103, 1)\n"
     ]
    }
   ],
   "source": [
    "# Set the sequence length for LSTM input, representing the number of time steps.\n",
    "sequence_length = 52  # Number of time steps used for target prediction, 52 weeks (One year).\n",
    "\n",
    "# Reshape data into sequences for LSTM.\n",
    "# Initialize empty lists to hold the sequences for the training set.\n",
    "X_train_seq, y_train_seq = [], []\n",
    "# Loop through the training data to create sequences of features and corresponding target values.\n",
    "for i in range(len(X_train_scaled) - sequence_length):\n",
    "    # Append a sequence of features for the current window.\n",
    "    X_train_seq.append(X_train_scaled[i:i + sequence_length]) # Sequence of features.\n",
    "    # Append the target value that comes immediately after the sequence.\n",
    "    y_train_seq.append(y_train_scaled[i + sequence_length]) # Target value following the sequence.\n",
    "# Initialize empty lists to hold the sequences for the test set    \n",
    "X_test_seq, y_test_seq = [], []\n",
    "# Same process for the test sets.\n",
    "for i in range(len(X_test_scaled) - sequence_length):\n",
    "    # Append a sequence of features for the current window in the test set.\n",
    "    X_test_seq.append(X_test_scaled[i:i + sequence_length])\n",
    "    # Append the target value immediately after the sequence in the test set.\n",
    "    y_test_seq.append(y_test_scaled[i + sequence_length])\n",
    "\n",
    "# Convert lists to numpy arrays to use in Neural Networks.\n",
    "X_train_seq, y_train_seq = np.array(X_train_seq), np.array(y_train_seq)\n",
    "X_test_seq, y_test_seq = np.array(X_test_seq), np.array(y_test_seq)\n",
    "\n",
    "# Print the reshaped input data for LSTM.\n",
    "print(f\"Reshaped X_train_seq shape: {X_train_seq.shape}\")\n",
    "print(f\"Reshaped y_train_seq shape: {y_train_seq.shape}\")\n",
    "print(f\"Reshaped X_test_seq shape: {X_test_seq.shape}\")\n",
    "print(f\"Reshaped  y_test_seq shape: { y_test_seq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabfeefd-1030-46a9-98dd-d8360110c6a8",
   "metadata": {},
   "source": [
    "### 1. Random Search (Two Layers - Model 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "02df0bff-13da-4447-9d23-aa502e430e91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running combination 1/30: {'units2': 32, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 64}\n",
      "Final Training Loss: 0.15308767557144165\n",
      "Final Validation Loss: 0.19366386532783508\n",
      "Running combination 2/30: {'units2': 128, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 0.7978256940841675\n",
      "Final Validation Loss: 0.588202714920044\n",
      "Running combination 3/30: {'units2': 64, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 64}\n",
      "Final Training Loss: 0.9574838280677795\n",
      "Final Validation Loss: 0.3374508023262024\n",
      "Running combination 4/30: {'units2': 64, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 50, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 32}\n",
      "Final Training Loss: 5.98805046081543\n",
      "Final Validation Loss: 5.138758182525635\n",
      "Running combination 5/30: {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 5.0, 'batch_size': 64}\n",
      "Final Training Loss: 2.1056315898895264\n",
      "Final Validation Loss: 1.5227985382080078\n",
      "Running combination 6/30: {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 120}\n",
      "Final Training Loss: 0.7837957739830017\n",
      "Final Validation Loss: 0.2470327615737915\n",
      "Running combination 7/30: {'units2': 32, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 64}\n",
      "Final Training Loss: 9.246450424194336\n",
      "Final Validation Loss: 8.302845001220703\n",
      "Running combination 8/30: {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 50, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 0.929799497127533\n",
      "Final Validation Loss: 0.6275292038917542\n",
      "Running combination 9/30: {'units2': 64, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 1.9673290252685547\n",
      "Final Validation Loss: 1.3995723724365234\n",
      "Running combination 10/30: {'units2': 32, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 0.47199857234954834\n",
      "Final Validation Loss: 0.24134118854999542\n",
      "Running combination 11/30: {'units2': 64, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 32}\n",
      "Final Training Loss: 5.52427339553833\n",
      "Final Validation Loss: 5.043456554412842\n",
      "Running combination 12/30: {'units2': 32, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 0.811389148235321\n",
      "Final Validation Loss: 0.23994146287441254\n",
      "Running combination 13/30: {'units2': 128, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 3.6757819652557373\n",
      "Final Validation Loss: 2.830618143081665\n",
      "Running combination 14/30: {'units2': 64, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 256}\n",
      "Final Training Loss: 0.9306192398071289\n",
      "Final Validation Loss: 0.24790140986442566\n",
      "Running combination 15/30: {'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 32}\n",
      "Final Training Loss: 0.8522672057151794\n",
      "Final Validation Loss: 0.30435073375701904\n",
      "Running combination 16/30: {'units2': 32, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 256}\n",
      "Final Training Loss: 1.718379020690918\n",
      "Final Validation Loss: 1.0740934610366821\n",
      "Running combination 17/30: {'units2': 128, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 5.0, 'batch_size': 120}\n",
      "Final Training Loss: 11.467302322387695\n",
      "Final Validation Loss: 9.938825607299805\n",
      "Running combination 18/30: {'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 200, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 13.071318626403809\n",
      "Final Validation Loss: 12.504610061645508\n",
      "Running combination 19/30: {'units2': 128, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 18.961305618286133\n",
      "Final Validation Loss: 18.311737060546875\n",
      "Running combination 20/30: {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 1.742727279663086\n",
      "Final Validation Loss: 0.8235494494438171\n",
      "Running combination 21/30: {'units2': 32, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.001, 'l2_lambda': 0.01, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 0.5063695311546326\n",
      "Final Validation Loss: 0.40033578872680664\n",
      "Running combination 22/30: {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.01, 'epochs': 50, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 1.2463914155960083\n",
      "Final Validation Loss: 0.6943463683128357\n",
      "Running combination 23/30: {'units2': 32, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 50, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 6.138966083526611\n",
      "Final Validation Loss: 5.180455684661865\n",
      "Running combination 24/30: {'units2': 64, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 2.231800079345703\n",
      "Final Validation Loss: 1.7667698860168457\n",
      "Running combination 25/30: {'units2': 32, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 256}\n",
      "Final Training Loss: 14.026309967041016\n",
      "Final Validation Loss: 13.166943550109863\n",
      "Running combination 26/30: {'units2': 32, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 64}\n",
      "Final Training Loss: 1.5392659902572632\n",
      "Final Validation Loss: 0.705216646194458\n",
      "Running combination 27/30: {'units2': 64, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 1.4581100940704346\n",
      "Final Validation Loss: 0.3011850416660309\n",
      "Running combination 28/30: {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 0.37820756435394287\n",
      "Final Validation Loss: 0.21705497801303864\n",
      "Running combination 29/30: {'units2': 128, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0001, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 256}\n",
      "Final Training Loss: 1.1006970405578613\n",
      "Final Validation Loss: 0.3155522644519806\n",
      "Running combination 30/30: {'units2': 32, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 200, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 64}\n",
      "Final Training Loss: 4.683297157287598\n",
      "Final Validation Loss: 4.181281566619873\n",
      "Top results:\n",
      "{'params': {'units2': 32, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 64}, 'final_train_loss': 0.15308767557144165, 'final_val_loss': 0.19366386532783508}\n",
      "{'params': {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 32}, 'final_train_loss': 0.37820756435394287, 'final_val_loss': 0.21705497801303864}\n",
      "{'params': {'units2': 32, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 120}, 'final_train_loss': 0.811389148235321, 'final_val_loss': 0.23994146287441254}\n",
      "{'params': {'units2': 32, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 32}, 'final_train_loss': 0.47199857234954834, 'final_val_loss': 0.24134118854999542}\n",
      "{'params': {'units2': 128, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 120}, 'final_train_loss': 0.7837957739830017, 'final_val_loss': 0.2470327615737915}\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Define parameter grid for random hyperparameter search.\n",
    "param_grid = {\n",
    "    'dropout_rate': [0.2, 0.3, 0.4],               # Dropout rate for regularization.\n",
    "    'recurrent_dropout': [0.1, 0.2],               # Recurrent dropout within LSTM layers.\n",
    "    'l2_lambda': [0.001, 0.01, 0.1],               # L2 regularization strength.\n",
    "    'learning_rate': [0.001, 0.0005, 0.0001],      # Learning rate for optimizer.\n",
    "    'learning_rate_decay': [1e-6, 1e-5, 0],        # Decay rate for learning rate over time.\n",
    "    'units1': [32, 64, 128],                       # Number of units in the first LSTM layer.\n",
    "    'units2': [32, 64, 128],                       # Number of units in the second LSTM layer.\n",
    "    'batch_size': [32, 64, 120, 256],              # Batch size for training.\n",
    "    'epochs': [50, 100, 200],                      # Number of epochs to train.\n",
    "    'optimizer': ['adam'],                         # Optimizer to use.\n",
    "    'clipnorm': [1.0, 5.0]                         # Gradient clipping to avoid exploding gradients.\n",
    "}\n",
    "\n",
    "# Generate a list of random combinations of hyperparameters.\n",
    "n_iter_search = 30  # Number of random combinations to attempt.\n",
    "# Random state being applied for reproducibility. \n",
    "random_combinations = list(ParameterSampler(param_grid, n_iter=n_iter_search, random_state=42)) \n",
    "\n",
    "# Define a function to build the LSTM model with given parameters.\n",
    "def build_model(dropout_rate, recurrent_dropout, l2_lambda, learning_rate, learning_rate_decay, clipnorm, units1, units2, optimizer_name, loss_function):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First LSTM layer with BatchNorm, Dropout, and recurrent_dropout.\n",
    "    model.add(LSTM(units=units1, return_sequences=True, \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),  # Input shape based on sequence data.\n",
    "                   kernel_regularizer=l2(l2_lambda), recurrent_dropout=recurrent_dropout))\n",
    "    model.add(BatchNormalization())  # Batch normalization for stable training.\n",
    "    model.add(Dropout(dropout_rate))  # Dropout for regularization.\n",
    "    \n",
    "    # Second LSTM layer with BatchNorm, Dropout, and recurrent_dropout.\n",
    "    model.add(LSTM(units=units2, return_sequences=False, kernel_regularizer=l2(l2_lambda), recurrent_dropout=recurrent_dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer with a single unit for regression output.\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Select optimizer with learning rate decay and gradient clipping.\n",
    "    optimizer = Adam(learning_rate=learning_rate, decay=learning_rate_decay, clipnorm=clipnorm)\n",
    "    \n",
    "    # Compile the model with the chosen optimizer and loss function.\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define early stopping to stop training when validation loss does not improve.\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Placeholder to store results of each model configuration.\n",
    "results = []\n",
    "\n",
    "# Run random search\n",
    "for i, params in enumerate(random_combinations):\n",
    "    print(f\"Running combination {i+1}/{len(random_combinations)}: {params}\")\n",
    "    \n",
    "    # Build model with the current parameters\n",
    "    model = build_model(\n",
    "        dropout_rate=params['dropout_rate'],\n",
    "        recurrent_dropout=params['recurrent_dropout'],\n",
    "        l2_lambda=params['l2_lambda'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        learning_rate_decay=params['learning_rate_decay'],\n",
    "        clipnorm=params['clipnorm'],\n",
    "        units1=params['units1'],\n",
    "        units2=params['units2'],\n",
    "        optimizer_name=params['optimizer'],\n",
    "        loss_function='mean_squared_error'\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq, \n",
    "        epochs=params['epochs'], \n",
    "        batch_size=params['batch_size'], \n",
    "        validation_data=(X_test_seq, y_test_seq), \n",
    "        callbacks=[early_stopping],\n",
    "        shuffle=False,\n",
    "        verbose=0  # Set verbose=0 to reduce output\n",
    "    )\n",
    "    \n",
    "    # Get final validation loss and training loss\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    \n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'params': params,\n",
    "        'final_train_loss': final_train_loss,\n",
    "        'final_val_loss': final_val_loss\n",
    "    })\n",
    "    \n",
    "    print(f\"Final Training Loss: {final_train_loss}\")\n",
    "    print(f\"Final Validation Loss: {final_val_loss}\")\n",
    "\n",
    "results = sorted(results, key=lambda x: x['final_val_loss'])\n",
    "print(\"Top results:\")\n",
    "for res in results[:5]:  # Show top 5 results\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a8113dbb-ab39-4910-89f2-dd14dff4f639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 334ms/step - loss: 3.0706 - val_loss: 0.7894\n",
      "Epoch 2/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 2.3499 - val_loss: 0.7886\n",
      "Epoch 3/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 1.9940 - val_loss: 0.7685\n",
      "Epoch 4/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 1.9330 - val_loss: 0.7525\n",
      "Epoch 5/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 1.7718 - val_loss: 0.7405\n",
      "Epoch 6/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 1.5964 - val_loss: 0.7345\n",
      "Epoch 7/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - loss: 1.5072 - val_loss: 0.7254\n",
      "Epoch 8/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 1.6778 - val_loss: 0.7198\n",
      "Epoch 9/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 1.4700 - val_loss: 0.7134\n",
      "Epoch 10/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - loss: 1.4249 - val_loss: 0.7043\n",
      "Epoch 11/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 1.4054 - val_loss: 0.6949\n",
      "Epoch 12/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 1.4243 - val_loss: 0.6881\n",
      "Epoch 13/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 1.3763 - val_loss: 0.6808\n",
      "Epoch 14/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 1.3525 - val_loss: 0.6731\n",
      "Epoch 15/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 1.2684 - val_loss: 0.6677\n",
      "Epoch 16/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 1.2168 - val_loss: 0.6624\n",
      "Epoch 17/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 1.1994 - val_loss: 0.6549\n",
      "Epoch 18/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - loss: 1.2301 - val_loss: 0.6481\n",
      "Epoch 19/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 1.1659 - val_loss: 0.6406\n",
      "Epoch 20/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 1.1527 - val_loss: 0.6340\n",
      "Epoch 21/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 1.1196 - val_loss: 0.6264\n",
      "Epoch 22/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 1.1567 - val_loss: 0.6206\n",
      "Epoch 23/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 1.1758 - val_loss: 0.6143\n",
      "Epoch 24/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 1.1277 - val_loss: 0.6068\n",
      "Epoch 25/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 1.0674 - val_loss: 0.6002\n",
      "Epoch 26/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 0.9722 - val_loss: 0.5945\n",
      "Epoch 27/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 0.9928 - val_loss: 0.5894\n",
      "Epoch 28/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 1.0384 - val_loss: 0.5836\n",
      "Epoch 29/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 0.9778 - val_loss: 0.5769\n",
      "Epoch 30/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 0.9079 - val_loss: 0.5696\n",
      "Epoch 31/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.8838 - val_loss: 0.5630\n",
      "Epoch 32/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.8792 - val_loss: 0.5555\n",
      "Epoch 33/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.8835 - val_loss: 0.5541\n",
      "Epoch 34/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.8606 - val_loss: 0.5503\n",
      "Epoch 35/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.8840 - val_loss: 0.5448\n",
      "Epoch 36/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.7508 - val_loss: 0.5390\n",
      "Epoch 37/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.8495 - val_loss: 0.5349\n",
      "Epoch 38/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.7992 - val_loss: 0.5286\n",
      "Epoch 39/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.7861 - val_loss: 0.5212\n",
      "Epoch 40/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.7890 - val_loss: 0.5198\n",
      "Epoch 41/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.7866 - val_loss: 0.5126\n",
      "Epoch 42/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 0.7219 - val_loss: 0.5064\n",
      "Epoch 43/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 0.7223 - val_loss: 0.5022\n",
      "Epoch 44/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 0.7128 - val_loss: 0.5020\n",
      "Epoch 45/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.7078 - val_loss: 0.4982\n",
      "Epoch 46/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.7217 - val_loss: 0.4883\n",
      "Epoch 47/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.6756 - val_loss: 0.4807\n",
      "Epoch 48/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.6311 - val_loss: 0.4766\n",
      "Epoch 49/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 0.6631 - val_loss: 0.4747\n",
      "Epoch 50/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.6541 - val_loss: 0.4698\n",
      "Epoch 51/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.6369 - val_loss: 0.4698\n",
      "Epoch 52/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.6174 - val_loss: 0.4668\n",
      "Epoch 53/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.6169 - val_loss: 0.4611\n",
      "Epoch 54/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.5755 - val_loss: 0.4563\n",
      "Epoch 55/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.5909 - val_loss: 0.4551\n",
      "Epoch 56/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.5844 - val_loss: 0.4455\n",
      "Epoch 57/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.5705 - val_loss: 0.4418\n",
      "Epoch 58/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.5394 - val_loss: 0.4419\n",
      "Epoch 59/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.5483 - val_loss: 0.4477\n",
      "Epoch 60/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.5378 - val_loss: 0.4438\n",
      "Epoch 61/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.5226 - val_loss: 0.4392\n",
      "Epoch 62/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.5449 - val_loss: 0.4380\n",
      "Epoch 63/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.4972 - val_loss: 0.4339\n",
      "Epoch 64/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.5119 - val_loss: 0.4275\n",
      "Epoch 65/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.4940 - val_loss: 0.4201\n",
      "Epoch 66/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.4795 - val_loss: 0.4170\n",
      "Epoch 67/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.5027 - val_loss: 0.4174\n",
      "Epoch 68/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.4705 - val_loss: 0.4117\n",
      "Epoch 69/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.4753 - val_loss: 0.4045\n",
      "Epoch 70/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.4352 - val_loss: 0.4006\n",
      "Epoch 71/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.4390 - val_loss: 0.3966\n",
      "Epoch 72/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.4562 - val_loss: 0.3911\n",
      "Epoch 73/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.4188 - val_loss: 0.3860\n",
      "Epoch 74/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 0.4356 - val_loss: 0.3832\n",
      "Epoch 75/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.4184 - val_loss: 0.3773\n",
      "Epoch 76/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.4475 - val_loss: 0.3749\n",
      "Epoch 77/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.4073 - val_loss: 0.3772\n",
      "Epoch 78/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.4012 - val_loss: 0.3743\n",
      "Epoch 79/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.3980 - val_loss: 0.3701\n",
      "Epoch 80/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 0.3992 - val_loss: 0.3660\n",
      "Epoch 81/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 0.3881 - val_loss: 0.3635\n",
      "Epoch 82/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.3950 - val_loss: 0.3605\n",
      "Epoch 83/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.3780 - val_loss: 0.3569\n",
      "Epoch 84/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.3706 - val_loss: 0.3549\n",
      "Epoch 85/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.3669 - val_loss: 0.3512\n",
      "Epoch 86/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.3589 - val_loss: 0.3497\n",
      "Epoch 87/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.3593 - val_loss: 0.3450\n",
      "Epoch 88/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 0.3657 - val_loss: 0.3415\n",
      "Epoch 89/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.3494 - val_loss: 0.3398\n",
      "Epoch 90/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.3416 - val_loss: 0.3366\n",
      "Epoch 91/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.3282 - val_loss: 0.3330\n",
      "Epoch 92/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.3279 - val_loss: 0.3295\n",
      "Epoch 93/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 0.3313 - val_loss: 0.3279\n",
      "Epoch 94/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 0.3305 - val_loss: 0.3268\n",
      "Epoch 95/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.3150 - val_loss: 0.3242\n",
      "Epoch 96/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.3177 - val_loss: 0.3209\n",
      "Epoch 97/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 0.3202 - val_loss: 0.3196\n",
      "Epoch 98/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.3092 - val_loss: 0.3170\n",
      "Epoch 99/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.3058 - val_loss: 0.3149\n",
      "Epoch 100/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 0.2993 - val_loss: 0.3125\n",
      "Epoch 101/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.2996 - val_loss: 0.3103\n",
      "Epoch 102/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 0.3081 - val_loss: 0.3080\n",
      "Epoch 103/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.2902 - val_loss: 0.3042\n",
      "Epoch 104/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.2898 - val_loss: 0.3014\n",
      "Epoch 105/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.2970 - val_loss: 0.3016\n",
      "Epoch 106/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.2744 - val_loss: 0.3005\n",
      "Epoch 107/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.2778 - val_loss: 0.2964\n",
      "Epoch 108/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.2794 - val_loss: 0.2944\n",
      "Epoch 109/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.2785 - val_loss: 0.2922\n",
      "Epoch 110/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.2754 - val_loss: 0.2918\n",
      "Epoch 111/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.2648 - val_loss: 0.2886\n",
      "Epoch 112/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.2601 - val_loss: 0.2862\n",
      "Epoch 113/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.2624 - val_loss: 0.2855\n",
      "Epoch 114/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 0.2571 - val_loss: 0.2842\n",
      "Epoch 115/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.2577 - val_loss: 0.2813\n",
      "Epoch 116/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 0.2537 - val_loss: 0.2792\n",
      "Epoch 117/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.2538 - val_loss: 0.2782\n",
      "Epoch 118/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.2545 - val_loss: 0.2767\n",
      "Epoch 119/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.2482 - val_loss: 0.2749\n",
      "Epoch 120/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.2472 - val_loss: 0.2727\n",
      "Epoch 121/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.2484 - val_loss: 0.2707\n",
      "Epoch 122/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.2375 - val_loss: 0.2690\n",
      "Epoch 123/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.2357 - val_loss: 0.2679\n",
      "Epoch 124/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.2365 - val_loss: 0.2668\n",
      "Epoch 125/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.2334 - val_loss: 0.2651\n",
      "Epoch 126/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.2422 - val_loss: 0.2624\n",
      "Epoch 127/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.2304 - val_loss: 0.2607\n",
      "Epoch 128/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.2273 - val_loss: 0.2610\n",
      "Epoch 129/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.2316 - val_loss: 0.2597\n",
      "Epoch 130/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.2274 - val_loss: 0.2568\n",
      "Epoch 131/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.2279 - val_loss: 0.2552\n",
      "Epoch 132/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.2191 - val_loss: 0.2548\n",
      "Epoch 133/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.2188 - val_loss: 0.2526\n",
      "Epoch 134/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.2231 - val_loss: 0.2500\n",
      "Epoch 135/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.2106 - val_loss: 0.2481\n",
      "Epoch 136/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - loss: 0.2104 - val_loss: 0.2474\n",
      "Epoch 137/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.2121 - val_loss: 0.2460\n",
      "Epoch 138/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.2108 - val_loss: 0.2452\n",
      "Epoch 139/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - loss: 0.2022 - val_loss: 0.2451\n",
      "Epoch 140/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.2100 - val_loss: 0.2434\n",
      "Epoch 141/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.2017 - val_loss: 0.2416\n",
      "Epoch 142/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - loss: 0.2059 - val_loss: 0.2407\n",
      "Epoch 143/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.1989 - val_loss: 0.2399\n",
      "Epoch 144/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.1994 - val_loss: 0.2387\n",
      "Epoch 145/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.2018 - val_loss: 0.2379\n",
      "Epoch 146/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.1977 - val_loss: 0.2367\n",
      "Epoch 147/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.1997 - val_loss: 0.2353\n",
      "Epoch 148/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 0.1895 - val_loss: 0.2334\n",
      "Epoch 149/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.1888 - val_loss: 0.2316\n",
      "Epoch 150/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.1969 - val_loss: 0.2308\n",
      "Epoch 151/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.1936 - val_loss: 0.2302\n",
      "Epoch 152/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 0.1877 - val_loss: 0.2295\n",
      "Epoch 153/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 0.1885 - val_loss: 0.2289\n",
      "Epoch 154/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.1852 - val_loss: 0.2281\n",
      "Epoch 155/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.1859 - val_loss: 0.2272\n",
      "Epoch 156/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.1862 - val_loss: 0.2261\n",
      "Epoch 157/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 0.1823 - val_loss: 0.2250\n",
      "Epoch 158/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.1810 - val_loss: 0.2237\n",
      "Epoch 159/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.1814 - val_loss: 0.2231\n",
      "Epoch 160/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.1797 - val_loss: 0.2224\n",
      "Epoch 161/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 0.1788 - val_loss: 0.2216\n",
      "Epoch 162/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 0.1808 - val_loss: 0.2206\n",
      "Epoch 163/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.1784 - val_loss: 0.2201\n",
      "Epoch 164/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 0.1771 - val_loss: 0.2191\n",
      "Epoch 165/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 0.1769 - val_loss: 0.2189\n",
      "Epoch 166/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.1722 - val_loss: 0.2179\n",
      "Epoch 167/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.1735 - val_loss: 0.2160\n",
      "Epoch 168/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.1704 - val_loss: 0.2152\n",
      "Epoch 169/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.1737 - val_loss: 0.2152\n",
      "Epoch 170/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 0.1724 - val_loss: 0.2147\n",
      "Epoch 171/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.1682 - val_loss: 0.2136\n",
      "Epoch 172/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - loss: 0.1679 - val_loss: 0.2130\n",
      "Epoch 173/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.1681 - val_loss: 0.2118\n",
      "Epoch 174/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - loss: 0.1675 - val_loss: 0.2112\n",
      "Epoch 175/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.1681 - val_loss: 0.2101\n",
      "Epoch 176/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 0.1645 - val_loss: 0.2089\n",
      "Epoch 177/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.1621 - val_loss: 0.2083\n",
      "Epoch 178/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.1627 - val_loss: 0.2079\n",
      "Epoch 179/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.1628 - val_loss: 0.2070\n",
      "Epoch 180/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.1619 - val_loss: 0.2064\n",
      "Epoch 181/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.1625 - val_loss: 0.2050\n",
      "Epoch 182/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.1585 - val_loss: 0.2049\n",
      "Epoch 183/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.1605 - val_loss: 0.2049\n",
      "Epoch 184/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.1608 - val_loss: 0.2041\n",
      "Epoch 185/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - loss: 0.1601 - val_loss: 0.2029\n",
      "Epoch 186/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - loss: 0.1576 - val_loss: 0.2020\n",
      "Epoch 187/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.1578 - val_loss: 0.2012\n",
      "Epoch 188/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 0.1581 - val_loss: 0.2009\n",
      "Epoch 189/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - loss: 0.1569 - val_loss: 0.2008\n",
      "Epoch 190/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.1540 - val_loss: 0.2008\n",
      "Epoch 191/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.1559 - val_loss: 0.1993\n",
      "Epoch 192/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.1546 - val_loss: 0.1983\n",
      "Epoch 193/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.1543 - val_loss: 0.1979\n",
      "Epoch 194/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.1530 - val_loss: 0.1972\n",
      "Epoch 195/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.1507 - val_loss: 0.1964\n",
      "Epoch 196/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.1524 - val_loss: 0.1958\n",
      "Epoch 197/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.1513 - val_loss: 0.1950\n",
      "Epoch 198/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.1499 - val_loss: 0.1944\n",
      "Epoch 199/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.1483 - val_loss: 0.1941\n",
      "Epoch 200/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - loss: 0.1494 - val_loss: 0.1937\n",
      "Final Training Loss: 0.15308767557144165\n",
      "Final Validation Loss: 0.19366386532783508\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Best hyperparameters found from tuning\n",
    "best_params = {\n",
    "    'units1': 32,\n",
    "    'units2': 32,\n",
    "    'recurrent_dropout': 0.2,\n",
    "    'optimizer': 'adam',\n",
    "    'loss_function': 'mean_squared_error',\n",
    "    'learning_rate_decay': 1e-06,\n",
    "    'learning_rate': 0.0005,\n",
    "    'l2_lambda': 0.01,\n",
    "    'epochs': 200,\n",
    "    'dropout_rate': 0.3,\n",
    "    'clipnorm': 5.0,\n",
    "    'batch_size': 64\n",
    "}\n",
    "\n",
    "# Build the model using the best parameters\n",
    "def build_best_model(params):\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units1'], return_sequences=True, \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "                   kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Second LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units2'], return_sequences=False, kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Select the optimizer\n",
    "    if params['optimizer'] == 'adam':\n",
    "        optimizer = Adam(learning_rate=params['learning_rate'], decay=params['learning_rate_decay'], clipnorm=params['clipnorm'])\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizer '{params['optimizer']}' is not implemented in this setup.\")\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate the model with the best hyperparameters\n",
    "model = build_best_model(best_params)\n",
    "\n",
    "# Early stopping to stop training when validation loss stops improving\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq, \n",
    "    epochs=best_params['epochs'], \n",
    "    batch_size=best_params['batch_size'], \n",
    "    validation_data=(X_test_seq, y_test_seq), \n",
    "    shuffle=False,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Print the final training and validation loss\n",
    "print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "45c7d2a4-393d-4d4b-9a52-f5113d5fe028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "LSTM Model Performance:\n",
      "Training RMSE: 0.01979999391524998\n",
      "Test RMSE: 0.02621229708819492\n",
      "Training MAE: 0.014390032860653771\n",
      "Test MAE: 0.020440061655561816\n",
      "Directional Accuracy on Training Data: 67.67922235722965%\n",
      "Directional Accuracy on Test Data: 57.84313725490197%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkcAAAHFCAYAAAD40125AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB1jUlEQVR4nO3deVhUZf8G8PvMwrDvuwLuIrihuOGepmJZpqWVuWWLppaapWSWVmZWlpqlr/1UUkvNXLLMXMp933AXlxBQQUSBYZ1hZs7vjwMjI4uIwAxwf67rXDlnnjnzPUdeud/nec5zBFEURRARERERAEBm7gKIiIiILAnDEREREVEBDEdEREREBTAcERERERXAcERERERUAMMRERERUQEMR0REREQFMBwRERERFcBwRERERFQAwxFRJYiMjIQgCBAEAbt37y70viiKaNCgAQRBQLdu3cr1uwVBwIwZMx75c9evX4cgCIiMjCxVu6+//rpsBVayixcvYsSIEfD394eVlRXc3d3Rt29fbN261dylFSn/56aobcSIEeYuD926dUPTpk3NXQZRuVKYuwCimsTBwQFLly4tFID27NmDa9euwcHBwTyF1RAbNmzAyy+/jHr16mH69Olo3Lgxbt++jeXLl6Nv375477338OWXX5q7zEKef/55vPvuu4X2e3h4mKEaouqP4YioEg0ePBg///wzvv/+ezg6Ohr3L126FB06dIBarTZjddXbtWvXMHToUDRr1gy7d++GnZ2d8b0XXngBY8aMwVdffYVWrVrhxRdfrLS6cnNzIQgCFIri/zn28vJC+/btK60mopqOw2pEleill14CAKxevdq4Ly0tDevXr8err75a5Gfu3buHt956C7Vq1YKVlRXq1auHadOmQaPRmLRTq9V4/fXX4ebmBnt7e/Tp0weXL18u8phXrlzByy+/DE9PT6hUKjRp0gTff/99OZ1l0eLi4vDKK6+YfOfcuXNhMBhM2i1atAgtWrSAvb09HBwcEBgYiA8++MD4flZWFiZPnoy6devC2toarq6uCA0NNbmmRfn222+RlZWF7777ziQY5Zs7dy6cnZ0xa9YsAMDp06chCAKWLl1aqO3WrVshCAI2b95s3Feaa7p7924IgoCVK1fi3XffRa1ataBSqXD16tWHX8CHGDFiBOzt7XH+/Hn06NEDdnZ28PDwwLhx45CVlWXSNicnBxEREahbty6srKxQq1YtjB07FqmpqYWO+8svv6BDhw6wt7eHvb09WrZsWeQ1OXbsGDp37gxbW1vUq1cPX3zxhcnfrcFgwGeffYbGjRvDxsYGzs7OaN68OebPn//Y505U3thzRFSJHB0d8fzzz2PZsmV48803AUhBSSaTYfDgwZg3b55J+5ycHHTv3h3Xrl3DzJkz0bx5c+zbtw+zZ89GVFQUtmzZAkCas9S/f38cPHgQH330Edq0aYMDBw4gPDy8UA0XLlxAWFgY/P39MXfuXHh7e2Pbtm14++23kZycjI8//rjcz/vOnTsICwuDVqvFp59+ijp16uDPP//E5MmTce3aNfzwww8AgDVr1uCtt97C+PHj8fXXX0Mmk+Hq1au4cOGC8ViTJk3CypUr8dlnnyEkJASZmZk4d+4c7t69W2INO3bsKLEHxtbWFr169cKvv/6KxMREtGjRAiEhIVi+fDlGjRpl0jYyMhKenp7o27cvgEe/phEREejQoQMWL14MmUwGT0/PEmsXRRE6na7QfrlcDkEQjK9zc3PRt29fvPnmm5g6dSoOHjyIzz77DLGxsfjjjz+Mx+rfvz/++ecfREREoHPnzjhz5gw+/vhjHDp0CIcOHYJKpQIAfPTRR/j0008xYMAAvPvuu3BycsK5c+cQGxtrUkdiYiKGDBmCd999Fx9//DE2btyIiIgI+Pr6YtiwYQCAL7/8EjNmzMCHH36ILl26IDc3F5cuXSoykBGZnUhEFW758uUiAPHYsWPirl27RADiuXPnRFEUxTZt2ogjRowQRVEUg4ODxa5duxo/t3jxYhGA+Ouvv5ocb86cOSIAcfv27aIoiuLWrVtFAOL8+fNN2s2aNUsEIH788cfGfb179xZr164tpqWlmbQdN26caG1tLd67d08URVGMiYkRAYjLly8v8dzy23311VfFtpk6daoIQDxy5IjJ/jFjxoiCIIjR0dHGGpydnUv8vqZNm4r9+/cvsU1RrK2txfbt25fYZsqUKSZ1LliwQARgrE8URfHevXuiSqUS3333XeO+0l7T/L/7Ll26lLpuAMVuK1euNLYbPnx4iT8D+/fvF0VRFP/++28RgPjll1+atFu7dq0IQFyyZIkoiqL433//iXK5XBwyZEiJ9XXt2rXIv9ugoCCxd+/extdPP/202LJly1KfN5E5cViNqJJ17doV9evXx7Jly3D27FkcO3as2CG1f//9F3Z2dnj++edN9uffpfTPP/8AAHbt2gUAGDJkiEm7l19+2eR1Tk4O/vnnHzz33HOwtbWFTqczbn379kVOTg4OHz5cHqdZ6DyCgoLQtm3bQuchiiL+/fdfAEDbtm2RmpqKl156Cb///juSk5MLHatt27bYunUrpk6dit27dyM7O7vc6hRFEQCMvTFDhgyBSqUyuWNv9erV0Gg0GDlyJICyXdOBAwc+Ul2DBg3CsWPHCm35PVcFFfczkP8zkn+tH7zT7YUXXoCdnZ3xZ2rHjh3Q6/UYO3bsQ+vz9vYu9HfbvHlzkx6mtm3b4vTp03jrrbewbds2zq8ji8ZwRFTJBEHAyJEjsWrVKixevBiNGjVC586di2x79+5deHt7mwydAICnpycUCoVxKOnu3btQKBRwc3Mzaeft7V3oeDqdDt999x2USqXJlv+LtqhA8rju3r0LHx+fQvt9fX2N7wPA0KFDsWzZMsTGxmLgwIHw9PREu3btsGPHDuNnFixYgClTpmDTpk3o3r07XF1d0b9/f1y5cqXEGvz9/RETE1Nim+vXrwMA/Pz8AACurq545plnsGLFCuj1egDSkFrbtm0RHBxsrP1Rr2lR16IkHh4eCA0NLbS5urqatCvpZ+DBn5UH73QTBAHe3t7Gdnfu3AEA1K5d+6H1PfidAKBSqUyCa0REBL7++mscPnwY4eHhcHNzQ48ePXD8+PGHHp+osjEcEZnBiBEjkJycjMWLFxt7IIri5uaG27dvG3s08iUlJUGn08Hd3d3YTqfTFZp3k5iYaPLaxcUFcrkcI0aMKLInorjeiMfl5uaGhISEQvtv3boFAMbzAICRI0fi4MGDSEtLw5YtWyCKIp5++mljL4SdnR1mzpyJS5cuITExEYsWLcLhw4fRr1+/Emt48skncfv27WJ7xrKysrBjxw40bdrUJFSOHDkSN2/exI4dO3DhwgUcO3bM5O+sLNf0wbBbXkr6GcgPMPk/K/nhJ58oikhMTDT+XeSHpxs3bpRLbQqFApMmTcLJkydx7949rF69GvHx8ejdu3ehCeNE5sZwRGQGtWrVwnvvvYd+/fph+PDhxbbr0aMHMjIysGnTJpP9K1asML4PAN27dwcA/PzzzybtfvnlF5PXtra26N69O06dOoXmzZsX2RtRVC/A4+rRowcuXLiAkydPFjoPQRCM9RdkZ2eH8PBwTJs2DVqtFufPny/UxsvLCyNGjMBLL72E6OjoEn/JTpw4ETY2Nhg/fjwyMzMLvT958mSkpKTgww8/NNnfq1cv1KpVC8uXL8fy5cthbW1tvOsQMN81LU5xPwP5a2vl/8ysWrXKpN369euRmZlpfL9Xr16Qy+VYtGhRudfo7OyM559/HmPHjsW9e/eMPXZEloJ3qxGZyRdffPHQNsOGDcP333+P4cOH4/r162jWrBn279+Pzz//HH379kXPnj0BSL/IunTpgvfffx+ZmZkIDQ3FgQMHsHLlykLHnD9/Pjp16oTOnTtjzJgxqFOnDtLT03H16lX88ccfxjkpj+rs2bP47bffCu1v06YNJk6ciBUrVuCpp57CJ598goCAAGzZsgU//PADxowZg0aNGgEAXn/9ddjY2KBjx47w8fFBYmIiZs+eDScnJ7Rp0wYA0K5dOzz99NNo3rw5XFxccPHiRaxcuRIdOnSAra1tsfXVr18fK1euxJAhQ9CmTRtMmjTJuAjksmXLsHXrVkyePBmDBw82+ZxcLsewYcPwzTffwNHREQMGDICTk1OlXNN8xfV4OTo6IigoyPjaysoKc+fORUZGBtq0aWO8Wy08PBydOnUCIPWg9e7dG1OmTIFarUbHjh2Nd6uFhIRg6NChAIA6derggw8+wKeffors7Gy89NJLcHJywoULF5CcnIyZM2c+0jn069cPTZs2RWhoKDw8PBAbG4t58+YhICAADRs2fIyrQ1QBzDodnKiGKHi3WkkevFtNFEXx7t274ujRo0UfHx9RoVCIAQEBYkREhJiTk2PSLjU1VXz11VdFZ2dn0dbWVnzyySfFS5cuFbpbTRSlO8xeffVVsVatWqJSqRQ9PDzEsLAw8bPPPjNpg0e4W624Lf/zsbGx4ssvvyy6ubmJSqVSbNy4sfjVV1+Jer3eeKyffvpJ7N69u+jl5SVaWVmJvr6+4qBBg8QzZ84Y20ydOlUMDQ0VXVxcRJVKJdarV0+cOHGimJycXGKd+c6fPy8OHz5crF27tqhUKkVXV1exT58+4pYtW4r9zOXLl43ns2PHjmKvw8Ouaf7dauvWrStVraJY8t1qHTt2NLYbPny4aGdnJ545c0bs1q2baGNjI7q6uopjxowRMzIyTI6ZnZ0tTpkyRQwICBCVSqXo4+MjjhkzRkxJSSn0/StWrBDbtGkjWltbi/b29mJISIjJz0TXrl3F4ODgQp8bPny4GBAQYHw9d+5cMSwsTHR3dxetrKxEf39/cdSoUeL169dLfS2IKosgig9MZiAioipnxIgR+O2335CRkWHuUoiqPM45IiIiIiqA4YiIiIioAA6rERERERXAniMiIiKiAhiOiIiIiApgOCIiIiIqgItAFsFgMODWrVtwcHCosGX+iYiIqHyJooj09HT4+vpCJit7/w/DURFu3bplfPAkERERVS3x8fGlemhycRiOiuDg4ABAuriOjo5mroaIiIhKQ61Ww8/Pz/h7vKwYjoqQP5Tm6OjIcERERFTFPO6UGE7IJiIiIiqA4YiIiIioAIYjIiIiogLMOudo9uzZ2LBhAy5dugQbGxuEhYVhzpw5aNy4cYmf27NnDyZNmoTz58/D19cX77//PkaPHm3SZv369Zg+fTquXbuG+vXrY9asWXjuuecq8nSIiKgAvV6P3Nxcc5dB1YyVldVj3aZfGmYNR3v27MHYsWPRpk0b6HQ6TJs2Db169cKFCxdgZ2dX5GdiYmLQt29fvP7661i1ahUOHDiAt956Cx4eHhg4cCAA4NChQxg8eDA+/fRTPPfcc9i4cSMGDRqE/fv3o127dpV5ikRENY4oikhMTERqaqq5S6FqSCaToW7durCysqqw77CoB8/euXMHnp6e2LNnD7p06VJkmylTpmDz5s24ePGicd/o0aNx+vRpHDp0CAAwePBgqNVqbN261dimT58+cHFxwerVqx9ah1qthpOTE9LS0ni3GhHRI0pISEBqaio8PT1ha2vLxXSp3OQv0qxUKuHv71/oZ6u8fn9b1K38aWlpAABXV9di2xw6dAi9evUy2de7d28sXboUubm5UCqVOHToECZOnFiozbx584o8pkajgUajMb5Wq9VlPAMioppNr9cbg5Gbm5u5y6FqyMPDA7du3YJOp4NSqayQ77CYCdmiKGLSpEno1KkTmjZtWmy7xMREeHl5mezz8vKCTqdDcnJyiW0SExOLPObs2bPh5ORk3Lg6NhFR2eTPMbK1tTVzJVRd5Q+n6fX6CvsOiwlH48aNw5kzZ0o17PVgN1r+yGDB/UW1Ka5rNyIiAmlpacYtPj7+UcsnIqICOJRGFaUyfrYsYlht/Pjx2Lx5M/bu3fvQZ6F4e3sX6gFKSkqCQqEwduEW1+bB3qR8KpUKKpXqMc6AiIiIqguz9hyJoohx48Zhw4YN+Pfff1G3bt2HfqZDhw7YsWOHyb7t27cjNDTUOPZYXJuwsLDyK56IiKgE3bp1w4QJE0rd/vr16xAEAVFRURVWE5WOWcPR2LFjsWrVKvzyyy9wcHBAYmIiEhMTkZ2dbWwTERGBYcOGGV+PHj0asbGxmDRpEi5evIhly5Zh6dKlmDx5srHNO++8g+3bt2POnDm4dOkS5syZg507dz7SDykREdUMgiCUuI0YMaJMx92wYQM+/fTTUrf38/NDQkJCifNuywND2MOZdVht0aJFAKR0XdDy5cuNP4wJCQmIi4szvle3bl389ddfmDhxIr7//nv4+vpiwYIFxjWOACAsLAxr1qzBhx9+iOnTp6N+/fpYu3at2dc40uoMuJupgd4gorYLJysSEVmChIQE45/Xrl2Ljz76CNHR0cZ9NjY2Ju3z74x+mJLuvC6KXC6Ht7f3I32GKobZh9WK2gqm9MjISOzevdvkc127dsXJkyeh0WgQExNTaHVsAHj++edx6dIlaLVaXLx4EQMGDKjgs3m4qPhUdJj9L4YtO2ruUoiIKI+3t7dxc3JygiAIxtc5OTlwdnbGr7/+im7dusHa2hqrVq3C3bt38dJLL6F27dqwtbVFs2bNCt1Q9OCwWp06dfD555/j1VdfhYODA/z9/bFkyRLj+w/26OzevRuCIOCff/5BaGgobG1tERYWZhLcAOCzzz6Dp6cnHBwc8Nprr2Hq1Klo2bJlma+HRqPB22+/DU9PT1hbW6NTp044duyY8f2UlBQMGTIEHh4esLGxQcOGDbF8+XIAgFarxbhx4+Dj4wNra2vUqVMHs2fPLnMt5mIxd6vVBLZWcgBAtrbibj8kIrIkoigiS6szy1aeaxxPmTIFb7/9Ni5evIjevXsjJycHrVu3xp9//olz587hjTfewNChQ3HkyJESjzN37lyEhobi1KlTeOuttzBmzBhcunSpxM9MmzYNc+fOxfHjx6FQKPDqq68a3/v5558xa9YszJkzBydOnIC/v79xVKas3n//faxfvx4//fQTTp48iQYNGqB37964d+8eAGD69Om4cOECtm7diosXL2LRokVwd3cHACxYsACbN2/Gr7/+iujoaKxatQp16tR5rHrMwSLuVqsprJV54SiX4YiIaobsXD2CPtpmlu++8Elv2FqVz6+5CRMmFBqBKDjXdfz48fj777+xbt26Eqdw9O3bF2+99RYAKXB9++232L17NwIDA4v9zKxZs9C1a1cAwNSpU/HUU08hJycH1tbW+O677zBq1CiMHDkSAPDRRx9h+/btyMjIKNN5ZmZmYtGiRYiMjER4eDgA4Mcff8SOHTuwdOlSvPfee4iLi0NISAhCQ0MBwCT8xMXFoWHDhujUqRMEQUBAQECZ6jA39hxVIhv2HBERVUn5QSCfXq/HrFmz0Lx5c7i5ucHe3h7bt283mSNblObNmxv/nD98l5SUVOrP+Pj4AIDxM9HR0Wjbtq1J+wdfP4pr164hNzcXHTt2NO5TKpVo27at8bFdY8aMwZo1a9CyZUu8//77OHjwoLHtiBEjEBUVhcaNG+Ptt9/G9u3by1yLObHnqBLZ5PUcaXQG6A0i5DIukkZE1ZuNUo4Ln/Q223eXlwcfhj537lx8++23mDdvHpo1awY7OztMmDABWq22xOM8OJFbEAQYDIZSfyZ/AcSCnyluYeSyKGpR5fz9+fvCw8MRGxuLLVu2YOfOnejRowfGjh2Lr7/+Gq1atUJMTAy2bt2KnTt3YtCgQejZsyd+++23MtdkDuw5qkT5c44AIIdDa0RUAwiCAFsrhVm2ilxJed++fXj22WfxyiuvoEWLFqhXrx6uXLlSYd9XnMaNG+PoUdObfI4fP17m4zVo0ABWVlbYv3+/cV9ubi6OHz+OJk2aGPd5eHhgxIgRWLVqFebNm2cysdzR0RGDBw/Gjz/+iLVr12L9+vXG+UpVBXuOKpFKcT+LZufqYafi5SciqooaNGiA9evX4+DBg3BxccE333yDxMREkwBRGcaPH4/XX38doaGhCAsLw9q1a3HmzBnUq1fvoZ998K43AAgKCsKYMWPw3nvvwdXVFf7+/vjyyy+RlZWFUaNGAZDmNbVu3RrBwcHQaDT4888/jef97bffwsfHBy1btoRMJsO6devg7e0NZ2fncj3visbfzpVIEATYKOXIztVz3hERURU2ffp0xMTEoHfv3rC1tcUbb7yB/v37Iy0trVLrGDJkCP777z9MnjwZOTk5GDRoEEaMGFGoN6koL774YqF9MTEx+OKLL2AwGDB06FCkp6cjNDQU27Ztg4uLCwDpwa8RERG4fv06bGxs0LlzZ6xZswYAYG9vjzlz5uDKlSuQy+Vo06YN/vrrL8hkVWugShDL817HakKtVsPJyQlpaWlwdHQs12O3/nQH7mZqsX1iFzTycijXYxMRmVtOTg5iYmJQt25dWFtbm7ucGunJJ5+Et7c3Vq5cae5SKkRJP2Pl9fubPUeVzHg7P3uOiIjoMWVlZWHx4sXo3bs35HI5Vq9ejZ07dxZ6vig9GoajSma8nZ8TsomI6DEJgoC//voLn332GTQaDRo3boz169ejZ8+e5i6tSmM4qmQ27DkiIqJyYmNjg507d5q7jGqnas2QqgbYc0RERGTZGI4qGXuOiIiILBvDUSXLD0dZ7DkiIiKySAxHlSx/lewc9hwRERFZJIajSmbNOUdEREQWjeGokhnnHDEcERERWSSGo0rGCdlERNVTt27dMGHCBOPrOnXqYN68eSV+RhAEbNq06bG/u7yOQxKGo0pmvJWf4YiIyCL069ev2EUTDx06BEEQcPLkyUc+7rFjx/DGG288bnkmZsyYgZYtWxban5CQgPDw8HL9rgdFRkZWuQfIlhXDUSXjsBoRkWUZNWoU/v33X8TGxhZ6b9myZWjZsiVatWr1yMf18PCAra1teZT4UN7e3lCpVJXyXTUBw1Ely+85ymLPERGRRXj66afh6emJyMhIk/1ZWVlYu3YtRo0ahbt37+Kll15C7dq1YWtri2bNmmH16tUlHvfBYbUrV66gS5cusLa2RlBQUJHPP5syZQoaNWoEW1tb1KtXD9OnT0dubi4Aqedm5syZOH36NARBgCAIxpofHFY7e/YsnnjiCdjY2MDNzQ1vvPEGMjIyjO+PGDEC/fv3x9dffw0fHx+4ublh7Nixxu8qi7i4ODz77LOwt7eHo6MjBg0ahNu3bxvfP336NLp37w4HBwc4OjqidevWOH78OAAgNjYW/fr1g4uLC+zs7BAcHIy//vqrzLU8Lj4+pJLl9xzlsOeIiGoCUQRys8zz3UpbQBAe2kyhUGDYsGGIjIzERx99BCHvM+vWrYNWq8WQIUOQlZWF1q1bY8qUKXB0dMSWLVswdOhQ1KtXD+3atXvodxgMBgwYMADu7u44fPgw1Gq1yfykfA4ODoiMjISvry/Onj2L119/HQ4ODnj//fcxePBgnDt3Dn///bfxkSFOTk6FjpGVlYU+ffqgffv2OHbsGJKSkvDaa69h3LhxJgFw165d8PHxwa5du3D16lUMHjwYLVu2xOuvv/7Q83mQKIro378/7OzssGfPHuh0Orz11lsYPHgwdu/eDQAYMmQIQkJCsGjRIsjlckRFRUGpVAIAxo4dC61Wi71798LOzg4XLlyAvb39I9dRXhiOKhkfH0JENUpuFvC5r3m++4NbgJVdqZq++uqr+Oqrr7B79250794dgDSkNmDAALi4uMDFxQWTJ082th8/fjz+/vtvrFu3rlThaOfOnbh48SKuX7+O2rVrAwA+//zzQvOEPvzwQ+Of69Spg3fffRdr167F+++/DxsbG9jb20OhUMDb27vY7/r555+RnZ2NFStWwM5OOv+FCxeiX79+mDNnDry8vAAALi4uWLhwIeRyOQIDA/HUU0/hn3/+KVM42rlzJ86cOYOYmBj4+fkBAFauXIng4GAcO3YMbdq0QVxcHN577z0EBgYCABo2bGj8fFxcHAYOHIhmzZoBAOrVq/fINZQnDqtVMuMK2RxWIyKyGIGBgQgLC8OyZcsAANeuXcO+ffvw6quvAgD0ej1mzZqF5s2bw83NDfb29ti+fTvi4uJKdfyLFy/C39/fGIwAoEOHDoXa/fbbb+jUqRO8vb1hb2+P6dOnl/o7Cn5XixYtjMEIADp27AiDwYDo6GjjvuDgYMjlcuNrHx8fJCUlPdJ3FfxOPz8/YzACgKCgIDg7O+PixYsAgEmTJuG1115Dz5498cUXX+DatWvGtm+//TY+++wzdOzYER9//DHOnDlTpjrKC3uOKll+zxGH1YioRlDaSj045vruRzBq1CiMGzcO33//PZYvX46AgAD06NEDADB37lx8++23mDdvHpo1awY7OztMmDABWq22VMcWRbHQPuGBIb/Dhw/jxRdfxMyZM9G7d284OTlhzZo1mDt37iOdhyiKhY5d1HfmD2kVfM9gMDzSdz3sOwvunzFjBl5++WVs2bIFW7duxccff4w1a9bgueeew2uvvYbevXtjy5Yt2L59O2bPno25c+di/PjxZarncbHnqJJxnSMiqlEEQRraMsdWivlGBQ0aNAhyuRy//PILfvrpJ4wcOdL4i33fvn149tln8corr6BFixaoV68erly5UupjBwUFIS4uDrdu3Q+Khw4dMmlz4MABBAQEYNq0aQgNDUXDhg0L3UFnZWUFvb7k3x9BQUGIiopCZmamybFlMhkaNWpU6pofRf75xcfHG/dduHABaWlpaNKkiXFfo0aNMHHiRGzfvh0DBgzA8uXLje/5+flh9OjR2LBhA9599138+OOPFVJraTAcVTLOOSIiskz29vYYPHgwPvjgA9y6dQsjRowwvtegQQPs2LEDBw8exMWLF/Hmm28iMTGx1Mfu2bMnGjdujGHDhuH06dPYt28fpk2bZtKmQYMGiIuLw5o1a3Dt2jUsWLAAGzduNGlTp04dxMTEICoqCsnJydBoNIW+a8iQIbC2tsbw4cNx7tw57Nq1C+PHj8fQoUON843KSq/XIyoqymS7cOECevbsiebNm2PIkCE4efIkjh49imHDhqFr164IDQ1FdnY2xo0bh927dyM2NhYHDhzAsWPHjMFpwoQJ2LZtG2JiYnDy5En8+++/JqGqsjEcVTL2HBERWa5Ro0YhJSUFPXv2hL+/v3H/9OnT0apVK/Tu3RvdunWDt7c3+vfvX+rjymQybNy4ERqNBm3btsVrr72GWbNmmbR59tlnMXHiRIwbNw4tW7bEwYMHMX36dJM2AwcORJ8+fdC9e3d4eHgUuZyAra0ttm3bhnv37qFNmzZ4/vnn0aNHDyxcuPDRLkYRMjIyEBISYrL17dvXuJSAi4sLunTpgp49e6JevXpYu3YtAEAul+Pu3bsYNmwYGjVqhEGDBiE8PBwzZ84EIIWusWPHokmTJujTpw8aN26MH3744bHrLStBLGogtIZTq9VwcnJCWloaHB0dy/XYKZlahHwqrW1xdVY4FHLmUyKqPnJychATE4O6devC2tra3OVQNVTSz1h5/f7mb+ZKlj+sBgA5urJNfCMiIqKKw3BUyVQKmXGOYJZWZ95iiIiIqBCGo0omCML9VbK17DkiIiKyNAxHZmDLO9aIiIgsFsORGVgrGY6IqHrjvT5UUSrjZ4vhyAzuP0KEc46IqHrJX3U5K8tMD5ulai9/VfKCjz4pb2Z9fMjevXvx1Vdf4cSJE0hISMDGjRtLXDdixIgR+OmnnwrtDwoKwvnz5wEAkZGRGDlyZKE22dnZFnNbKR8hQkTVlVwuh7Ozs/EZXba2tsU+yoLoURkMBty5cwe2trZQKCouwpg1HGVmZqJFixYYOXIkBg4c+ND28+fPxxdffGF8rdPp0KJFC7zwwgsm7RwdHU0ergfAYoIRUHAhSE7IJqLqJ/+J8WV9iClRSWQyGfz9/Ss0dJs1HIWHhyM8PLzU7Z2cnODk5GR8vWnTJqSkpBTqKRIEwfg/TkuU33PEYTUiqo4EQYCPjw88PT2Rm5tr7nKomrGysoJMVrGzgswajh7X0qVL0bNnTwQEBJjsz8jIQEBAAPR6PVq2bIlPP/0UISEhxR5Ho9GYPJ9GrVZXWM3A/Z4jDqsRUXUml8srdF4IUUWpshOyExISsHXrVrz22msm+wMDAxEZGYnNmzdj9erVsLa2RseOHUt8evLs2bONvVJOTk7w8/Or0Nr58FkiIiLLVWXDUWRkJJydnQtN4G7fvj1eeeUVtGjRAp07d8avv/6KRo0a4bvvviv2WBEREUhLSzNu8fHxFVr7/bvVGI6IiIgsTZUcVhNFEcuWLcPQoUNhZWVVYluZTIY2bdqU2HOkUqmgUqnKu8xi2XCdIyIiIotVJXuO9uzZg6tXr2LUqFEPbSuKIqKiouDj41MJlZWO8VZ+9hwRERFZHLP2HGVkZODq1avG1zExMYiKioKrqyv8/f0RERGBmzdvYsWKFSafW7p0Kdq1a4emTZsWOubMmTPRvn17NGzYEGq1GgsWLEBUVBS+//77Cj+f0uKcIyIiIstl1nB0/PhxdO/e3fh60qRJAIDhw4cjMjISCQkJiIuLM/lMWloa1q9fj/nz5xd5zNTUVLzxxhtITEyEk5MTQkJCsHfvXrRt27biTuQRcc4RERGR5RJEPgCnELVaDScnJ6SlpcHR0bHcj7/maBymbjiLnk088X/D25T78YmIiGqi8vr9XSXnHFV1HFYjIiKyXAxHZsBhNSIiIsvFcGQGxp4jhiMiIiKLw3BkBnx8CBERkeViODIDzjkiIiKyXAxHZsA5R0RERJaL4cgMjCtks+eIiIjI4jAcmUF+z1GuXkSu3mDmaoiIiKgghiMzyO85AjjviIiIyNIwHJmBlVwGmSD9mQ+fJSIisiwMR2YgCIJxaI09R0RERJaF4chMbKykZ/7yjjUiIiLLwnBkJjZW0qVnzxEREZFlYTgyE+Mq2ew5IiIisigMR2bCOUdERESWieHITPJv5+ecIyIiIsvCcGQm7DkiIiKyTAxHZsJHiBAREVkmhiMzsVHyVn4iIiJLxHBkJsZb+RmOiIiILArDkZkYb+XnsBoREZFFYTgyk/xwxGE1IiIiy8JwZCb5jw/h3WpERESWheHITGyUfHwIERGRJWI4MhPjrfwcViMiIrIoDEdmYs05R0RERBaJ4chMbDnniIiIyCIxHJkJb+UnIiKyTAxHZpK/CCSH1YiIiCwLw5GZ5D8+hMNqREREloXhyEx4txoREZFlYjgyk/w5R+w5IiIisiwMR2aSH450BhFancHM1RAREVE+hiMzyR9WA9h7REREZEkYjsxEKRcglwkAeDs/ERGRJTFrONq7dy/69esHX19fCIKATZs2ldh+9+7dEASh0Hbp0iWTduvXr0dQUBBUKhWCgoKwcePGCjyLshEEwTi0xtv5iYiILIdZw1FmZiZatGiBhQsXPtLnoqOjkZCQYNwaNmxofO/QoUMYPHgwhg4ditOnT2Po0KEYNGgQjhw5Ut7lP7b8obUsrc7MlRAREVE+hTm/PDw8HOHh4Y/8OU9PTzg7Oxf53rx58/Dkk08iIiICABAREYE9e/Zg3rx5WL169eOUW+7srOS4A/YcERERWZIqOecoJCQEPj4+6NGjB3bt2mXy3qFDh9CrVy+Tfb1798bBgwcrs8RScbRRAgDU2blmroSIiIjymbXn6FH5+PhgyZIlaN26NTQaDVauXIkePXpg9+7d6NKlCwAgMTERXl5eJp/z8vJCYmJiscfVaDTQaDTG12q1umJO4AEO1tLlT8/hsBoREZGlqFLhqHHjxmjcuLHxdYcOHRAfH4+vv/7aGI4AabJzQaIoFtpX0OzZszFz5szyL/ghHK3zeo5y2HNERERkKarksFpB7du3x5UrV4yvvb29C/USJSUlFepNKigiIgJpaWnGLT4+vsLqLYg9R0RERJanyoejU6dOwcfHx/i6Q4cO2LFjh0mb7du3IywsrNhjqFQqODo6mmyVwdhzxDlHREREFsOsw2oZGRm4evWq8XVMTAyioqLg6uoKf39/RERE4ObNm1ixYgUA6U60OnXqIDg4GFqtFqtWrcL69euxfv164zHeeecddOnSBXPmzMGzzz6L33//HTt37sT+/fsr/fwexoHDakRERBbHrOHo+PHj6N69u/H1pEmTAADDhw9HZGQkEhISEBcXZ3xfq9Vi8uTJuHnzJmxsbBAcHIwtW7agb9++xjZhYWFYs2YNPvzwQ0yfPh3169fH2rVr0a5du8o7sVJytJEuv5rDakRERBZDEEVRNHcRlkatVsPJyQlpaWkVOsS2/sQNvLvuNDo3dMfKUZYX3oiIiKqS8vr9XeXnHFVlnJBNRERkeRiOzMi4CCTnHBEREVkMhiMzYs8RERGR5WE4MiPeyk9ERGR5GI7MKD8caXQGaHR8+CwREZElYDgyI3vr+yspcGiNiIjIMjAcmZFcJsBBlbfWEYfWiIiILALDkZlxUjYREZFlYTgyM97OT0REZFkYjsyMPUdERESWheHIzHg7PxERkWVhODIz9hwRERFZFoYjM+OcIyIiIsvCcGRmHFYjIiKyLAxHZsZhNSIiIsvCcGRmHFYjIiKyLAxHZpbfc6RmzxEREZFFYDgyM845IiIisiwMR2bGOUdERESWheHIzDjniIiIyLIwHJlZ/rBahkYHg0E0czVERETEcGRm+cNqoghkaDm0RkREZG4MR2ZmrZTDSiH9NXBSNhERkfkxHFkAR07KJiIishgMRxaAt/MTERFZDoYjC8Db+YmIiCwHw5EF4O38REREloPhyALkD6ux54iIiMj8GI4sgPH5apxzREREZHYMRxaAw2pERESWg+HIAjioOCGbiIjIUjAcWQD2HBEREVkOhiMLwFv5iYiILAfDkQXgIpBERESWg+HIArDniIiIyHIwHFkAzjkiIiKyHGYNR3v37kW/fv3g6+sLQRCwadOmEttv2LABTz75JDw8PODo6IgOHTpg27ZtJm0iIyMhCEKhLScnpwLP5PHcD0fsOSIiIjI3s4ajzMxMtGjRAgsXLixV+7179+LJJ5/EX3/9hRMnTqB79+7o168fTp06ZdLO0dERCQkJJpu1tXVFnEK5yB9W0+oMyMnVm7kaIiKimk1hzi8PDw9HeHh4qdvPmzfP5PXnn3+O33//HX/88QdCQkKM+wVBgLe3d3mVWeHsrRQQBEAUpaE1a6Xc3CURERHVWFV6zpHBYEB6ejpcXV1N9mdkZCAgIAC1a9fG008/Xahn6UEajQZqtdpkq0wymQB7LgRJRERkEap0OJo7dy4yMzMxaNAg477AwEBERkZi8+bNWL16NaytrdGxY0dcuXKl2OPMnj0bTk5Oxs3Pz68yyjdR0u386Tm5iL2bWdklERER1UhVNhytXr0aM2bMwNq1a+Hp6Wnc3759e7zyyito0aIFOnfujF9//RWNGjXCd999V+yxIiIikJaWZtzi4+Mr4xRMlHQ7/6ifjqP717sRdzersssiIiKqccw656is1q5di1GjRmHdunXo2bNniW1lMhnatGlTYs+RSqWCSqUq7zIfSXG38+fk6nEiNgUGETh/Kw3+brbmKI+IiKjGqHI9R6tXr8aIESPwyy+/4Kmnnnpoe1EUERUVBR8fn0qoruzyh9Ue7Dm6mpQBvUEEANxIya70uoiIiGoas/YcZWRk4OrVq8bXMTExiIqKgqurK/z9/REREYGbN29ixYoVAKRgNGzYMMyfPx/t27dHYmIiAMDGxgZOTk4AgJkzZ6J9+/Zo2LAh1Go1FixYgKioKHz//feVf4KPwDFvWO3BOUcXEu5PDr+RwmE1IiKiimbWnqPjx48jJCTEeBv+pEmTEBISgo8++ggAkJCQgLi4OGP7//3vf9DpdBg7dix8fHyM2zvvvGNsk5qaijfeeANNmjRBr169cPPmTezduxdt27at3JN7RPnDamkPhKOLJuGIPUdEREQVzaw9R926dYMoisW+HxkZafJ69+7dDz3mt99+i2+//fYxK6t8tV1sAADRiekm+xmOiIiIKleVm3NUXYXWkdZqOh6bAkPeHCNRFHEx4X5YupGSVWKYJCIiosfHcGQhgn0dYaOUIy07F5eTpECUkJaDtOxcyGUCACBTq0dqFh9OS0REVJEYjiyEUi5DqwBnAMCxmHsA7g+pNfCwh4eDtNQAh9aIiIgqFsORBWmTN7R29HoKgPvhqImPg3FOEu9YIyIiqlgMRxakbV0pHB2LuWcy36iJjyNqu0iLP7LniIiIqGJVyRWyq6sQPxco5QIS1Tm4kZJdoOfIEal5t/iz54iIiKhisefIgthYydG0lrSY5e7LdxCT97BZqecof1iNPUdEREQVieHIwrTNm3f08+FYiCLgbq+Ch4OKw2pERESVhOHIwuTPO7qUmD/fyAEATCZkc60jIiKiisNwZGFCA1whCPdfB/k4AgBqOUvhKFOrL/SIESIiIio/DEcWxslWicZeDsbXTfLCkbVSzrWOiIiIKgHDkQXKX+8IuB+OAHCtIyIiokrAcGSB2uTNO7KSy1DPw864n5OyiYiIKh7XObJAXRt5INDbAaF1XKCU38+vvJ2fiIio4jEcWSAnGyX+ntCl0H4OqxEREVU8DqtVIRxWIyIiqngMR1VIwWE1rnVERERUMcoUjuLj43Hjxg3j66NHj2LChAlYsmRJuRVGheWvdZSh0XGtIyIiogpSpnD08ssvY9euXQCAxMREPPnkkzh69Cg++OADfPLJJ+VaIN3HtY6IiIgqXpnC0blz59C2bVsAwK+//oqmTZvi4MGD+OWXXxAZGVme9dEDOCmbiIioYpUpHOXm5kKlknowdu7ciWeeeQYAEBgYiISEhPKrjgrhpGwiIqKKVaZwFBwcjMWLF2Pfvn3YsWMH+vTpAwC4desW3NzcyrVAMpXfc3T9bqaZKyEiIqqeyhSO5syZg//973/o1q0bXnrpJbRo0QIAsHnzZuNwG1WMhp72AIArtzPMXAkREVH1VKZFILt164bk5GSo1Wq4uLgY97/xxhuwtbUtt+KosEZ5D6W9fDsdoihCEAQzV0RERFS9lKnnKDs7GxqNxhiMYmNjMW/ePERHR8PT07NcCyRTDTztIQhASlYu7mRozF0OERFRtVOmcPTss89ixYoVAIDU1FS0a9cOc+fORf/+/bFo0aJyLZBMWSvlqOMmPYz2ciKH1oiIiMpbmcLRyZMn0blzZwDAb7/9Bi8vL8TGxmLFihVYsGBBuRZIhTXykuYdXb6dbuZKiIiIqp8yhaOsrCw4OEhzX7Zv344BAwZAJpOhffv2iI2NLdcCqbDGBeYdERERUfkqUzhq0KABNm3ahPj4eGzbtg29evUCACQlJcHR0bFcC6TCGnlL4Sia4YiIiKjclSkcffTRR5g8eTLq1KmDtm3bokOHDgCkXqSQkJByLZAKM/YcJabzAbRERETlrEy38j///PPo1KkTEhISjGscAUCPHj3w3HPPlVtxVLQ67nZQygVkavW4mZptXDWbiIiIHl+ZwhEAeHt7w9vbGzdu3IAgCKhVqxYXgKwkSrkM9dztEX07HZdvpzMcERERlaMyDasZDAZ88skncHJyQkBAAPz9/eHs7IxPP/0UBoOhvGukIuTPO7rMlbKJiIjKVZl6jqZNm4alS5fiiy++QMeOHSGKIg4cOIAZM2YgJycHs2bNKu866QGNvezxB6R5R0RERFR+yhSOfvrpJ/zf//0fnnnmGeO+Fi1aoFatWnjrrbcYjipB/mNEeMcaERFR+SrTsNq9e/cQGBhYaH9gYCDu3btX6uPs3bsX/fr1g6+vLwRBwKZNmx76mT179qB169awtrZGvXr1sHjx4kJt1q9fj6CgIKhUKgQFBWHjxo2lrqmqaJw3rHYlKQN6A+9YIyIiKi9lCkctWrTAwoULC+1fuHAhmjdvXurjZGZmFnusosTExKBv377o3LkzTp06hQ8++ABvv/021q9fb2xz6NAhDB48GEOHDsXp06cxdOhQDBo0CEeOHCl1XVWBn4strJUyaHUGxN7NNHc5RERE1YYglmGhnD179uCpp56Cv78/OnToAEEQcPDgQcTHx+Ovv/4yPlrkkQoRBGzcuBH9+/cvts2UKVOwefNmXLx40bhv9OjROH36NA4dOgQAGDx4MNRqNbZu3Wps06dPH7i4uGD16tWlqkWtVsPJyQlpaWkWvahlv+/24+zNNCx+pRX6NPUxdzlERERmVV6/v8vUc9S1a1dcvnwZzz33HFJTU3Hv3j0MGDAA58+fx/Lly8tczMMcOnTIuBp3vt69e+P48ePIzc0tsc3BgweLPa5Go4FarTbZqoJGXrxjjYiIqLyVeZ0jX1/fQhOvT58+jZ9++gnLli177MKKkpiYCC8vL5N9Xl5e0Ol0SE5Oho+PT7FtEhMTiz3u7NmzMXPmzAqpuSI19pYeQMtJ2UREROWnTD1H5iQIgsnr/FHBgvuLavPgvoIiIiKQlpZm3OLj48ux4oqT33N0MaFq9HQRERFVBWXuOTIHb2/vQj1ASUlJUCgUcHNzK7HNg71JBalUKqhUqvIvuIK1qO0MpVzAf3cycSlRjUBvy50fRUREVFVUqZ6jDh06YMeOHSb7tm/fjtDQUCiVyhLbhIWFVVqdlcXFzgo9m0ihb+2xqtHbRUREZOkeqedowIABJb6fmpr6SF+ekZGBq1evGl/HxMQgKioKrq6u8Pf3R0REBG7evIkVK1YAkO5MW7hwISZNmoTXX38dhw4dwtKlS03uQnvnnXfQpUsXzJkzB88++yx+//137Ny5E/v373+k2qqKQW38sPVcIjaeuomp4YFQKeTmLomIiKhKe6Rw5OTk9ND3hw0bVurjHT9+HN27dze+njRpEgBg+PDhiIyMREJCAuLi4ozv161bF3/99RcmTpyI77//Hr6+vliwYAEGDhxobBMWFoY1a9bgww8/xPTp01G/fn2sXbsW7dq1K3VdVUmXhh7wcbJGQloOtp+/jX4tfM1dEhERUZVWpnWOqruqss5Rvm+2R2PBv1fRqYE7Vr1WPUMgERHRw5h1nSOyLC+E+gEA9l9NRvy9LDNXQ0REVLUxHFUDfq626NTAHQCw7jgnZhMRET0OhqNqYlAbqfdo3YkbfBAtERHRY2A4qiZ6BXnB2VaJhLQc7LmcZO5yiIiIqiyGo2rCWinHwFa1AQCrDseZvJeWnYunv9uHsb+cBOffExERlYzhqBp5uZ0/AGBXdBJupNyfmL10fwzO3VRjy5kEbDx101zlERERVQkMR9VIfQ97hNV3gygCq49KvUdpWblYvj/G2Obzvy5BnZNrrhKJiIgsHsNRNfNK+wAA0uNEtDoDlh6IQbpGh0Ze9qjnYYfkDA2+3XHZzFUSERFZLoajaubJIC94OqiQnKHFr8fjjb1GE3o2wsxnggEAPx28jgu31OYsk4iIyGIxHFUzSrkML+bd1j9j83mka3Ro7OWAPsHe6NzQA32becMgAh9vPsfJ2UREREVgOKqGXmzrD5kA6PLWO3qnZ0PIZAIA4MOngmCjlOPY9RQcunbXnGUSERFZJIajasjX2QY9mngBgLHXqOB7fZv5AAD2Xkk2S31ERESWTGHuAqhiTA0PhCiKGPfE/V6jfJ0aumH9yRs4eI3hiIiI6EEMR9VUfQ97/N/wNkW+F1Zfeg7b2ZtpSM3SwtnWqjJLIyIismgcVquBvByt0dDTHqKIEucd5eTqcVudU4mVERERmR/DUQ3VsYHUe3SghKG1d389jU5z/kV0YnpllUVERGR2DEc1lDEcXS265ygnV48dF28jVy9iVzQfZEtERDUHw1EN1a6eK+QyATHJmbiZml3o/VNxqdDqDHl/Tqns8oiIiMyG4aiGcrRWokVtJwDAgauFh9aOxNzvUToZl8oFI4mIqMZgOKrB7g+tFQ5Hh/+7H47upGtwK40Ts4mIqGZgOKrBCs47KtgzlJOrx6m4VACAm510mz+H1oiIqKZgOKrBQvydYaOUIzlDg8u3M4z7T8enQqMzwN1ehaeaS6tp54clIiKi6o7hqAZTKeRoU9cVALDvyh3j/iMx9wBIk7Zb+bsAAE6y54iIiGoIhqMarmsjDwDAikOxxrvT8idjt6/nhhB/ZwDA+ZtqaHR6s9RIRERUmRiOargX2/jB3V6FuHtZWHlYCkgnYqVeovZ1XeHvagtXOyto9QZcuKU2c7VEREQVj+GohrNTKfBur0YAgAX/XMG+K3eQk2uAm50VGnjaQxAEhPg5A+C8IyIiqhkYjggvtK6NRl72SMvOxfu/nQEgzTcSBAEAjENrp+JTAQCpWVoM+b/D+GDjWXOUS0REVKEYjggKuQwf9G0CALibqQUgzTfKF5I3KftUXAq0OgPeXHkCB67exS9H4pCh0VV+wURERBWI4YgASBOzOzd0N75uV/d+OGpe2wmCANxIyca4X04a72YDgCu3+VBaIiKqXhiOCAAgCAIiwptAKRfg52qDhp72xvccrJVo5OkAANh+4TZkAuDjZA0AuMxwRERE1YzC3AWQ5QjydcTWd7rAXqWATCaYvBfi74zovCA045lgxCRnYvmB6yaLRxIREVUH7DkiEw087eGd1ytU0FPNfaCQCXizSz0M61AHjbykniT2HBERUXXDniMqlc4NPXDhkz6wUkh5muGIiIiqK/YcUanlByMAaOglzUm6rdYgLSvXXCURERGVO4YjKhNHayV88ydlJ7H3iIiIqg+zh6MffvgBdevWhbW1NVq3bo19+/YV23bEiBEQBKHQFhwcbGwTGRlZZJucnJzKOJ0apZG3NLQWnchwRERE1YdZw9HatWsxYcIETJs2DadOnULnzp0RHh6OuLi4ItvPnz8fCQkJxi0+Ph6urq544YUXTNo5OjqatEtISIC1deFJxvR48ucdca0jIiKqTswajr755huMGjUKr732Gpo0aYJ58+bBz88PixYtKrK9k5MTvL29jdvx48eRkpKCkSNHmrQTBMGknbe3d2WcTo2TH46iGY6IiKgaMVs40mq1OHHiBHr16mWyv1evXjh48GCpjrF06VL07NkTAQEBJvszMjIQEBCA2rVr4+mnn8apU6dKPI5Go4FarTbZ6OEa5U3KvsK1joiIqBoxWzhKTk6GXq+Hl5eXyX4vLy8kJiY+9PMJCQnYunUrXnvtNZP9gYGBiIyMxObNm7F69WpYW1ujY8eOuHLlSrHHmj17NpycnIybn59f2U6qhmngaQ9BkJ7HlpyhMXc5RERE5cLsE7Lzn/yeTxTFQvuKEhkZCWdnZ/Tv399kf/v27fHKK6+gRYsW6Ny5M3799Vc0atQI3333XbHHioiIQFpamnGLj48v07nUNLZWCvi52ALgekdERFR9mC0cubu7Qy6XF+olSkpKKtSb9CBRFLFs2TIMHToUVlZWJbaVyWRo06ZNiT1HKpUKjo6OJhuVzv1J2dLQ2p10DT754wIu3OLQJBERVU1mC0dWVlZo3bo1duzYYbJ/x44dCAsLK/Gze/bswdWrVzFq1KiHfo8oioiKioKPj89j1UtFy593FH07Hbl6A8asOoFlB2IwetUJZGv1Zq6OiIjo0Zl1WG3SpEn4v//7PyxbtgwXL17ExIkTERcXh9GjRwOQhruGDRtW6HNLly5Fu3bt0LRp00LvzZw5E9u2bcN///2HqKgojBo1ClFRUcZjUvlq7H3/dv6vtkXjeGwKACDuXhYW/GvaW/d71E28s+YU0rK5ojYREVkusz5bbfDgwbh79y4++eQTJCQkoGnTpvjrr7+Md58lJCQUWvMoLS0N69evx/z584s8ZmpqKt544w0kJibCyckJISEh2Lt3L9q2bVvh51MTNfSUwlFUfCqOXZeC0ZB2/vj5SBx+3Psfnm3pi0BvR6w9Focp688CkALVW90amK1mIiKikgiiKIrmLsLSqNVqODk5IS0tjfOPHiInV4/gj7dBb5B+jEZ1qovpTwfhzZXHse38bYT4O+Oltv6Ysv4M8n/S6nvYYeekrqWaeE9ERFRa5fX72+x3q1HVZq2UI8BNumOtlb8zpoYHAgBmPtMU9ioFTsWl4v3fpGA0ONQPKoUM1+5k4syNNHOWTUREVCyGI3ps459ogJ5NPPH9kFZQyqUfKW8na7zXu7GxzUtt/TF7QDP0DpZWK99w8oZZaiUiInoYDqsVgcNq5UNvEPHNjmjYWikwpmt9yGQCdkcnYcTyY3CxVeLIBz1hpWA+JyKi8lFev7/NOiGbqje5TMB7vQNN9nVq4A5PBxWS0jXYFZ1k7EkiIiKyFPy/7VSpFHIZ+ofUAmA6tBZ3NwtJ6hxzlUVERGTEcESVbkArKRz9eykJu6KTMCryGLp8tQtPf7efC0cSEZHZMRxRpQv0dkSwryNy9SJGLj+Gfy4lAQCS0jXYei7BzNUREVFNx3BEZvFiGz8AgEIm4IXWtfFyO38AwK/H+dBfIiIyL07IJrMY0i4Avs42aOztgNoutriZmo3VR+Nw+L97iLubBf+8tZOIiIgqG3uOyCxkMgE9mnihtosUgmo526BTA3cAwG8n2HtERETmw3BEFuOFUGmo7bcTN4yPIyEiIqpsDEdkMXoFecHRWoFbaTk4cDXZ3OUQEVENxXBEFsNaKTeugbTuBB8vQkRE5sFwRBZlUN7Q2rbziUjN0pq5GiIiqokYjsiiBPs6oomPI7Q6A1Ydji2xbU6uHleT0jk/iYiIyhXDEVkUQRAwums9AMD/9v6HtKzcYttO23gOPb/Zi7azdmLq+jPYFZ3EoERERI+N4YgsTr/mvgj0dkB6jg5L9l0rsk1qlhZ/nL4FALibqcWaY/EYufwYPvr9XGWWSkRE1RDDEVkcmUzApCcbAQCW7b+OO+maQm3+OpsIrd6AQG8H/PxaOwxsVTtvfwJ7j4iI6LEwHJFFejLICy38nJGdq8f3u64Wen/TqZsAgOdCaqFjA3d8MbAZHKwVSMnKxekbqZVcLRERVScMR2SRBEHA+70bAwB+ORKHm6nZxvfi72Xh6PV7EATgmZa+AAClXIbODaUVtndH36n8gomIqNpgOCKL1bGBO8Lqu0GrN+CzPy9AFKXhss15c4061HODj5ONsX23xp4AgN3RSZVfLBERVRsMR2TRIsKbQCETsPVcIv639z+IoogNJ6UFIvMXjMzXrZEHAODMjTQkZxSep0RERFQaDEdk0ZrVdsLHzwQDAL78+xIW7bmGa3cyoVLIEN7U26Stp6M1gnwcAQB7L3NojYiIyobhiCzeK+38MTjUDwYR+PLvaADShG0Ha2Whtt0Dpd6jXZx3REREZcRwRBZPEATMfDYYLfycjfuee2BILV/+vKN9V+7wln4iIioThiOqEqyVcvzvldao5WyDeh526JI3v+hBIX7OcLRWIDUrF1HxqZVbJBERVQsMR1RleDtZ49/JXbF9Qhco5UX/6CrkMnTOC068a42IiMqC4YiqFJVCDkUxwShfN2M44rwjIiJ6dAxHVO10bewBQQDO3kzDybgUc5dDRERVDMMRVTueDtbGZ61N33SOE7OJiOiRMBxRtTQ1PBCO1gqcv6XGz0dizV0OERFVIQxHVC2526vwXt6z2b7aFs0Vs4mIqNQYjqjaerldAJrWckR6jg6z/7pk7nKIiKiKYDiiaksuE/Dps00BAOtP3sAu3tpPRESlwHBE1VqIvwteae8PABj780mThSFzcvWYv/MK5u+8wknbRERkZPZw9MMPP6Bu3bqwtrZG69atsW/fvmLb7t69G4IgFNouXTIdMlm/fj2CgoKgUqkQFBSEjRs3VvRpkAX76OlgdG7ojiytHiOXH8W1Oxm4lKjGswsP4Nudl/Htzsv45I/zEEUGJCIiMnM4Wrt2LSZMmIBp06bh1KlT6Ny5M8LDwxEXF1fi56Kjo5GQkGDcGjZsaHzv0KFDGDx4MIYOHYrTp09j6NChGDRoEI4cOVLRp0MWykohw6JXWqN5bSekZOXixSWH8czCA4i+nQ4XWyUEAfjpUCx+2H3N3KUSEZEFEEQz/t/ldu3aoVWrVli0aJFxX5MmTdC/f3/Mnj27UPvdu3eje/fuSElJgbOzc5HHHDx4MNRqNbZu3Wrc16dPH7i4uGD16tWlqkutVsPJyQlpaWlwdHR8tJMii3U3Q4MXFh/Cf8mZAIAnAj3x5fPN8cfpW5j5xwUAwJfPN8egUD9zlklERGVUXr+/zdZzpNVqceLECfTq1ctkf69evXDw4MESPxsSEgIfHx/06NEDu3btMnnv0KFDhY7Zu3fvhx6Tqj83exVWjGqLfi18Meu5plg6PBTu9iqM7FgXo7vWBwBEbDiLX47EcYiNiKgGU5jri5OTk6HX6+Hl5WWy38vLC4mJiUV+xsfHB0uWLEHr1q2h0WiwcuVK9OjRA7t370aXLl0AAImJiY90TADQaDTQaO6vg6NWq8t6WmTharvY4ruXQgrtn9KnMZIzNPjtxA18sPEsTsal4LP+TWGtlJuhSiIiMiezhaN8giCYvBZFsdC+fI0bN0bjxo2Nrzt06ID4+Hh8/fXXxnD0qMcEgNmzZ2PmzJllKZ+qCUEQ8OXA5qjnYYevt0XjtxM3cP6WGkuGtoafq625yyMiokpktmE1d3d3yOXyQj06SUlJhXp+StK+fXtcuXLF+Nrb2/uRjxkREYG0tDTjFh8fX+rvp+pDJhPwVrcGWDmqHdzsrHAxQY2Rkceg0enNXRoREVUis4UjKysrtG7dGjt27DDZv2PHDoSFhZX6OKdOnYKPj4/xdYcOHQodc/v27SUeU6VSwdHR0WSjmqtjA3f8+XYnuNurcDUpAwv+ufLwDxERUbVh1mG1SZMmYejQoQgNDUWHDh2wZMkSxMXFYfTo0QCkHp2bN29ixYoVAIB58+ahTp06CA4OhlarxapVq7B+/XqsX7/eeMx33nkHXbp0wZw5c/Dss8/i999/x86dO7F//36znCNVTT5ONvisf1OMXnUCi/f8h/CmPmhay8ncZRERUSUwazgaPHgw7t69i08++QQJCQlo2rQp/vrrLwQEBAAAEhISTNY80mq1mDx5Mm7evAkbGxsEBwdjy5Yt6Nu3r7FNWFgY1qxZgw8//BDTp09H/fr1sXbtWrRr167Sz4+qtj5NvfFUcx9sOZOAyetOY/O4TrBSmH3dVCIiqmBmXefIUnGdI8p3N0ODJ7/di3uZWkzo2RATejYyd0lERFSMKr/OEVFV4GavwoxnggEA83Zewce/n0OmRmfmqoiIqCIxHBE9RL/mPhjVqS4A6TEjfebvxcGryWauioiIKgrDEdFDCIKA6U8HYeWotqjlbIP4e9l4+f+OYPK600jO0Dz8AEREVKUwHBGVUueGHtg2sQteae8PAPjtxA088fVurDx0HXoDp+4REVUXDEdEj8BepcBn/Zthw1thCPZ1hDpHh+m/n8cLiw8i/l6WucsjIqJywHBEVAat/F2weVwnfPJsMBxUCpyMS0X4/H3YeOqGuUsjIqLHxHBEVEZymYBhHergr3c6o00dF2RodJi49jTG/XISt9U55i6PiIjKiOGI6DH5udpi9evtMenJRpDLBPx5JgFPfL0bi3Zf43PZiIiqIC4CWQQuAklldeZGKj76/Tyi4lMBAAFutpjYsxGebu4DhZz/X4SIqCKV1+9vhqMiMBzR4zAYRGw8dRNf/H0Jd9KlW/3rutthbPcG6N/SlyGJiKiCMBxVIIYjKg8ZGh1+OngdP+77D6lZuQCAEH9nLBkaCg8HlZmrIyKqfvj4ECILZ69SYGz3Btg/5QlMDQ+Eo7UCp+JS0f/7A7iUqDZ3eUREVAyGI6IKZq9SYHTX+vh9XCfUdbfDzdRsDPzhIP65eNvcpRERUREYjogqSV13O2x8Kwwd6rkhU6vHqJ+OY+LaKCTxtn8iIovCcERUiZxtrfDTq20xIqwOBAHYeOomnpi7B/+37z8+goSIyEIwHBFVMiuFDDOeCcamtzqihZ8zMjQ6fLblImZsPm/u0oiICAxHRGbTws8ZG8eE4dP+TSEIwMrDsfjlSJy5yyIiqvEYjojMSCYTMLR9ACb3agwA+Oj3czgac8/MVRER1WwMR5XtwHzg7jVzV0EW5q1u9fFUcx/oDCLGrDqBm6nZ5i6JiKjGYjiqTJf+AnZ8BCzuBBxbCnD9TcojCAK+er45gnwccTdTiyE/HsbVpAxzl0VEVCMxHFUm72ZA3S5AbhawZRKwaiCgvmXuqiyPXgekxpu7ikpna6XAkmGtUcvZBtfvZuG57w9gV3SSucsiIqpxGI4qk7MfMPR3oM8cQGENXPsHWNgW+DsCuPefaduq1KukyQBuHAeyCsyVEUVp+PD0WiDuSNGfy04pfJ4ZScCP3YF5TYF/Pqla16Ec1Haxxe/jOqJtHVeka3R4NfIYfth9FTq9wdylERHVGHy2WhEq5dlqdy4DG98Ebp3M2yEA9bpKYSA1DlDfBNwaAKGvAs0HA9ZF1CGKgDYDUNgAckXZ6ki/Ddy5JAU3l7qAIEj7DXog+QqQfQ/walr09wNA/FHg1+FAel4PmIMv4FYfuBMNZOb1eghyYPBKIPCp+5/b940UfnxbAn2/BmqHAvdigJXPASkx99s1ewF49ntAUbOeRabVGfDx5nNYfVTqQWvgaY+I8EA8EegJIf/viIiITPDBsxWo0h48azAA1/4FjiwGru4ovp3SDqjfXQpDuVmANlMKHum3AV02YGUvhQu/9oBrXSA9AUi7AWTeAWxcAMdagIOPFHyyU6TtXgxw87gUxPKpnACf5oAuB0g8Jx0bACAA7o2AWq2BgDApxDn5SXVv/xAw6KQatA/MkZFbSd+dEiP9+eW1QL3uwK7Pgb1fmrZtPhj4bzeQcRtwDgBCRwL/fiYdO6ATEPQskBYHpN2Uagx7G5DJy+NvwWKJooi1x+Ix5+9LSMl7cG37eq745NmmaOTlYNI2Q6NDtlbPB9oSUY3GcFSBKi0cFZR8BbiyA7B1lYKHgzdw9R/g2P8BydEV+MUC4BIgzX3Sa03fUtpJ4Up9o/DH7Dyk8AUAwc8Bz3wnhbfb54F71wDX+oBvCCBTAL+NAC7+AShtpd6js+ukz3WdCqTFA1E/3z+uV1PglfXS+V/bBawdCmjTC39/sxeA/otL12Nm0AP6XEBpXaorYmnUObn4Ydc1LDsQA63OACu5DG/3aIA3u9ZHrt6A5QeuY/Gea8jJ1WNKn0CM6lSXvUtEVCMxHFUgs4Sj4ogiEHsASDwrzVNS2gJKG8DeU9rsPIHUWCDusLSlJ0i9NU61pPeyU6QhuvQEAIIUdmxcAAcvqSfIt5U0ZKbTSsNriWek7/FpAbjWk3pnMu5Iw3/xR4Hr+6T5RaIekCmB3rOAtm/cH44rik4DrHkZuLrz/r4+c4D2o6U/xx8Fds6U6ui/CLBxvt/u9nlg92zpOjj7S+d+YL7Uo9TkGWDgUkCulK7P9X1STfYeUnhLiZW+879dQI4a6DAWeGI6oLCqiL+pCncjJQsf/34e/1yShisbezngbqYWyRkak3Y9Aj3x1Qst4GpXNc+TiKisGI4qkEWFI0ukSZcCkksdaRivNLRZwOrBQOxBaY5R6Miyf3/0VuDXYVJPl2+IFN6K6t0qSq3WUqAqbd2VTacFzqwBarcBPJsUelsURfwedQsz/jiP1LyhNn9XW7zbqxHU2bn4dMtFaHUGeDta4/shIWgd4FrZZ0BEZDYMRxWI4aiCiCKQk2baM1RWV3YCa4dI86MAaVJ6va7S3KbMO9JdbzbOQP0eQIMe0lymzeOl71c5Ao3DARtXaRhTaSP1RBn0gJUdEDxA6lkr6FaUNKfKr53UU1XSOebPC9OkS8d0qVO63qrMZGkYMe4gYO0MvLlXGvIswp10DZbsvYY67nZ4obUfrBTSjafnb6Vh/OpT+O9OJqzkMnzWvykGtfF7+HcTEVUDDEcViOGoiog/Clz4HajTWQpGSpuS26fGA+tfA+IPl9xOYQ2EDAU6vAXcOgUc+kGavA5IQ5KBTwF1u0qT2ZMuSHfmZd2TwpM2AxAfuO1epgDcGwPeTYFWw4A6nQp/Z+I5YPVL0qTzfL4hwKvbHvlOvUyNDpPXncbWc4kAgBFhdfDhU02gkHPlDiKq3hiOKhDDUTWm1wGX/pTmaWXdA7LuSsNzMgUgyICki/eDUEFyK0DlILUvLSt76b8F7+ITZECPj4GO70jztAx64NRK4O8PgNxMaTmFp76WQlx2CtDmden1IzIYRCz49wrm7bwCAAj2dcS47g3QK9gbchknaxNR9cRwVIEYjmowUZQmdu+bKy0tYOsOtHkNaDMKsHWT5kxd2AQknJaCjFcQ4BkE2HtJYUhlL/1XaQvIZNLx0uKlieXnNwJn1krfE/ycdMfdv59JvU+AtHr6Cz9JQ31XdgA/Py/tH7gUaPZ8mU7n73MJePfX08jU6gEA9TzsMLprfQwIqVVkT5Ioijh/S41/LiYhNVuLTg3cEVbfHTZW1XvZBCKqHhiOKhDDEQGQ5gCpHMpvAUpRBI4vBbZOkeY45bNxAbq8D7R93XQ+0z+fAvvyeo1c6gJ+baUJ5Y6+0t14tu7SHXwF5zMZDNKw4X97gJxUQJMOTZYascmZuHpPgyy9HDIY4G2lQaAL4KISofdqjovWLfB7aj38cVWL22rTu99UChk6NnDHiLA66NzQ3XSZAG0mkJ0q3R1JRGRmDEcViOGIKlTsIeluu5xUoN2bQOd3pYD0IIMe2PA6cG598cdSWEvLMfi3k9ZyOr9RWrqhjKIM9bANYUip0xcyF3/sib6Dm6nZxvfb1XXFe70bI7SWDXD0R2D/N1I4aj4YeGKaFNaIiMyE4agCMRxRhdNmSus/2ZbiVvvsFODmCSD+mLSeU+ad+9uDq5ID0t14jfoATrWlni8re2l+kz4X0GuRozdgb1wutl7JglZnQKgsGp2Vl9BAjDU9Tu22EIP745pHT/x8UYdfjlxHgD4eHWXnMFq5BV64Z9JclKsgtH1dGh6095I25N2hmJMmfb+dh7Q+l7WTVH9qnLQpVNLK6C51in9UDRHRQzAcVSCGI6oSRBG4e1Va/DP+sNTTFPg00KBnqVYDT1Ln4MC1ZDT2ckQTHwcIGUnApT+AcxulhUdR4J8Gr6YwpMRCVmC18huiO+brBuCKoTamKNagg/xC+ZyXrTvgFQx4N5PWelLaSuFOkEnzvtwaSgGruIVHtVnSf61sy6ceIqoyGI4qEMMR1XjpidIyCec3AnGH7u9X2iLXOwTJfr2R1OhFaEQlLtxKw+aom3C8uRtD5P/CR7gLDyEVHoIagiBAsHaS1pwS5FJvUU6qdCxBnreae21ArwFSrpf+bkCVI+DeMC9ENZfmZN06JT2r8MZRaU6Xg6+0yrt7Q+kBx74h0uT5ktapIqIqrdqEox9++AFfffUVEhISEBwcjHnz5qFz585Ftt2wYQMWLVqEqKgoaDQaBAcHY8aMGejdu7exTWRkJEaOLLz6cnZ2NqytS/dsLYYjogLSbkqBw7V+Xrgo+nl28fey8OeZBGw+fQsXE9QQYIBMJsPXL7TAcyG17zfUaaWAZONa+FiadOk5g7fPSWs/JV+WhuMgSutHpSdIw3APriVVWnIV4OwnTWp3rCUN43k0BjyaAG71GZyIqrjy+v1diqd2Vpy1a9diwoQJ+OGHH9CxY0f873//Q3h4OC5cuAB//8ITO/fu3Ysnn3wSn3/+OZydnbF8+XL069cPR44cQUhIiLGdo6MjoqNNH9Za2mBERA9wqgU4PffQZn6uthjTrT7GdKuPq0npWPDPVWw+fQuTfj2NbK0BL7fzR0qmFj8duo4tZxKgUsrgaqeCm50Vwuq7YWCr2pCpHIBaraStODoNcO8/aU2q/BB175o0BFevO1C/u7TC+L3/8tpdAG6elFY516RJQ5F3rxZxYEGaA2bnCdi5S71OHo0B90bSXCmI0lCmQiX1VHHYjqjaMmvPUbt27dCqVSssWrTIuK9Jkybo378/Zs+eXapjBAcHY/Dgwfjoo48ASD1HEyZMQGpqapnrYs8R0eMzGETM+OM8VhySJnr3DvbC3svJyM7VF9k+xN8Zs/o3Q5BvBf1vzmCQFv9MuwGob0nP47t7TXrg8p3ooie3l8TJXxqyc290/792HnlzoQRp3pdjbWm9KyKqFFW+50ir1eLEiROYOnWqyf5evXrh4MGDpTqGwWBAeno6XF1N7/jJyMhAQEAA9Ho9WrZsiU8//dSkZ+lBGo0GGs39tV3UavUjnAkRFUUmEzDzmWDYKOX4397/sO38bQBAkI8jXutcF862StzN0CLuXhaWH7iOU3Gp6LdwP0aE1cH4JxrA2fb++k1anQF/nU2Ah4MKHRu4l7Ug6YHDRT10WBSl5/FlJUvzotJvA3evSKEp+TKQk/dvgiBIdxrmpEqPekmLA679U/x3WtkDXk2lyeWude8P50GQllxQ35KO5xUkLcng6FO2cyOicmW2cJScnAy9Xg8vL9MHfHp5eSExMbFUx5g7dy4yMzMxaNAg477AwEBERkaiWbNmUKvVmD9/Pjp27IjTp0+jYcOGRR5n9uzZmDlzZtlPhoiKJAgCpoYHwsvRGseu38NLbf0LLyQJYEi7AHz65wVsOZuApftj8OvxeLzRuR6GdaiDv84lYOG/V43rLT3fujY+6hcER+tynB8kCNLDhh984HBxMu9Kocm4XckLUWkwDr/lZkm9UfGHH/48v3z23kU8mDnvWgkyqTdKaSutb6W0ub/JlPd7rORKaS6Vaz1pHpWTHyDjCudEj8Jsw2q3bt1CrVq1cPDgQXTo0MG4f9asWVi5ciUuXbpU4udXr16N1157Db///jt69uxZbDuDwYBWrVqhS5cuWLBgQZFtiuo58vPz47AaUSXbHZ2EL7ZewqVEackAmQAY8v6FcrWzQkqWFqII+DhZ44O+TeBqZwWNTg+dXkQTH0f4uVrQPCC9Tup9SjwrzY3KH85LuwlAlHqQHH2lOUwJZ4A7F8s+0bwkcqu8sFRfujPQxlmak2VlJz1XMDdLmsclVwJWDtIjcKyd7s+9svfi/CqqMqr8sJq7uzvkcnmhXqKkpKRCvUkPWrt2LUaNGoV169aVGIwAQCaToU2bNrhy5UqxbVQqFVSqcnpEBBGVWbfGnujS0AN/nk3AtzsuIyY5E+72KozpVh9D2vnj3M00TF53GtfvZmH86lOFPl/L2QYd6ruhgac9bJRy2CjlsFMp4G5vBU9Ha3g4qGCvKvzPnsEgQi+KUBbxvLkykyukSeKeTQAMemhzaDOB2xcAXXbR7xv0gC4HyM2WNl32/T/n39EHSK/vxUiT1O/FSMsk5PdwlZWdhzQJ3SVA6qky6PM2HSDm/ddgkMKWXivVY8i9/2eFSurJcq0vDS/auNxfoFSfK02Uz0mTahcNeZsoBTgrO6mdTAFjr1z+uYqi1KPm7MceMipXZp+Q3bp1a/zwww/GfUFBQXj22WeLnZC9evVqvPrqq1i9ejX69+//0O8QRRFt27ZFs2bNsGzZslLVxQnZROan0xtw+kYqgnycTB58m6XV4Zvtl7ErOglKuQwqhQx6UcSlhHToDA//58zTQYXG3g5o7OUAjc6ACwlqXExQwyCKGNe9AUZ3rV/kQ3mrJINe6rG6d02afJ6emLdieaoUxhQqQGEjDdfptNIwoDZDWpU94w6QmSQFsqogv4fMwUcKXzYuUpDLzZbOQaeResBUjtLm4J0X2OpJPWQ5aunaaNOlhUgdfEyfWyiK0vUsZimLGivrnnT3p4O31Bsqk9+/+SH5shSC/TsUv2hrOasW6xytXbsWQ4cOxeLFi9GhQwcsWbIEP/74I86fP4+AgABERETg5s2bWLFiBQApGA0bNgzz58/HgAEDjMexsbGBk5MTAGDmzJlo3749GjZsCLVajQULFmDlypU4cOAA2rZtW6q6GI6Iqp5MjQ7HY1Nw+L+7uK3OgSbXgOxcPdJzcpGcocWddA0yNLqHHqelnzPmDmqB+h72lVC1hRPzHv+Scl3aUmOl3iBBLvXkyOR5f87b5CppeE6ulMKK3Er6sybjfkBLjZWCiDZDWtdKrpSG8aydpTAjyKXeIOTN29JkSEFOzL/LUbg/vwqQeqhS46UesnIlSD1mMsX9WiFK52RlL/VoGXRS6NJrpVXba7cBaoVKy1/cuST1BCZHS5/NzZF6+2RKaWjTxkUasvRrCwR0AnyaS0Hu7lXpOon6+4/hsXGBca0vg/5+71p+751eI4Vb4P5QqKrAz2/+r/mSAkp2ChC9VQrTcmXePDYZkJspXf/cbClMqxylv6/UWOkB14lnYezJkymkxVcz75j2gHo0AdqPAZoPkv6OK1C1CEeAtAjkl19+iYSEBDRt2hTffvstunTpAgAYMWIErl+/jt27dwMAunXrhj179hQ6xvDhwxEZGQkAmDhxIjZs2IDExEQ4OTkhJCQEM2bMMJnX9DAMR0TVU3pOLq4kZeByYjqib6fDSi5DkK8jgnwcceZGGmb8cR7pOTqoFDL0aeqNFrWd0cLPGQ7WCtxKzcat1Byoc3LhamsFN3sruNur0MDTHnZFDNVRJTLopbv/7l4DMpOlX/TZKdIvaEXepHW5lfSLPr+HSH1TWgcr5boUMCBIv/St7KRjlHvYegiZUgp65UVhk/dMRa10fnYeeeuAPQHUDpUCX9ZdKQxd+ktaXb6s3+/gI12zgp+XqwC3BlKIyl8mw8oBsHO7f1OBdzPgmaLnApdVtQlHlojhiKhmupWajSnrz2DfleRSf0YmAI29HRHi74w2dVzQuaEH3O0Lz2HU6PQ48t897I6+A6VcwIBWtdHY26E8y6ey0Ouk0GTlcH9NKlGUgoP6ptRDkz9RXW4l9aJoM6XPyBT3e8tSYoAbx6UtIxHwCJRWlPcMkhYXzb/DUK/NC2+pUjiLOwTEHrz/WB17L2lullwpLS+RkSi1FWT3e+qMf5aZ9tKJBqnXJjerbNfCMxio3VoKm/pcqffKyg5Q2km16zR588PUUu9XnS7Sg6YdvKTPpCdINxzYuUtDnDK5VPuplcCR/wFp8abf59ceGLWtbLUWg+GoAjEcEdVcoiji0LW7OBGbgqj4VJy+kQatTg9fZxv4OtvAyUaJlCwt7mZokajOwZ30wj0MTWs5orW/C/SiiCytHimZWhyNuYdMrekCmCH+zni+dW2E+LmgvqcdVApOKK6R8ufo2LpKvVePS5MhzRcTZFJokiml4b1r/wJX/5HW77Jxlh7kbOsGBIQBwc9JK8JXFL1OGmrUZubdTJAjBc46ncr1axiOKhDDERGVVmJaDqLiU3AyLhUHribj/K3iF5H1cFDhicaeSM3W4p+LSSYTyBUyAQFutpAJAjI0OqTn6KCUC/B1tkEtZxv4u9oitI4r2tdzNS6QmanR4UpSBgyiiCbejiYT14lqIoajCsRwRERldSddg31X7iA6MR0qpRy2VnLYWcnRws8ZTX2dIJNJk2KT0nOw/sRN/HvpNi4lpiM95+GTxQFpGkljLwdkanWIv3d/0qtMAOp72KNpLScE+zqiaS0nBPk6FlosM0OjQ3SiGnfStehQ3w1ONnzYLlUfDEcViOGIiCqTKIpIVOfgalIG5IIAB2sl7K0V0Oj0uJmSjVup2bh8OwOH/7uLK0mmz4DzcFBBFIHkjKInEDtYK+Bko4STjRLpOTrE3bs/H8VaKUPfZj54sY0/GnraI9dggN4gwkYph5ONstBK5gXFJGciU6NDsK9jie2IKhPDUQViOCIiS5WUnoNTcalwslGikZcDXO2kIbYkdQ7O3UrDuZtqnM/7b/4jVx7k7WgNGys5YpIzi/0eB2sF/FxsUcfdFi1qO6N1gAua+Dhi35VkrDh0HQev3QUgPSvv1U510a+FD+dMkdkxHFUghiMiqg7SsnKRnKlBWnYu0rJzoVLIEOjtCFc7K4iiiKj4VKw5Go8/z9wyThZXygXk6h/+a0EmAEq5DBqd9MgTJxsl3O2toJDJoJALUMgEyGUCFHIZBAD6vFXIZYKAQG8HtPJ3QUt/Z2Rr9biUmI7oRDXSc3So5WyD2q428HWyga2VAlYKaaFPbydrWCsZvqhkDEcViOGIiGoSQ97E8Pz5UDm5etxIyULcvSxcuZ2BU3GpOB6bguQMDVztrPBiGz8MaR8AOys5Vh+Nx08HryNRXbEraasUMrTyd0GH+m4I9HZArl5Edq4e2bl6aHL1yNbqkaPTw8XWCvU97I3rT127k4ErtzOQmJaNZrWd0bGBG2ytuC5VdcVwVIEYjoiITImiiDvpGjjZKgsNn+XqDTh/Sw1Nrh46gwidQYTeYECuXoTeIMIgilDIBMgEATk6A87eSMXJuFScvZkGG6UcTXwcEOjtCGdbJW6mZONGSjYS1TnIydVDqzMgSyuFoPJgpZAhrL4b6rnbQxCkdbblcgEqhRyqvF4qlVIO67z/Gvcp5FApZVAW6BnT6AzI1OiQlauHrVIOHycbeDmpOLxoRgxHFYjhiIio4hkMohRQHjKhWxRFXLuTiUPXknHov7u4kZIN67wHC1srZdJDhq3kUCnkSErPwbWkTMQkZ0KrN6CWsw0aeNrDw0GFIzF3Te7wqygutko421rByUYJRxslrORSMJTLBNhYyeFsYwVnWyXsVAqIomh8uoedSgFHG2kCvZudCj5O1nC2LXliPJliOKpADEdERFWb3iAiV28wmackiiKuJmVgd/Qd3M3UQoQIiIDOIEKrM0Cj00OjM0CTK/05J7fAvrz3dXoxr0fMAJVCWqrBWilHllaHhLQc4xys8qJSyOBqZwWDmN8LB9hayY13IDpaS/91slXCWiFDdq4eGRo9srU6WCvlsFcp4GCthEIuQJ/XqwdRhK1KATsrOexUCng4qODjZAMfJ+tSPQpHFEWkZefitlqDRHUONLl6tK/vVmjZCHMor9/fHHglIqJqRy4TIJeZDm8JgoCGXg5o6FUxj20RRRGpWblISpcmwadmaaHO0UGnN0AvijAYRGRq9UjNykVathYZGj1kgvERusjQ6KHOmzyfnKHB3UwtNDoDEtJM53PdywRupFRMD5iLrRJ13e1Q190e7g5WuJ2Wg5t5zxXM0OiMYfHBbhWVQoaeTbzwbEtf1POwe+Co0hnqDAbczXsI9J10af7awNa1K+Q8Hhd7jorAniMiIjI3jU6PJLUGKVnavLAnGFdQT8vOhTpvS8vOhTpHhyytDnZWCthaKWBrJYdGp0d63mrrer0Ied5cKQDI1OiRqdEhQ6NDUnoOElJzkK4p3UKk+ZxtlfB2tIZGZyhxWYjitA5wwfoxYY/8uZKw54iIiKgaUynk8HO1hZ+rbaV8X3pOLuLvZSMmORMxyRlIztDCy9EatVykR9g42SigUkjDiA7WCuOQpSiKOH9LjU2nbmLbhURk5OhQsNclvwtGLhPgZmcFDwcVPBxUaFRBPXjlgT1HRWDPERERUdVTXr+/ZeVYExEREVGVx3BEREREVADDEREREVEBDEdEREREBTAcERERERXAcERERERUAMMRERERUQEMR0REREQFMBwRERERFcBwRERERFQAwxERERFRAQxHRERERAUwHBEREREVwHBEREREVIDC3AVYIlEUAQBqtdrMlRAREVFp5f/ezv89XlYMR0VIT08HAPj5+Zm5EiIiInpU6enpcHJyKvPnBfFx41U1ZDAYcOvWLTg4OEAQhHI9tlqthp+fH+Lj4+Ho6Fiux65qeC0kvA738Vrcx2txH6/FfbwWkuKugyiKSE9Ph6+vL2Syss8cYs9REWQyGWrXrl2h3+Ho6Fijf7AL4rWQ8Drcx2txH6/FfbwW9/FaSIq6Do/TY5SPE7KJiIiICmA4IiIiIiqA4aiSqVQqfPzxx1CpVOYuxex4LSS8DvfxWtzHa3Efr8V9vBaSir4OnJBNREREVAB7joiIiIgKYDgiIiIiKoDhiIiIiKgAhiMiIiKiAhiOKtEPP/yAunXrwtraGq1bt8a+ffvMXVKFmz17Ntq0aQMHBwd4enqif//+iI6ONmkjiiJmzJgBX19f2NjYoFu3bjh//ryZKq4cs2fPhiAImDBhgnFfTboON2/exCuvvAI3NzfY2tqiZcuWOHHihPH9mnItdDodPvzwQ9StWxc2NjaoV68ePvnkExgMBmOb6not9u7di379+sHX1xeCIGDTpk0m75fmvDUaDcaPHw93d3fY2dnhmWeewY0bNyrxLMpHSdciNzcXU6ZMQbNmzWBnZwdfX18MGzYMt27dMjlGTbgWD3rzzTchCALmzZtnsr88rgXDUSVZu3YtJkyYgGnTpuHUqVPo3LkzwsPDERcXZ+7SKtSePXswduxYHD58GDt27IBOp0OvXr2QmZlpbPPll1/im2++wcKFC3Hs2DF4e3vjySefND7jrro5duwYlixZgubNm5vsrynXISUlBR07doRSqcTWrVtx4cIFzJ07F87OzsY2NeVazJkzB4sXL8bChQtx8eJFfPnll/jqq6/w3XffGdtU12uRmZmJFi1aYOHChUW+X5rznjBhAjZu3Ig1a9Zg//79yMjIwNNPPw29Xl9Zp1EuSroWWVlZOHnyJKZPn46TJ09iw4YNuHz5Mp555hmTdjXhWhS0adMmHDlyBL6+voXeK5drIVKlaNu2rTh69GiTfYGBgeLUqVPNVJF5JCUliQDEPXv2iKIoigaDQfT29ha/+OILY5ucnBzRyclJXLx4sbnKrDDp6eliw4YNxR07dohdu3YV33nnHVEUa9Z1mDJlitipU6di369J1+Kpp54SX331VZN9AwYMEF955RVRFGvOtQAgbty40fi6NOedmpoqKpVKcc2aNcY2N2/eFGUymfj3339XWu3l7cFrUZSjR4+KAMTY2FhRFGvetbhx44ZYq1Yt8dy5c2JAQID47bffGt8rr2vBnqNKoNVqceLECfTq1ctkf69evXDw4EEzVWUeaWlpAABXV1cAQExMDBITE02ujUqlQteuXavltRk7diyeeuop9OzZ02R/TboOmzdvRmhoKF544QV4enoiJCQEP/74o/H9mnQtOnXqhH/++QeXL18GAJw+fRr79+9H3759AdSsa1FQac77xIkTyM3NNWnj6+uLpk2bVutrA0j/jgqCYOxtrUnXwmAwYOjQoXjvvfcQHBxc6P3yuhZ88GwlSE5Ohl6vh5eXl8l+Ly8vJCYmmqmqyieKIiZNmoROnTqhadOmAGA8/6KuTWxsbKXXWJHWrFmDkydP4tixY4Xeq0nX4b///sOiRYswadIkfPDBBzh69CjefvttqFQqDBs2rEZdiylTpiAtLQ2BgYGQy+XQ6/WYNWsWXnrpJQA16+eioNKcd2JiIqysrODi4lKoTXX+dzUnJwdTp07Fyy+/bHzgak26FnPmzIFCocDbb79d5PvldS0YjiqRIAgmr0VRLLSvOhs3bhzOnDmD/fv3F3qvul+b+Ph4vPPOO9i+fTusra2LbVfdrwMg/T+/0NBQfP755wCAkJAQnD9/HosWLcKwYcOM7WrCtVi7di1WrVqFX375BcHBwYiKisKECRPg6+uL4cOHG9vVhGtRlLKcd3W+Nrm5uXjxxRdhMBjwww8/PLR9dbsWJ06cwPz583Hy5MlHPq9HvRYcVqsE7u7ukMvlhVJrUlJSof9nVF2NHz8emzdvxq5du1C7dm3jfm9vbwCo9tfmxIkTSEpKQuvWraFQKKBQKLBnzx4sWLAACoXCeK7V/ToAgI+PD4KCgkz2NWnSxHhzQk35mQCA9957D1OnTsWLL76IZs2aYejQoZg4cSJmz54NoGZdi4JKc97e3t7QarVISUkptk11kpubi0GDBiEmJgY7duww9hoBNeda7Nu3D0lJSfD39zf+OxobG4t3330XderUAVB+14LhqBJYWVmhdevW2LFjh8n+HTt2ICwszExVVQ5RFDFu3Dhs2LAB//77L+rWrWvyft26deHt7W1ybbRaLfbs2VOtrk2PHj1w9uxZREVFGbfQ0FAMGTIEUVFRqFevXo24DgDQsWPHQss5XL58GQEBAQBqzs8EIN2JJJOZ/jMsl8uNt/LXpGtRUGnOu3Xr1lAqlSZtEhIScO7cuWp3bfKD0ZUrV7Bz5064ubmZvF9TrsXQoUNx5swZk39HfX198d5772Hbtm0AyvFaPPL0cSqTNWvWiEqlUly6dKl44cIFccKECaKdnZ14/fp1c5dWocaMGSM6OTmJu3fvFhMSEoxbVlaWsc0XX3whOjk5iRs2bBDPnj0rvvTSS6KPj4+oVqvNWHnFK3i3mijWnOtw9OhRUaFQiLNmzRKvXLki/vzzz6Ktra24atUqY5uaci2GDx8u1qpVS/zzzz/FmJgYccOGDaK7u7v4/vvvG9tU12uRnp4unjp1Sjx16pQIQPzmm2/EU6dOGe/AKs15jx49Wqxdu7a4c+dO8eTJk+ITTzwhtmjRQtTpdOY6rTIp6Vrk5uaKzzzzjFi7dm0xKirK5N9RjUZjPEZNuBZFefBuNVEsn2vBcFSJvv/+ezEgIEC0srISW7VqZbydvToDUOS2fPlyYxuDwSB+/PHHore3t6hSqcQuXbqIZ8+eNV/RleTBcFSTrsMff/whNm3aVFSpVGJgYKC4ZMkSk/dryrVQq9XiO++8I/r7+4vW1tZivXr1xGnTppn80quu12LXrl1F/tswfPhwURRLd97Z2dniuHHjRFdXV9HGxkZ8+umnxbi4ODOczeMp6VrExMQU++/orl27jMeoCdeiKEWFo/K4FoIoiuKjd24RERERVU+cc0RERERUAMMRERERUQEMR0REREQFMBwRERERFcBwRERERFQAwxERERFRAQxHRERERAUwHBERFUMQBGzatMncZRBRJWM4IiKLNGLECAiCUGjr06ePuUsjompOYe4CiIiK06dPHyxfvtxkn0qlMlM1RFRTsOeIiCyWSqWCt7e3yebi4gJAGvJatGgRwsPDYWNjg7p162LdunUmnz979iyeeOIJ2NjYwM3NDW+88QYyMjJM2ixbtgzBwcFQqVTw8fHBuHHjTN5PTk7Gc889B1tbWzRs2BCbN282vpeSkoIhQ4bAw8MDNjY2aNiwYaEwR0RVD8MREVVZ06dPx8CBA3H69Gm88soreOmll3Dx4kUAQFZWFvr06QMXFxccO3YM69atw86dO03Cz6JFizB27Fi88cYbOHv2LDZv3owGDRqYfMfMmTMxaNAgnDlzBn379sWQIUNw79494/dfuHABW7duxcWLF7Fo0SK4u7tX3gUgoopRpsfmEhFVsOHDh4tyuVy0s7Mz2T755BNRFEURgDh69GiTz7Rr104cM2aMKIqiuGTJEtHFxUXMyMgwvr9lyxZRJpOJiYmJoiiKoq+vrzht2rRiawAgfvjhh8bXGRkZoiAI4tatW0VRFMV+/fqJI0eOLJ8TJiKLwTlHRGSxunfvjkWLFpnsc3V1Nf65Q4cOJu916NABUVFRAICLFy+iRYsWsLOzM77fsWNHGAwGREdHQxAE3Lp1Cz169CixhubNmxv/bGdnBwcHByQlJQEAxowZg4EDB+LkyZPo1asX+vfvj7CwsDKdKxFZDoYjIrJYdnZ2hYa5HkYQBACAKIrGPxfVxsbGplTHUyqVhT5rMBgAAOHh4YiNjcWWLVuwc+dO9OjRA2PHjsXXX3/9SDUTkWXhnCMiqrIOHz5c6HVgYCAAICgoCFFRUcjMzDS+f+DAAchkMjRq1AgODg6oU6cO/vnnn8eqwcPDAyNGjMCqVaswb948LFmy5LGOR0Tmx54jIrJYGo0GiYmJJvsUCoVx0vO6desQGhqKTp064eeff8bRo0exdOlSAMCQIUPw8ccfY/jw4ZgxYwbu3LmD8ePHY+jQofDy8gIAzJgxA6NHj4anpyfCw8ORnp6OAwcOYPz48aWq76OPPkLr1q0RHBwMjUaDP//8E02aNCnHK0BE5sBwREQW6++//4aPj4/JvsaNG+PSpUsApDvJ1qxZg7feegve3t74+eefERQUBACwtbXFtm3b8M4776BNmzawtbXFwIED8c033xiPNXz4cOTk5ODbb7/F5MmT4e7ujueff77U9VlZWSEiIgLXr1+HjY0NOnfujDVr1pTDmROROQmiKIrmLoKI6FEJgoCNGzeif//+5i6FiKoZzjkiIiIiKoDhiIiIiKgAzjkioiqJMwKIqKKw54iIiIioAIYjIiIiogIYjoiIiIgKYDgiIiIiKoDhiIiIiKgAhiMiIiKiAhiOiIiIiApgOCIiIiIqgOGIiIiIqID/B6MImcH1cGe6AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict on both training and test sequences\n",
    "y_train_pred = model.predict(X_train_seq)\n",
    "y_test_pred = model.predict(X_test_seq)\n",
    "\n",
    "# Inverse scale predictions and actual values to original scale\n",
    "y_train_pred_rescaled = scaler_y.inverse_transform(y_train_pred)\n",
    "y_test_pred_rescaled = scaler_y.inverse_transform(y_test_pred)\n",
    "y_train_actual_rescaled = scaler_y.inverse_transform(y_train_seq)\n",
    "y_test_actual_rescaled = scaler_y.inverse_transform(y_test_seq)\n",
    "\n",
    "# Define a function for directional accuracy\n",
    "def directional_accuracy(y_true, y_pred):\n",
    "    return np.mean((np.sign(y_true[1:] - y_true[:-1]) == np.sign(y_pred[1:] - y_pred[:-1])).astype(int)) * 100\n",
    "\n",
    "# Calculate metrics.\n",
    "# Root Mean Squared Error (RMSE) for training set\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_actual_rescaled, y_train_pred_rescaled))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_actual_rescaled, y_test_pred_rescaled)) # RMSE for test set.\n",
    "train_mae = mean_absolute_error(y_train_actual_rescaled, y_train_pred_rescaled) # Mean Absolute Error (MAE) for training set.\n",
    "test_mae = mean_absolute_error(y_test_actual_rescaled, y_test_pred_rescaled) # MAE for test set.\n",
    "# Directional accuracy for training set.\n",
    "train_directional_accuracy = directional_accuracy(y_train_actual_rescaled.flatten(), y_train_pred_rescaled.flatten())\n",
    "# Directional accuracy for test set.\n",
    "test_directional_accuracy = directional_accuracy(y_test_actual_rescaled.flatten(), y_test_pred_rescaled.flatten())\n",
    "\n",
    "# Display score metrics to evaluate the model's performance.\n",
    "print(\"LSTM Model Performance:\")\n",
    "print(f\"Training RMSE: {train_rmse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Training MAE: {train_mae}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Directional Accuracy on Training Data: {train_directional_accuracy}%\")\n",
    "print(f\"Directional Accuracy on Test Data: {test_directional_accuracy}%\")\n",
    "\n",
    "# Plot training & validation loss values, it will help visualize how well the model fits the data over training epochs.\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df62f9e9-ffea-4b05-b984-bc9ba5baa368",
   "metadata": {},
   "source": [
    "### Three Layers (Model 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bacd45f9-46ce-4a4b-84b3-0a7cb3494d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running combination 1/30: {'units3': 32, 'units2': 32, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 32}\n",
      "Final Training Loss: 0.31081756949424744\n",
      "Final Validation Loss: 0.776311993598938\n",
      "Running combination 2/30: {'units3': 128, 'units2': 64, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 3.267017364501953\n",
      "Final Validation Loss: 2.864630937576294\n",
      "Running combination 3/30: {'units3': 128, 'units2': 64, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.4, 'clipnorm': 5.0, 'batch_size': 256}\n",
      "Final Training Loss: 1.2463526725769043\n",
      "Final Validation Loss: 0.40394943952560425\n",
      "Running combination 4/30: {'units3': 128, 'units2': 64, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 5.0, 'batch_size': 120}\n",
      "Final Training Loss: 7.20762300491333\n",
      "Final Validation Loss: 6.094005584716797\n",
      "Running combination 5/30: {'units3': 64, 'units2': 64, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 3.4619333744049072\n",
      "Final Validation Loss: 2.944054126739502\n",
      "Running combination 6/30: {'units3': 64, 'units2': 32, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 1.2618343830108643\n",
      "Final Validation Loss: 0.34231215715408325\n",
      "Running combination 7/30: {'units3': 32, 'units2': 32, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 256}\n",
      "Final Training Loss: 2.060930013656616\n",
      "Final Validation Loss: 1.5583962202072144\n",
      "Running combination 8/30: {'units3': 32, 'units2': 128, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 64}\n",
      "Final Training Loss: 3.4469377994537354\n",
      "Final Validation Loss: 2.342259168624878\n",
      "Running combination 9/30: {'units3': 32, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 256}\n",
      "Final Training Loss: 1.4383416175842285\n",
      "Final Validation Loss: 0.366670697927475\n",
      "Running combination 10/30: {'units3': 64, 'units2': 64, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0001, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 64}\n",
      "Final Training Loss: 1.5169789791107178\n",
      "Final Validation Loss: 0.3970092535018921\n",
      "Running combination 11/30: {'units3': 128, 'units2': 128, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.001, 'epochs': 200, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 1.6821414232254028\n",
      "Final Validation Loss: 0.6079508066177368\n",
      "Running combination 12/30: {'units3': 32, 'units2': 64, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 32}\n",
      "Final Training Loss: 15.81805419921875\n",
      "Final Validation Loss: 14.972208976745605\n",
      "Running combination 13/30: {'units3': 128, 'units2': 64, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 64}\n",
      "Final Training Loss: 0.8279988765716553\n",
      "Final Validation Loss: 0.4572591185569763\n",
      "Running combination 14/30: {'units3': 32, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 120}\n",
      "Final Training Loss: 0.6296482682228088\n",
      "Final Validation Loss: 0.3589296340942383\n",
      "Running combination 15/30: {'units3': 128, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 3.206968069076538\n",
      "Final Validation Loss: 1.994337797164917\n",
      "Running combination 16/30: {'units3': 64, 'units2': 64, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 2.0357325077056885\n",
      "Final Validation Loss: 1.6651852130889893\n",
      "Running combination 17/30: {'units3': 128, 'units2': 32, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 14.88690185546875\n",
      "Final Validation Loss: 14.080066680908203\n",
      "Running combination 18/30: {'units3': 32, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 1.7895896434783936\n",
      "Final Validation Loss: 0.3744671642780304\n",
      "Running combination 19/30: {'units3': 128, 'units2': 128, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 4.197436332702637\n",
      "Final Validation Loss: 3.1068782806396484\n",
      "Running combination 20/30: {'units3': 64, 'units2': 32, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 120}\n",
      "Final Training Loss: 2.464466094970703\n",
      "Final Validation Loss: 1.7121291160583496\n",
      "Running combination 21/30: {'units3': 64, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 50, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 32}\n",
      "Final Training Loss: 2.757735013961792\n",
      "Final Validation Loss: 2.0170390605926514\n",
      "Running combination 22/30: {'units3': 128, 'units2': 128, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 5.0, 'batch_size': 256}\n",
      "Final Training Loss: 31.221874237060547\n",
      "Final Validation Loss: 28.750707626342773\n",
      "Running combination 23/30: {'units3': 32, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 200, 'dropout_rate': 0.4, 'clipnorm': 5.0, 'batch_size': 256}\n",
      "Final Training Loss: 16.669702529907227\n",
      "Final Validation Loss: 14.899700164794922\n",
      "Running combination 24/30: {'units3': 64, 'units2': 128, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.1, 'epochs': 200, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 256}\n",
      "Final Training Loss: 36.52252960205078\n",
      "Final Validation Loss: 35.74982452392578\n",
      "Running combination 25/30: {'units3': 64, 'units2': 64, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 120}\n",
      "Final Training Loss: 10.768044471740723\n",
      "Final Validation Loss: 9.871023178100586\n",
      "Running combination 26/30: {'units3': 32, 'units2': 64, 'units1': 64, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.01, 'epochs': 100, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 256}\n",
      "Final Training Loss: 2.897397994995117\n",
      "Final Validation Loss: 2.030792474746704\n",
      "Running combination 27/30: {'units3': 64, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.1, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 1.0, 'batch_size': 64}\n",
      "Final Training Loss: 2.1611669063568115\n",
      "Final Validation Loss: 1.6311352252960205\n",
      "Running combination 28/30: {'units3': 64, 'units2': 64, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.01, 'epochs': 100, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 120}\n",
      "Final Training Loss: 2.3054661750793457\n",
      "Final Validation Loss: 1.5798420906066895\n",
      "Running combination 29/30: {'units3': 64, 'units2': 32, 'units1': 32, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.0005, 'l2_lambda': 0.1, 'epochs': 50, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 256}\n",
      "Final Training Loss: 10.395135879516602\n",
      "Final Validation Loss: 9.73738956451416\n",
      "Running combination 30/30: {'units3': 128, 'units2': 128, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0001, 'l2_lambda': 0.01, 'epochs': 200, 'dropout_rate': 0.4, 'clipnorm': 5.0, 'batch_size': 32}\n",
      "Final Training Loss: 4.373569965362549\n",
      "Final Validation Loss: 2.801323175430298\n",
      "Top results:\n",
      "{'params': {'units3': 64, 'units2': 32, 'units1': 32, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.0001, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.3, 'clipnorm': 1.0, 'batch_size': 32}, 'final_train_loss': 1.2618343830108643, 'final_val_loss': 0.34231215715408325}\n",
      "{'params': {'units3': 32, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 0, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.2, 'clipnorm': 5.0, 'batch_size': 120}, 'final_train_loss': 0.6296482682228088, 'final_val_loss': 0.3589296340942383}\n",
      "{'params': {'units3': 32, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-06, 'learning_rate': 0.001, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 256}, 'final_train_loss': 1.4383416175842285, 'final_val_loss': 0.366670697927475}\n",
      "{'params': {'units3': 32, 'units2': 32, 'units1': 128, 'recurrent_dropout': 0.2, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0005, 'l2_lambda': 0.001, 'epochs': 100, 'dropout_rate': 0.4, 'clipnorm': 1.0, 'batch_size': 120}, 'final_train_loss': 1.7895896434783936, 'final_val_loss': 0.3744671642780304}\n",
      "{'params': {'units3': 64, 'units2': 64, 'units1': 64, 'recurrent_dropout': 0.1, 'optimizer': 'adam', 'learning_rate_decay': 1e-05, 'learning_rate': 0.0001, 'l2_lambda': 0.001, 'epochs': 50, 'dropout_rate': 0.3, 'clipnorm': 5.0, 'batch_size': 64}, 'final_train_loss': 1.5169789791107178, 'final_val_loss': 0.3970092535018921}\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'dropout_rate': [0.2, 0.3, 0.4],\n",
    "    'recurrent_dropout': [0.1, 0.2],\n",
    "    'l2_lambda': [0.001, 0.01, 0.1],\n",
    "    'learning_rate': [0.001, 0.0005, 0.0001],\n",
    "    'learning_rate_decay': [1e-6, 1e-5, 0],\n",
    "    'units1': [32, 64, 128],\n",
    "    'units2': [32, 64, 128],\n",
    "    'units3': [32, 64, 128],\n",
    "    'batch_size': [32, 64, 120, 256],  \n",
    "    'epochs': [50, 100, 200],\n",
    "    'optimizer': ['adam'],\n",
    "    'clipnorm': [1.0, 5.0]\n",
    "}\n",
    "\n",
    "# Generate a list of random combinations with ParameterSampler\n",
    "n_iter_search = 30\n",
    "random_combinations = list(ParameterSampler(param_grid, n_iter=n_iter_search, random_state=42))\n",
    "\n",
    "# Define the function to build the model with variable parameters\n",
    "def build_model(dropout_rate, recurrent_dropout, l2_lambda, learning_rate, learning_rate_decay, clipnorm, units1, units2, units3, optimizer_name, loss_function):\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=units1, return_sequences=True, \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "                   kernel_regularizer=l2(l2_lambda), recurrent_dropout=recurrent_dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Second LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=units2, return_sequences=True, kernel_regularizer=l2(l2_lambda), recurrent_dropout=recurrent_dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Third LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=units3, return_sequences=False, kernel_regularizer=l2(l2_lambda), recurrent_dropout=recurrent_dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer.\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Select optimizer with decay and clipping.\n",
    "    optimizer = Adam(learning_rate=learning_rate, decay=learning_rate_decay, clipnorm=clipnorm)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Placeholder to keep track of results\n",
    "results = []\n",
    "\n",
    "# Run random search\n",
    "for i, params in enumerate(random_combinations):\n",
    "    print(f\"Running combination {i+1}/{len(random_combinations)}: {params}\")\n",
    "    \n",
    "    # Build model with the current parameters\n",
    "    model = build_model(\n",
    "        dropout_rate=params['dropout_rate'],\n",
    "        recurrent_dropout=params['recurrent_dropout'],\n",
    "        l2_lambda=params['l2_lambda'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        learning_rate_decay=params['learning_rate_decay'],\n",
    "        clipnorm=params['clipnorm'],\n",
    "        units1=params['units1'],\n",
    "        units2=params['units2'],\n",
    "        units3=params['units3'],\n",
    "        optimizer_name=params['optimizer'],\n",
    "        loss_function='mean_squared_error'\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq, \n",
    "        epochs=params['epochs'], \n",
    "        batch_size=params['batch_size'], \n",
    "        validation_data=(X_test_seq, y_test_seq), \n",
    "        callbacks=[early_stopping],\n",
    "        shuffle=False,\n",
    "        verbose=0  # Set verbose=0 to reduce output\n",
    "    )\n",
    "    \n",
    "    # Get final validation loss and training loss\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    \n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'params': params,\n",
    "        'final_train_loss': final_train_loss,\n",
    "        'final_val_loss': final_val_loss\n",
    "    })\n",
    "    \n",
    "    print(f\"Final Training Loss: {final_train_loss}\")\n",
    "    print(f\"Final Validation Loss: {final_val_loss}\")\n",
    "\n",
    "results = sorted(results, key=lambda x: x['final_val_loss'])\n",
    "print(\"Top results:\")\n",
    "for res in results[:5]:  # Show top 5 results\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e4003031-897c-423e-b4b0-0a65e1ac0eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 136ms/step - loss: 2.2972 - val_loss: 0.3032\n",
      "Epoch 2/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 1.1532 - val_loss: 0.3000\n",
      "Epoch 3/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 1.2281 - val_loss: 0.3031\n",
      "Epoch 4/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 1.1740 - val_loss: 0.3025\n",
      "Epoch 5/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.9092 - val_loss: 0.3018\n",
      "Epoch 6/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.8365 - val_loss: 0.3001\n",
      "Epoch 7/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.7710 - val_loss: 0.2983\n",
      "Epoch 8/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.8113 - val_loss: 0.2979\n",
      "Epoch 9/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.7847 - val_loss: 0.2988\n",
      "Epoch 10/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.7191 - val_loss: 0.2967\n",
      "Epoch 11/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.7467 - val_loss: 0.2979\n",
      "Epoch 12/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.6462 - val_loss: 0.2963\n",
      "Epoch 13/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.6524 - val_loss: 0.2964\n",
      "Epoch 14/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.6267 - val_loss: 0.2953\n",
      "Epoch 15/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.6458 - val_loss: 0.2959\n",
      "Epoch 16/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.6363 - val_loss: 0.2954\n",
      "Epoch 17/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.6577 - val_loss: 0.2945\n",
      "Epoch 18/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.5871 - val_loss: 0.2950\n",
      "Epoch 19/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.6874 - val_loss: 0.2947\n",
      "Epoch 20/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - loss: 0.6106 - val_loss: 0.2925\n",
      "Epoch 21/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.5374 - val_loss: 0.2932\n",
      "Epoch 22/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.5547 - val_loss: 0.2909\n",
      "Epoch 23/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.5679 - val_loss: 0.2905\n",
      "Epoch 24/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.5464 - val_loss: 0.2899\n",
      "Epoch 25/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.4988 - val_loss: 0.2891\n",
      "Epoch 26/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.5014 - val_loss: 0.2893\n",
      "Epoch 27/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.4933 - val_loss: 0.2901\n",
      "Epoch 28/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.5093 - val_loss: 0.2900\n",
      "Epoch 29/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.4824 - val_loss: 0.2890\n",
      "Epoch 30/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.4803 - val_loss: 0.2900\n",
      "Epoch 31/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.4342 - val_loss: 0.2905\n",
      "Epoch 32/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.4497 - val_loss: 0.2932\n",
      "Epoch 33/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.4417 - val_loss: 0.2908\n",
      "Epoch 34/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.4410 - val_loss: 0.2927\n",
      "Epoch 35/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.4262 - val_loss: 0.2931\n",
      "Epoch 36/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.4271 - val_loss: 0.3028\n",
      "Epoch 37/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.4433 - val_loss: 0.2993\n",
      "Epoch 38/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.4603 - val_loss: 0.3051\n",
      "Epoch 39/100\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.4199 - val_loss: 0.3019\n",
      "Final Training Loss: 0.4192259907722473\n",
      "Final Validation Loss: 0.3019464612007141\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Best hyperparameters found from tuning\n",
    "best_params = {\n",
    "    'units1': 32,\n",
    "    'units2': 32,\n",
    "    'units3': 128,\n",
    "    'recurrent_dropout': 0.2,\n",
    "    'optimizer': 'adam',\n",
    "    'loss_function': 'mean_squared_error',\n",
    "    'learning_rate_decay': 0,\n",
    "    'learning_rate': 0.001,\n",
    "    'l2_lambda': 0.001,\n",
    "    'epochs': 100,\n",
    "    'dropout_rate': 0.2,\n",
    "    'clipnorm': 5.0,\n",
    "    'batch_size': 120\n",
    "}\n",
    "\n",
    "# Build the model using the best parameters\n",
    "def build_best_model(params):\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units1'], return_sequences=True, \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "                   kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Second LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units2'], return_sequences=True, kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Third LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units3'], return_sequences=False, kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Select the optimizer\n",
    "    if params['optimizer'] == 'adam':\n",
    "        optimizer = Adam(learning_rate=params['learning_rate'], decay=params['learning_rate_decay'], clipnorm=params['clipnorm'])\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizer '{params['optimizer']}' is not implemented in this setup.\")\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate the model with the best hyperparameters\n",
    "model = build_best_model(best_params)\n",
    "\n",
    "# Early stopping to stop training when validation loss stops improving\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq, \n",
    "    epochs=best_params['epochs'], \n",
    "    batch_size=best_params['batch_size'], \n",
    "    validation_data=(X_test_seq, y_test_seq), \n",
    "    shuffle=False,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Print the final training and validation loss\n",
    "print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "60c09f6a-68c2-496b-a1f9-30beb3f26bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "LSTM Model Performance:\n",
      "Training RMSE: 0.020703937886024152\n",
      "Test RMSE: 0.025596058689768005\n",
      "Training MAE: 0.015075211381962031\n",
      "Test MAE: 0.020021269906603724\n",
      "Directional Accuracy on Training Data: 54.921020656136086%\n",
      "Directional Accuracy on Test Data: 53.92156862745098%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict on both training and test sequences\n",
    "y_train_pred = model.predict(X_train_seq)\n",
    "y_test_pred = model.predict(X_test_seq)\n",
    "\n",
    "# Inverse scale predictions and actual values to original scale\n",
    "y_train_pred_rescaled = scaler_y.inverse_transform(y_train_pred)\n",
    "y_test_pred_rescaled = scaler_y.inverse_transform(y_test_pred)\n",
    "y_train_actual_rescaled = scaler_y.inverse_transform(y_train_seq)\n",
    "y_test_actual_rescaled = scaler_y.inverse_transform(y_test_seq)\n",
    "\n",
    "# Define a function for directional accuracy\n",
    "def directional_accuracy(y_true, y_pred):\n",
    "    return np.mean((np.sign(y_true[1:] - y_true[:-1]) == np.sign(y_pred[1:] - y_pred[:-1])).astype(int)) * 100\n",
    "\n",
    "# Calculate metrics.\n",
    "# Root Mean Squared Error (RMSE) for training set\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_actual_rescaled, y_train_pred_rescaled))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_actual_rescaled, y_test_pred_rescaled)) # RMSE for test set.\n",
    "train_mae = mean_absolute_error(y_train_actual_rescaled, y_train_pred_rescaled) # Mean Absolute Error (MAE) for training set.\n",
    "test_mae = mean_absolute_error(y_test_actual_rescaled, y_test_pred_rescaled) # MAE for test set.\n",
    "# Directional accuracy for training set.\n",
    "train_directional_accuracy = directional_accuracy(y_train_actual_rescaled.flatten(), y_train_pred_rescaled.flatten())\n",
    "# Directional accuracy for test set.\n",
    "test_directional_accuracy = directional_accuracy(y_test_actual_rescaled.flatten(), y_test_pred_rescaled.flatten())\n",
    "\n",
    "# Display score metrics to evaluate the model's performance.\n",
    "print(\"LSTM Model Performance:\")\n",
    "print(f\"Training RMSE: {train_rmse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Training MAE: {train_mae}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Directional Accuracy on Training Data: {train_directional_accuracy}%\")\n",
    "print(f\"Directional Accuracy on Test Data: {test_directional_accuracy}%\")\n",
    "\n",
    "# Plot training & validation loss values, it will help visualize how well the model fits the data over training epochs.\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172547a5-7042-477a-934e-a313d2229dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1612d5e-c65b-4f18-a9c3-f6bb98f1a5ac",
   "metadata": {},
   "source": [
    "### 2. Optuna (Model 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8188b2df-0143-445f-8193-138fafc91758",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 23:37:04,680] A new study created in memory with name: no-name-b38a2585-1f90-44b6-baa6-e768df90429a\n",
      "[I 2024-11-13 23:37:14,004] Trial 0 finished with value: 0.5938663482666016 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.2, 'l2_lambda': 0.0037550037026405905, 'learning_rate': 0.0001, 'learning_rate_decay': 1e-05, 'clipnorm': 1.0, 'units': 32, 'num_layers': 3, 'epochs': 100, 'batch_size': 64}. Best is trial 0 with value: 0.5938663482666016.\n",
      "[I 2024-11-13 23:37:25,792] Trial 1 finished with value: 2.5742692947387695 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.2, 'l2_lambda': 0.04641587067816149, 'learning_rate': 0.0001, 'learning_rate_decay': 1e-05, 'clipnorm': 1.0, 'units': 32, 'num_layers': 2, 'epochs': 50, 'batch_size': 256}. Best is trial 0 with value: 0.5938663482666016.\n",
      "[I 2024-11-13 23:38:31,915] Trial 2 finished with value: 2.628880500793457 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.2, 'l2_lambda': 0.02754390996680696, 'learning_rate': 0.0001, 'learning_rate_decay': 1e-05, 'clipnorm': 1.0, 'units': 128, 'num_layers': 2, 'epochs': 100, 'batch_size': 120}. Best is trial 0 with value: 0.5938663482666016.\n",
      "[I 2024-11-13 23:39:12,772] Trial 3 finished with value: 0.38714301586151123 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.2, 'l2_lambda': 0.06953241045943279, 'learning_rate': 0.0001, 'learning_rate_decay': 1e-05, 'clipnorm': 1.0, 'units': 128, 'num_layers': 1, 'epochs': 150, 'batch_size': 256}. Best is trial 3 with value: 0.38714301586151123.\n",
      "[I 2024-11-13 23:39:20,283] Trial 4 finished with value: 0.25469544529914856 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.1, 'l2_lambda': 0.07320157289972352, 'learning_rate': 0.0005, 'learning_rate_decay': 1e-05, 'clipnorm': 1.0, 'units': 64, 'num_layers': 1, 'epochs': 150, 'batch_size': 32}. Best is trial 4 with value: 0.25469544529914856.\n",
      "[I 2024-11-13 23:40:04,466] Trial 5 finished with value: 0.2664451599121094 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.2, 'l2_lambda': 0.06621901977279396, 'learning_rate': 0.0005, 'learning_rate_decay': 0, 'clipnorm': 1.0, 'units': 64, 'num_layers': 2, 'epochs': 150, 'batch_size': 256}. Best is trial 4 with value: 0.25469544529914856.\n",
      "[I 2024-11-13 23:41:04,151] Trial 6 finished with value: 1.7569224834442139 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0038300508438275606, 'learning_rate': 0.0001, 'learning_rate_decay': 0, 'clipnorm': 5.0, 'units': 128, 'num_layers': 3, 'epochs': 50, 'batch_size': 256}. Best is trial 4 with value: 0.25469544529914856.\n",
      "[I 2024-11-13 23:41:15,298] Trial 7 finished with value: 0.16830813884735107 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0019223481607694784, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 150, 'batch_size': 64}. Best is trial 7 with value: 0.16830813884735107.\n",
      "[I 2024-11-13 23:41:31,851] Trial 8 finished with value: 0.5513286590576172 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0017384974140603195, 'learning_rate': 0.0001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 64, 'num_layers': 3, 'epochs': 150, 'batch_size': 120}. Best is trial 7 with value: 0.16830813884735107.\n",
      "[I 2024-11-13 23:41:40,669] Trial 9 finished with value: 0.2691876292228699 and parameters: {'dropout_rate': 0.4, 'recurrent_dropout': 0.1, 'l2_lambda': 0.028980866560207673, 'learning_rate': 0.0005, 'learning_rate_decay': 1e-06, 'clipnorm': 1.0, 'units': 64, 'num_layers': 1, 'epochs': 200, 'batch_size': 120}. Best is trial 7 with value: 0.16830813884735107.\n",
      "[I 2024-11-13 23:41:52,422] Trial 10 finished with value: 0.16402064263820648 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0010012325682668814, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 200, 'batch_size': 64}. Best is trial 10 with value: 0.16402064263820648.\n",
      "[I 2024-11-13 23:42:07,159] Trial 11 finished with value: 0.1622612029314041 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0011454828358668492, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 200, 'batch_size': 64}. Best is trial 11 with value: 0.1622612029314041.\n",
      "[I 2024-11-13 23:42:20,579] Trial 12 finished with value: 0.1630329191684723 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0011554791595316783, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 200, 'batch_size': 64}. Best is trial 11 with value: 0.1622612029314041.\n",
      "[I 2024-11-13 23:42:30,558] Trial 13 finished with value: 0.18354441225528717 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.008097857902138784, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 200, 'batch_size': 64}. Best is trial 11 with value: 0.1622612029314041.\n",
      "[I 2024-11-13 23:43:29,915] Trial 14 finished with value: 0.2661224603652954 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0010494090688129005, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 2, 'epochs': 200, 'batch_size': 64}. Best is trial 11 with value: 0.1622612029314041.\n",
      "[I 2024-11-13 23:43:39,871] Trial 15 finished with value: 0.17374856770038605 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.008178069238115187, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 200, 'batch_size': 32}. Best is trial 11 with value: 0.1622612029314041.\n",
      "[I 2024-11-13 23:43:46,777] Trial 16 finished with value: 0.3561486005783081 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0034273241341765317, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 32, 'num_layers': 2, 'epochs': 200, 'batch_size': 64}. Best is trial 11 with value: 0.1622612029314041.\n",
      "[I 2024-11-13 23:43:56,861] Trial 17 finished with value: 0.17072701454162598 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0021937683714059186, 'learning_rate': 0.001, 'learning_rate_decay': 0, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 100, 'batch_size': 64}. Best is trial 11 with value: 0.1622612029314041.\n",
      "[I 2024-11-13 23:44:06,776] Trial 18 finished with value: 0.19405688345432281 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0129316545365617, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 200, 'batch_size': 64}. Best is trial 11 with value: 0.1622612029314041.\n",
      "[I 2024-11-13 23:44:40,003] Trial 19 finished with value: 0.19023051857948303 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0015110031084659837, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 32, 'num_layers': 2, 'epochs': 150, 'batch_size': 32}. Best is trial 11 with value: 0.1622612029314041.\n",
      "[I 2024-11-13 23:46:12,368] Trial 20 finished with value: 0.1862986534833908 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.1, 'l2_lambda': 0.005616343791737484, 'learning_rate': 0.001, 'learning_rate_decay': 0, 'clipnorm': 5.0, 'units': 128, 'num_layers': 2, 'epochs': 200, 'batch_size': 64}. Best is trial 11 with value: 0.1622612029314041.\n",
      "[I 2024-11-13 23:46:22,090] Trial 21 finished with value: 0.16351181268692017 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0010046340599894252, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 200, 'batch_size': 64}. Best is trial 11 with value: 0.1622612029314041.\n",
      "[I 2024-11-13 23:46:31,643] Trial 22 finished with value: 0.17423775792121887 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0026891631353326622, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 200, 'batch_size': 64}. Best is trial 11 with value: 0.1622612029314041.\n",
      "[I 2024-11-13 23:46:41,045] Trial 23 finished with value: 0.16490621864795685 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.001325732956047539, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 200, 'batch_size': 64}. Best is trial 11 with value: 0.1622612029314041.\n",
      "[I 2024-11-13 23:46:52,734] Trial 24 finished with value: 0.161590576171875 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0010193813003283602, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 150, 'batch_size': 64}. Best is trial 24 with value: 0.161590576171875.\n",
      "[I 2024-11-13 23:47:04,637] Trial 25 finished with value: 0.1697656810283661 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0021357695554007445, 'learning_rate': 0.0005, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 150, 'batch_size': 64}. Best is trial 24 with value: 0.161590576171875.\n",
      "[I 2024-11-13 23:47:15,062] Trial 26 finished with value: 0.16607779264450073 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.2, 'l2_lambda': 0.0014483802438908899, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 100, 'batch_size': 64}. Best is trial 24 with value: 0.161590576171875.\n",
      "[I 2024-11-13 23:48:09,166] Trial 27 finished with value: 0.19069549441337585 and parameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.013403924059505022, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 2, 'epochs': 150, 'batch_size': 32}. Best is trial 24 with value: 0.161590576171875.\n",
      "[I 2024-11-13 23:48:13,933] Trial 28 finished with value: 0.20070971548557281 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.1, 'l2_lambda': 0.002515855607097069, 'learning_rate': 0.001, 'learning_rate_decay': 0, 'clipnorm': 5.0, 'units': 32, 'num_layers': 1, 'epochs': 150, 'batch_size': 120}. Best is trial 24 with value: 0.161590576171875.\n",
      "[I 2024-11-13 23:49:05,011] Trial 29 finished with value: 0.45196351408958435 and parameters: {'dropout_rate': 0.30000000000000004, 'recurrent_dropout': 0.2, 'l2_lambda': 0.004509518771235155, 'learning_rate': 0.0005, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 64, 'num_layers': 3, 'epochs': 100, 'batch_size': 64}. Best is trial 24 with value: 0.161590576171875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'dropout_rate': 0.2, 'recurrent_dropout': 0.1, 'l2_lambda': 0.0010193813003283602, 'learning_rate': 0.001, 'learning_rate_decay': 1e-06, 'clipnorm': 5.0, 'units': 128, 'num_layers': 1, 'epochs': 150, 'batch_size': 64}\n",
      "Best validation loss: 0.161590576171875\n"
     ]
    }
   ],
   "source": [
    "# Set global random state for reproducibility.\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# Optuna's TPESampler with a fixed random seed for reproducibility in parameter search.\n",
    "sampler = TPESampler(seed=random_seed)\n",
    "\n",
    "# Create an Optuna study with direction \"minimize\" to minimize the validation loss.\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "\n",
    "# Define the objective function for hyperparameter optimization.\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna to optimize hyperparameters of an LSTM model.\n",
    "    Takes a trial object from Optuna and returns the validation loss of the model with given parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define hyperparameters to tune.\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.2, 0.4, step=0.1)  # Dropout rate for LSTM layers.\n",
    "    recurrent_dropout = trial.suggest_categorical(\"recurrent_dropout\", [0.1, 0.2])  # Recurrent dropout rate for LSTM layers.\n",
    "    l2_lambda = trial.suggest_loguniform(\"l2_lambda\", 1e-3, 1e-1)  # L2 regularization factor.\n",
    "    learning_rate = trial.suggest_categorical(\"learning_rate\", [0.001, 0.0005, 0.0001])  # Learning rate for the optimizer.\n",
    "    learning_rate_decay = trial.suggest_categorical(\"learning_rate_decay\", [1e-6, 1e-5, 0])  # Learning rate decay.\n",
    "    clipnorm = trial.suggest_categorical(\"clipnorm\", [1.0, 5.0])  # Gradient clipping norm.\n",
    "    units = trial.suggest_categorical(\"units\", [32, 64, 128])  # Number of units in LSTM layers.\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)  # Number of LSTM layers.\n",
    "    epochs = trial.suggest_int(\"epochs\", 50, 200, step=50)  # Number of epochs.\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 120, 256])  # Batch size.\n",
    "    \n",
    "    # Initialize the Sequential model.\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add LSTM layers based on num_layers.\n",
    "    for i in range(num_layers):\n",
    "        # Set return_sequences=True for all but the last LSTM layer.\n",
    "        return_sequences = (i < num_layers - 1)\n",
    "        \n",
    "        # Add LSTM layer with specified hyperparameters.\n",
    "        model.add(LSTM(\n",
    "            units=units,\n",
    "            return_sequences=return_sequences,\n",
    "            input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]) if i == 0 else None,  # Set input shape only for the first layer.\n",
    "            kernel_regularizer=l2(l2_lambda),\n",
    "            recurrent_dropout=recurrent_dropout\n",
    "        ))\n",
    "\n",
    "        # Add BatchNormalization and Dropout after each LSTM layer.\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Add the output layer with a single unit (regression).\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Initialize the optimizer with learning rate, decay, and gradient clipping norm.\n",
    "    optimizer = Adam(learning_rate=learning_rate, decay=learning_rate_decay, clipnorm=clipnorm)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Early stopping callback to avoid overfitting.\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model with given hyperparameters.\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test_seq, y_test_seq),\n",
    "        callbacks=[early_stopping],\n",
    "        shuffle=False,\n",
    "        verbose=0  # Set verbose=0 to suppress training logs for faster experimentation.\n",
    "    )\n",
    "\n",
    "    # Retrieve the minimum validation loss from the training history.\n",
    "    val_loss = min(history.history['val_loss'])\n",
    "    \n",
    "    # Return the validation loss to be minimized by Optuna.\n",
    "    return val_loss\n",
    "\n",
    "# Run the Optuna study for a given number of trials.\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Print the best parameters found by the study.\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "# Print the best validation loss achieved with the optimal parameters.\n",
    "print(\"Best validation loss:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "380b21c6-5a69-48bb-bb94-72d833ba78d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 235ms/step - loss: 0.3138 - val_loss: 0.1680\n",
      "Epoch 2/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - loss: 0.2122 - val_loss: 0.1687\n",
      "Epoch 3/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.2021 - val_loss: 0.1664\n",
      "Epoch 4/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - loss: 0.1955 - val_loss: 0.1668\n",
      "Epoch 5/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.2148 - val_loss: 0.1662\n",
      "Epoch 6/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - loss: 0.2034 - val_loss: 0.1707\n",
      "Epoch 7/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - loss: 0.1852 - val_loss: 0.1658\n",
      "Epoch 8/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 0.1670 - val_loss: 0.1653\n",
      "Epoch 9/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - loss: 0.1648 - val_loss: 0.1644\n",
      "Epoch 10/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - loss: 0.1606 - val_loss: 0.1648\n",
      "Epoch 11/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 0.1633 - val_loss: 0.1646\n",
      "Epoch 12/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - loss: 0.1575 - val_loss: 0.1638\n",
      "Epoch 13/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.1438 - val_loss: 0.1673\n",
      "Epoch 14/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.1519 - val_loss: 0.1632\n",
      "Epoch 15/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - loss: 0.1409 - val_loss: 0.1649\n",
      "Epoch 16/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.1406 - val_loss: 0.1635\n",
      "Epoch 17/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 0.1273 - val_loss: 0.1666\n",
      "Epoch 18/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 0.1275 - val_loss: 0.1756\n",
      "Epoch 19/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - loss: 0.1342 - val_loss: 0.1745\n",
      "Epoch 20/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - loss: 0.1249 - val_loss: 0.1839\n",
      "Epoch 21/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - loss: 0.1263 - val_loss: 0.1908\n",
      "Epoch 22/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.1148 - val_loss: 0.1914\n",
      "Epoch 23/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.1265 - val_loss: 0.2002\n",
      "Epoch 24/150\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - loss: 0.1219 - val_loss: 0.1809\n",
      "Final Training Loss: 0.12591534852981567\n",
      "Final Validation Loss: 0.1809389591217041\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Best hyperparameters found from tuning\n",
    "best_params = {\n",
    "    'units1': 128,\n",
    "    'recurrent_dropout': 0.1,\n",
    "    'optimizer': 'adam',\n",
    "    'loss_function': 'mean_squared_error',\n",
    "    'learning_rate_decay': 1e-06,\n",
    "    'learning_rate': 0.001,\n",
    "    'l2_lambda': 0.0010193813003283602,\n",
    "    'epochs': 150,\n",
    "    'dropout_rate': 0.2,\n",
    "    'clipnorm': 5.0,\n",
    "    'batch_size': 64\n",
    "}\n",
    "\n",
    "# Build the model using the best parameters\n",
    "def build_best_model(params):\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units1'], return_sequences=False, \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "                   kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "     # Select the optimizer with specified parameters\n",
    "    if params['optimizer'] == 'adam':\n",
    "        optimizer = Adam(learning_rate=params['learning_rate'], decay=params['learning_rate_decay'], clipnorm=params['clipnorm'])\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizer '{params['optimizer']}' is not implemented in this setup.\")\n",
    "\n",
    "    # Select the loss function based on tuning results.\n",
    "    if params['loss_function'] == 'huber_loss':\n",
    "        loss = Huber()  # Huber loss is more robust to outliers\n",
    "    else:\n",
    "        loss = MeanSquaredError()  # Use MSE as default loss function for regression\n",
    "    \n",
    "    # Compile the model with the specified optimizer and loss function\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate the model with the best hyperparameters.\n",
    "model = build_best_model(best_params)\n",
    "\n",
    "# Early stopping to stop training when validation loss stops improving.\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping.\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq, \n",
    "    epochs=best_params['epochs'], \n",
    "    batch_size=best_params['batch_size'], \n",
    "    validation_data=(X_test_seq, y_test_seq), \n",
    "    shuffle=False,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Print the final training and validation loss.\n",
    "print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d413bd7e-3423-4058-8025-9bfab8c8df47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step \n",
      "LSTM Model Performance:\n",
      "Training RMSE: 0.020671718128507245\n",
      "Test RMSE: 0.02560304501407214\n",
      "Training MAE: 0.0148340448185367\n",
      "Test MAE: 0.019921678957826334\n",
      "Directional Accuracy on Training Data: 67.43620899149452%\n",
      "Directional Accuracy on Test Data: 65.68627450980392%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict on both training and test sequences\n",
    "y_train_pred = model.predict(X_train_seq)\n",
    "y_test_pred = model.predict(X_test_seq)\n",
    "\n",
    "# Inverse scale predictions and actual values to original scale\n",
    "y_train_pred_rescaled = scaler_y.inverse_transform(y_train_pred)\n",
    "y_test_pred_rescaled = scaler_y.inverse_transform(y_test_pred)\n",
    "y_train_actual_rescaled = scaler_y.inverse_transform(y_train_seq)\n",
    "y_test_actual_rescaled = scaler_y.inverse_transform(y_test_seq)\n",
    "\n",
    "# Define a function for directional accuracy\n",
    "def directional_accuracy(y_true, y_pred):\n",
    "    return np.mean((np.sign(y_true[1:] - y_true[:-1]) == np.sign(y_pred[1:] - y_pred[:-1])).astype(int)) * 100\n",
    "\n",
    "# Calculate metrics.\n",
    "# Root Mean Squared Error (RMSE) for training set\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_actual_rescaled, y_train_pred_rescaled))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_actual_rescaled, y_test_pred_rescaled)) # RMSE for test set.\n",
    "train_mae = mean_absolute_error(y_train_actual_rescaled, y_train_pred_rescaled) # Mean Absolute Error (MAE) for training set.\n",
    "test_mae = mean_absolute_error(y_test_actual_rescaled, y_test_pred_rescaled) # MAE for test set.\n",
    "# Directional accuracy for training set.\n",
    "train_directional_accuracy = directional_accuracy(y_train_actual_rescaled.flatten(), y_train_pred_rescaled.flatten())\n",
    "# Directional accuracy for test set.\n",
    "test_directional_accuracy = directional_accuracy(y_test_actual_rescaled.flatten(), y_test_pred_rescaled.flatten())\n",
    "\n",
    "# Display score metrics to evaluate the model's performance.\n",
    "print(\"LSTM Model Performance:\")\n",
    "print(f\"Training RMSE: {train_rmse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Training MAE: {train_mae}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Directional Accuracy on Training Data: {train_directional_accuracy}%\")\n",
    "print(f\"Directional Accuracy on Test Data: {test_directional_accuracy}%\")\n",
    "\n",
    "# Plot training & validation loss values, it will help visualize how well the model fits the data over training epochs.\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0d6571-1bb4-4c9f-9430-009ff3e72156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b091176e-f6d2-4c95-88db-6f36c0123a17",
   "metadata": {},
   "source": [
    "### 3. Keras Tuner (Model 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3362f907-9b32-4fc2-9d26-c7525079ff88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 01m 12s]\n",
      "val_loss: 0.17832288146018982\n",
      "\n",
      "Best val_loss So Far: 0.15399901568889618\n",
      "Total elapsed time: 00h 27m 20s\n",
      "Built model with params: dropout_rate=0.3, recurrent_dropout=0.2, l2_lambda=0.07913979537147964, learning_rate=0.001, learning_rate_decay=1e-05, clipnorm=5.0, units=32, num_layers=2, batch_size=64\n",
      "Best hyperparameters: {'dropout_rate': 0.3, 'recurrent_dropout': 0.2, 'l2_lambda': 0.07913979537147964, 'learning_rate': 0.001, 'learning_rate_decay': 1e-05, 'clipnorm': 5.0, 'units': 32, 'num_layers': 2, 'batch_size': 64}\n",
      "Best batch size: 64\n"
     ]
    }
   ],
   "source": [
    "# Set a global random seed for reproducibility.\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# Build model function with hyperparameter choices.\n",
    "def build_model(hp): #hp (kerastuner.HyperParameters) - Hyperparameter search space.\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Define hyperparameters using `hp` for various tuning options.\n",
    "    dropout_rate = hp.Choice(\"dropout_rate\", [0.2, 0.3, 0.4])  # Dropout rates to fight overfitting.\n",
    "    recurrent_dropout = hp.Choice(\"recurrent_dropout\", [0.1, 0.2])  # Recurrent dropout for LSTM layers.\n",
    "    l2_lambda = hp.Float(\"l2_lambda\", min_value=0.001, max_value=0.1, sampling=\"log\")  # L2 regularization factor.\n",
    "    learning_rate = hp.Choice(\"learning_rate\", [0.001, 0.0005, 0.0001])  # Learning rate choices.\n",
    "    learning_rate_decay = hp.Choice(\"learning_rate_decay\", [1e-5, 0.0])  # Learning rate decay for gradual reduction.\n",
    "    clipnorm = hp.Choice(\"clipnorm\", [1.0, 5.0])  # Gradient clipping norm to prevent exploding gradients.\n",
    "    units = hp.Choice(\"units\", [32, 64, 128])  # Number of units for LSTM layers.\n",
    "    num_layers = hp.Int(\"num_layers\", 1, 3)  # Number of LSTM layers.\n",
    "    batch_size = hp.Choice(\"batch_size\", [32, 64, 120, 256])  # Batch size choices.\n",
    "\n",
    "    # Add LSTM layers based on the number of layers selected.\n",
    "    for i in range(num_layers):\n",
    "        return_sequences = (i < num_layers - 1)\n",
    "        model.add(LSTM(units=units, return_sequences=return_sequences,\n",
    "                       input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]) if i == 0 else None,\n",
    "                       kernel_regularizer=l2(l2_lambda), recurrent_dropout=recurrent_dropout))\n",
    "        model.add(BatchNormalization()) # Add batch normalization to stabilize training.\n",
    "        model.add(Dropout(dropout_rate))  # Add dropout to help with overfitting.\n",
    "\n",
    "    model.add(Dense(1))  # Output layer for a single continuous value.\n",
    "    # Configure optimizer with learning rate, decay, and gradient clipping.\n",
    "    optimizer = Adam(learning_rate=learning_rate, decay=learning_rate_decay, clipnorm=clipnorm)\n",
    "     # Compile model with specified optimizer and mean squared error loss.\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    print(f\"Built model with params: dropout_rate={dropout_rate}, recurrent_dropout={recurrent_dropout}, \"\n",
    "          f\"l2_lambda={l2_lambda}, learning_rate={learning_rate}, learning_rate_decay={learning_rate_decay}, \"\n",
    "          f\"clipnorm={clipnorm}, units={units}, num_layers={num_layers}, batch_size={batch_size}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Set up Bayesian Optimization tuner to search for optimal hyperparameters.\n",
    "tuner = BayesianOptimization(\n",
    "    build_model,  # Model-building function.\n",
    "    objective=\"val_loss\",  # Target metric for optimization.\n",
    "    max_trials=30,  # Maximum number of trials to run.\n",
    "    executions_per_trial=1,  # Number of times to execute each trial.\n",
    "    directory=\"tuner_dir\",  # Directory to store tuning results.\n",
    "    project_name=\"lstm_tuning_capstone\",  # Tuning project name.\n",
    "    overwrite=True  # Overwrite existing tuner results if present.\n",
    ")\n",
    "\n",
    "# Define early stopping.\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=True)\n",
    "\n",
    "# Perform tuning with verbose logging.\n",
    "tuner.search(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_test_seq, y_test_seq),\n",
    "    epochs=200,\n",
    "    callbacks=[early_stopping],\n",
    "    shuffle=False,\n",
    "    verbose=1  # Ensure output of each trial.\n",
    ")\n",
    "\n",
    "# Retrieve the best model and hyperparameters.\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Best hyperparameters:\", best_hyperparameters.values)\n",
    "print(\"Best batch size:\", best_hyperparameters.get(\"batch_size\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61f5d375-0388-4da5-b9c2-b3d32c495318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - loss: 7.1004 - val_loss: 4.6413\n",
      "Epoch 2/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 5.7887 - val_loss: 4.2101\n",
      "Epoch 3/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 5.1766 - val_loss: 3.7745\n",
      "Epoch 4/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.6594 - val_loss: 3.3858\n",
      "Epoch 5/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.1329 - val_loss: 3.0367\n",
      "Epoch 6/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.6436 - val_loss: 2.7276\n",
      "Epoch 7/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.2509 - val_loss: 2.4579\n",
      "Epoch 8/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.0655 - val_loss: 2.2231\n",
      "Epoch 9/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.7139 - val_loss: 2.0163\n",
      "Epoch 10/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.4693 - val_loss: 1.8315\n",
      "Epoch 11/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.2295 - val_loss: 1.6690\n",
      "Epoch 12/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.0407 - val_loss: 1.5280\n",
      "Epoch 13/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.8906 - val_loss: 1.4008\n",
      "Epoch 14/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.7790 - val_loss: 1.2890\n",
      "Epoch 15/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.5761 - val_loss: 1.1914\n",
      "Epoch 16/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.4853 - val_loss: 1.1042\n",
      "Epoch 17/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.4199 - val_loss: 1.0255\n",
      "Epoch 18/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.3269 - val_loss: 0.9567\n",
      "Epoch 19/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.2052 - val_loss: 0.8944\n",
      "Epoch 20/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.1633 - val_loss: 0.8390\n",
      "Epoch 21/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.0560 - val_loss: 0.7878\n",
      "Epoch 22/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.0468 - val_loss: 0.7431\n",
      "Epoch 23/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.0019 - val_loss: 0.7002\n",
      "Epoch 24/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.9095 - val_loss: 0.6630\n",
      "Epoch 25/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.8641 - val_loss: 0.6289\n",
      "Epoch 26/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.7373 - val_loss: 0.5974\n",
      "Epoch 27/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.7373 - val_loss: 0.5713\n",
      "Epoch 28/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.7327 - val_loss: 0.5450\n",
      "Epoch 29/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.6757 - val_loss: 0.5188\n",
      "Epoch 30/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.6199 - val_loss: 0.4951\n",
      "Epoch 31/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.5770 - val_loss: 0.4747\n",
      "Epoch 32/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.5571 - val_loss: 0.4561\n",
      "Epoch 33/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.5427 - val_loss: 0.4412\n",
      "Epoch 34/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.5101 - val_loss: 0.4232\n",
      "Epoch 35/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.5062 - val_loss: 0.4048\n",
      "Epoch 36/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.4606 - val_loss: 0.3899\n",
      "Epoch 37/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.4675 - val_loss: 0.3791\n",
      "Epoch 38/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.4221 - val_loss: 0.3631\n",
      "Epoch 39/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.4120 - val_loss: 0.3506\n",
      "Epoch 40/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3862 - val_loss: 0.3406\n",
      "Epoch 41/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3623 - val_loss: 0.3270\n",
      "Epoch 42/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3540 - val_loss: 0.3195\n",
      "Epoch 43/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3187 - val_loss: 0.3100\n",
      "Epoch 44/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3325 - val_loss: 0.3009\n",
      "Epoch 45/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2961 - val_loss: 0.2909\n",
      "Epoch 46/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2758 - val_loss: 0.2900\n",
      "Epoch 47/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2777 - val_loss: 0.2733\n",
      "Epoch 48/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2735 - val_loss: 0.2682\n",
      "Epoch 49/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2549 - val_loss: 0.2578\n",
      "Epoch 50/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2354 - val_loss: 0.2514\n",
      "Epoch 51/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2324 - val_loss: 0.2491\n",
      "Epoch 52/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2367 - val_loss: 0.2425\n",
      "Epoch 53/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2307 - val_loss: 0.2520\n",
      "Epoch 54/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2192 - val_loss: 0.2309\n",
      "Epoch 55/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2045 - val_loss: 0.2424\n",
      "Epoch 56/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2056 - val_loss: 0.2205\n",
      "Epoch 57/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1952 - val_loss: 0.2215\n",
      "Epoch 58/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1825 - val_loss: 0.2134\n",
      "Epoch 59/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1794 - val_loss: 0.2128\n",
      "Epoch 60/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1756 - val_loss: 0.2070\n",
      "Epoch 61/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1725 - val_loss: 0.2029\n",
      "Epoch 62/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1619 - val_loss: 0.1996\n",
      "Epoch 63/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1620 - val_loss: 0.1988\n",
      "Epoch 64/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1631 - val_loss: 0.1944\n",
      "Epoch 65/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1582 - val_loss: 0.1926\n",
      "Epoch 66/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1587 - val_loss: 0.1901\n",
      "Epoch 67/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1534 - val_loss: 0.1883\n",
      "Epoch 68/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1500 - val_loss: 0.1852\n",
      "Epoch 69/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1468 - val_loss: 0.1826\n",
      "Epoch 70/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1470 - val_loss: 0.1848\n",
      "Epoch 71/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1415 - val_loss: 0.1791\n",
      "Epoch 72/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1395 - val_loss: 0.1787\n",
      "Epoch 73/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1391 - val_loss: 0.1772\n",
      "Epoch 74/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1378 - val_loss: 0.1744\n",
      "Epoch 75/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1360 - val_loss: 0.1753\n",
      "Epoch 76/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1334 - val_loss: 0.1725\n",
      "Epoch 77/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1361 - val_loss: 0.1716\n",
      "Epoch 78/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1321 - val_loss: 0.1705\n",
      "Epoch 79/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1288 - val_loss: 0.1693\n",
      "Epoch 80/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1297 - val_loss: 0.1683\n",
      "Epoch 81/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1286 - val_loss: 0.1667\n",
      "Epoch 82/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1247 - val_loss: 0.1665\n",
      "Epoch 83/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1247 - val_loss: 0.1656\n",
      "Epoch 84/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1246 - val_loss: 0.1649\n",
      "Epoch 85/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1246 - val_loss: 0.1655\n",
      "Epoch 86/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1242 - val_loss: 0.1644\n",
      "Epoch 87/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1227 - val_loss: 0.1643\n",
      "Epoch 88/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1226 - val_loss: 0.1641\n",
      "Epoch 89/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1196 - val_loss: 0.1633\n",
      "Epoch 90/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1213 - val_loss: 0.1623\n",
      "Epoch 91/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1212 - val_loss: 0.1605\n",
      "Epoch 92/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1209 - val_loss: 0.1600\n",
      "Epoch 93/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1184 - val_loss: 0.1591\n",
      "Epoch 94/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1198 - val_loss: 0.1590\n",
      "Epoch 95/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1176 - val_loss: 0.1595\n",
      "Epoch 96/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1187 - val_loss: 0.1589\n",
      "Epoch 97/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1177 - val_loss: 0.1577\n",
      "Epoch 98/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1177 - val_loss: 0.1572\n",
      "Epoch 99/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1166 - val_loss: 0.1566\n",
      "Epoch 100/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1182 - val_loss: 0.1580\n",
      "Epoch 101/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1165 - val_loss: 0.1575\n",
      "Epoch 102/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1169 - val_loss: 0.1569\n",
      "Epoch 103/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1153 - val_loss: 0.1567\n",
      "Epoch 104/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1167 - val_loss: 0.1580\n",
      "Epoch 105/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1154 - val_loss: 0.1590\n",
      "Epoch 106/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1158 - val_loss: 0.1584\n",
      "Epoch 107/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1148 - val_loss: 0.1567\n",
      "Epoch 108/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1140 - val_loss: 0.1559\n",
      "Epoch 109/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1144 - val_loss: 0.1555\n",
      "Epoch 110/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1136 - val_loss: 0.1550\n",
      "Epoch 111/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1140 - val_loss: 0.1552\n",
      "Epoch 112/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1137 - val_loss: 0.1543\n",
      "Epoch 113/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1138 - val_loss: 0.1547\n",
      "Epoch 114/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1138 - val_loss: 0.1555\n",
      "Epoch 115/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1137 - val_loss: 0.1561\n",
      "Epoch 116/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1144 - val_loss: 0.1567\n",
      "Epoch 117/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1146 - val_loss: 0.1562\n",
      "Epoch 118/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1132 - val_loss: 0.1562\n",
      "Epoch 119/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1130 - val_loss: 0.1571\n",
      "Epoch 120/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1137 - val_loss: 0.1578\n",
      "Epoch 121/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1125 - val_loss: 0.1570\n",
      "Epoch 122/200\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1143 - val_loss: 0.1557\n",
      "Final Training Loss: 0.1171279326081276\n",
      "Final Validation Loss: 0.1557455062866211\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Best hyperparameters found from tuning\n",
    "best_params = {\n",
    "    'units1': 32,\n",
    "    'units2': 32,\n",
    "    'recurrent_dropout': 0.2,\n",
    "    'optimizer': 'adam',\n",
    "    'loss_function': 'mean_squared_error',\n",
    "    'learning_rate_decay': 1e-05,\n",
    "    'learning_rate': 0.001,\n",
    "    'l2_lambda': 0.07913979537147964,\n",
    "    'epochs': 200,\n",
    "    'dropout_rate': 0.3,\n",
    "    'clipnorm': 5.0,\n",
    "    'batch_size': 64\n",
    "}\n",
    "\n",
    "# Build the model using the best parameters\n",
    "def build_best_model(params):\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units1'], return_sequences=True, \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "                   kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Second LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units2'], return_sequences=False, kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Select the optimizer\n",
    "    if params['optimizer'] == 'adam':\n",
    "        optimizer = Adam(learning_rate=params['learning_rate'], decay=params['learning_rate_decay'], clipnorm=params['clipnorm'])\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizer '{params['optimizer']}' is not implemented in this setup.\")\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate the model with the best hyperparameters\n",
    "model = build_best_model(best_params)\n",
    "\n",
    "# Early stopping to stop training when validation loss stops improving\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq, \n",
    "    epochs=best_params['epochs'], \n",
    "    batch_size=best_params['batch_size'], \n",
    "    validation_data=(X_test_seq, y_test_seq), \n",
    "    shuffle=False,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Print the final training and validation loss\n",
    "print(f\"Final Training Loss: {history.history['loss'][-1]}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff1f53b4-34ba-4182-9694-a8b8f41ea255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "LSTM Model Performance:\n",
      "Training RMSE: 0.020611598175652383\n",
      "Test RMSE: 0.02522957816660202\n",
      "Training MAE: 0.014958895645402624\n",
      "Test MAE: 0.019810834494472664\n",
      "Directional Accuracy on Training Data: 57.95868772782503%\n",
      "Directional Accuracy on Test Data: 50.98039215686274%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict on both training and test sequences\n",
    "y_train_pred = model.predict(X_train_seq)\n",
    "y_test_pred = model.predict(X_test_seq)\n",
    "\n",
    "# Inverse scale predictions and actual values to original scale\n",
    "y_train_pred_rescaled = scaler_y.inverse_transform(y_train_pred)\n",
    "y_test_pred_rescaled = scaler_y.inverse_transform(y_test_pred)\n",
    "y_train_actual_rescaled = scaler_y.inverse_transform(y_train_seq)\n",
    "y_test_actual_rescaled = scaler_y.inverse_transform(y_test_seq)\n",
    "\n",
    "# Define a function for directional accuracy\n",
    "def directional_accuracy(y_true, y_pred):\n",
    "    return np.mean((np.sign(y_true[1:] - y_true[:-1]) == np.sign(y_pred[1:] - y_pred[:-1])).astype(int)) * 100\n",
    "\n",
    "# Calculate metrics.\n",
    "# Root Mean Squared Error (RMSE) for training set\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_actual_rescaled, y_train_pred_rescaled))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_actual_rescaled, y_test_pred_rescaled)) # RMSE for test set.\n",
    "train_mae = mean_absolute_error(y_train_actual_rescaled, y_train_pred_rescaled) # Mean Absolute Error (MAE) for training set.\n",
    "test_mae = mean_absolute_error(y_test_actual_rescaled, y_test_pred_rescaled) # MAE for test set.\n",
    "# Directional accuracy for training set.\n",
    "train_directional_accuracy = directional_accuracy(y_train_actual_rescaled.flatten(), y_train_pred_rescaled.flatten())\n",
    "# Directional accuracy for test set.\n",
    "test_directional_accuracy = directional_accuracy(y_test_actual_rescaled.flatten(), y_test_pred_rescaled.flatten())\n",
    "\n",
    "# Display score metrics to evaluate the model's performance.\n",
    "print(\"LSTM Model Performance:\")\n",
    "print(f\"Training RMSE: {train_rmse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Training MAE: {train_mae}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Directional Accuracy on Training Data: {train_directional_accuracy}%\")\n",
    "print(f\"Directional Accuracy on Test Data: {test_directional_accuracy}%\")\n",
    "\n",
    "# Plot training & validation loss values, it will help visualize how well the model fits the data over training epochs.\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe75419-e6b1-4798-9f01-7ae71162b05a",
   "metadata": {},
   "source": [
    "### Model Performance Metrics for Sequence Length = 52\n",
    "\n",
    "| Metric         | Model 1       | Model 2       | Model 3       | Model 4       |\n",
    "|-----------------|---------------|---------------|---------------|---------------|\n",
    "| **Train RMSE** | 0.019799994   | 0.020703938   | 0.020671718   | 0.020611598   |\n",
    "| **Test RMSE**  | 0.026212297   | 0.025596059   | 0.025603045   | 0.025229578   |\n",
    "| **Train MAE**  | 0.014390033   | 0.015075211   | 0.014834045   | 0.014958896   |\n",
    "| **Test MAE**   | 0.020440062   | 0.02002127    | 0.019921679   | 0.019810834   |\n",
    "| **Train Loss** | 0.153087676   | 0.419225991   | 0.125915349   | 0.117127933   |\n",
    "| **Val Loss**   | 0.193663865   | 0.301946461   | 0.180938959   | 0.155745506   |\n",
    "| **Train DA**   | 68%           | 55%           | 67%           | 58%           |\n",
    "| **Test DA**    | 58%           | 54%           | 66%           | 51%           |\n",
    "\n",
    "- For the sequence length of 52 the optimal model is the **Model 4**, the approach presented the lowest metrics for RMSE, train and val loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99ec82f4-b53f-4ba8-86d6-3b75fd0aae1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descaled Training Loss: -0.001856387540864885\n",
      "Descaled Validation Loss: 0.0004970015376326215\n"
     ]
    }
   ],
   "source": [
    "# Extract scaled train and validation losses from history\n",
    "scaled_train_loss = history.history['loss'][-1]  # Final train loss\n",
    "scaled_val_loss = history.history['val_loss'][-1]  # Final validation loss\n",
    "\n",
    "# Reshape the losses to match the expected input shape for MinMaxScaler\n",
    "scaled_train_loss_reshaped = np.array(scaled_train_loss).reshape(-1, 1)\n",
    "scaled_val_loss_reshaped = np.array(scaled_val_loss).reshape(-1, 1)\n",
    "\n",
    "# Descale train and validation losses\n",
    "descaled_train_loss = scaler_y.inverse_transform(scaled_train_loss_reshaped)[0, 0]\n",
    "descaled_val_loss = scaler_y.inverse_transform(scaled_val_loss_reshaped)[0, 0]\n",
    "\n",
    "# Print the descaled losses\n",
    "print(f\"Descaled Training Loss: {descaled_train_loss}\")\n",
    "print(f\"Descaled Validation Loss: {descaled_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f99a4b7-f06c-4400-a463-3504c2d693ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descaled Training Loss: -0.0018563875408648877\n",
      "Descaled Validation Loss: 0.0004970015376326194\n"
     ]
    }
   ],
   "source": [
    "## Extract scaled losses from history\n",
    "scaled_train_loss = history.history['loss'][-1]  # Final train loss\n",
    "scaled_val_loss = history.history['val_loss'][-1]  # Final validation loss\n",
    "\n",
    "# Retrieve the Min and Max values used in the scaler\n",
    "data_min = scaler_y.data_min_[0]  # Minimum value used for scaling\n",
    "data_max = scaler_y.data_max_[0]  # Maximum value used for scaling\n",
    "\n",
    "# Compute the descaled values manually\n",
    "descaled_train_loss = scaled_train_loss * (data_max - data_min) / 2 + (data_max + data_min) / 2\n",
    "descaled_val_loss = scaled_val_loss * (data_max - data_min) / 2 + (data_max + data_min) / 2\n",
    "\n",
    "# Print the descaled losses\n",
    "print(f\"Descaled Training Loss: {descaled_train_loss}\")\n",
    "print(f\"Descaled Validation Loss: {descaled_val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d89860fe-eb5f-4bfe-a5a0-be15d8e967cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "Fold 1 RMSE: 0.504116501115201\n",
      "Fold 2\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "Fold 2 RMSE: 0.35037178765968696\n",
      "Fold 3\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Fold 3 RMSE: 0.2814915249500041\n",
      "Fold 4\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "Fold 4 RMSE: 0.4159963505659501\n",
      "Average RMSE from TSCV: 0.3879940410727105\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random_seed = 40\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# Best hyperparameters found from tuning\n",
    "best_params = {\n",
    "    'units1': 32,\n",
    "    'units2': 32,\n",
    "    'recurrent_dropout': 0.2,\n",
    "    'optimizer': 'adam',\n",
    "    'loss_function': 'mean_squared_error',\n",
    "    'learning_rate_decay': 1e-05,\n",
    "    'learning_rate': 0.001,\n",
    "    'l2_lambda': 0.07913979537147964,\n",
    "    'epochs': 200,\n",
    "    'dropout_rate': 0.3,\n",
    "    'clipnorm': 5.0,\n",
    "    'batch_size': 64\n",
    "}\n",
    "\n",
    "# Build the model using the best parameters\n",
    "def build_best_model(params):\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units1'], return_sequences=True, \n",
    "                   input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]),\n",
    "                   kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Second LSTM layer with BatchNorm, Dropout, and recurrent_dropout\n",
    "    model.add(LSTM(units=params['units2'], return_sequences=False, kernel_regularizer=l2(params['l2_lambda']), \n",
    "                   recurrent_dropout=params['recurrent_dropout']))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Select the optimizer\n",
    "    optimizer = Adam(learning_rate=params['learning_rate'], \n",
    "                     decay=params['learning_rate_decay'], \n",
    "                     clipnorm=params['clipnorm'])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss=params['loss_function'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define the number of splits for Time Series Cross-Validation\n",
    "n_splits = 4\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Initialize the list to store RMSE scores\n",
    "tscv_rmse_scores = []\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Create the model once and save its initial weights\n",
    "model = build_best_model(best_params)\n",
    "initial_weights = model.get_weights()\n",
    "\n",
    "# Perform TSCV\n",
    "for fold, (train_index, test_index) in enumerate(tscv.split(X_train_scaled)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "    # Reset model to initial weights\n",
    "    model.set_weights(initial_weights)\n",
    "    \n",
    "    # Define train and test sets\n",
    "    train, test = X_train_scaled[train_index], X_train_scaled[test_index]\n",
    "    y_train_fold, y_test_fold = y_train_scaled[train_index], y_train_scaled[test_index]\n",
    "    \n",
    "    # Create sequences\n",
    "    X_train_seq, y_train_seq = [], []\n",
    "    for i in range(len(train) - sequence_length):\n",
    "        X_train_seq.append(train[i:i + sequence_length])\n",
    "        y_train_seq.append(y_train_fold[i + sequence_length])\n",
    "    \n",
    "    X_test_seq, y_test_seq = [], []\n",
    "    for i in range(len(test) - sequence_length):\n",
    "        X_test_seq.append(test[i:i + sequence_length])\n",
    "        y_test_seq.append(y_test_fold[i + sequence_length])\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_train_seq, y_train_seq = np.array(X_train_seq), np.array(y_train_seq)\n",
    "    X_test_seq, y_test_seq = np.array(X_test_seq), np.array(y_test_seq)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=best_params['epochs'],\n",
    "        batch_size=best_params['batch_size'],\n",
    "        validation_data=(X_test_seq, y_test_seq),\n",
    "        shuffle=False,\n",
    "        verbose=0,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = model.predict(X_test_seq)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_seq, y_pred))\n",
    "    tscv_rmse_scores.append(rmse)\n",
    "    print(f\"Fold {fold + 1} RMSE: {rmse}\")\n",
    "\n",
    "# Calculate the average RMSE across all folds\n",
    "avg_rmse = np.mean(tscv_rmse_scores)\n",
    "print(f\"Average RMSE from TSCV: {avg_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f7b0da-ea1d-42eb-9475-88ef22ba16c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling window starting at index 0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step\n",
      "Rolling window RMSE: 0.3350777160441686\n",
      "Rolling window starting at index 53\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n",
      "Rolling window RMSE: 0.4008095926140036\n",
      "Rolling window starting at index 106\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step\n",
      "Rolling window RMSE: 0.30655856151243666\n",
      "Rolling window starting at index 159\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002DF53C014E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "Rolling window RMSE: 0.2460107838467971\n",
      "Rolling window starting at index 212\n",
      "WARNING:tensorflow:6 out of the last 18 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002DF57B8A480> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "Rolling window RMSE: 0.1634210741398854\n",
      "Rolling window starting at index 265\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step\n",
      "Rolling window RMSE: 0.36582332838996773\n",
      "Rolling window starting at index 318\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "Rolling window RMSE: 0.3291968926279718\n",
      "Rolling window starting at index 371\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step\n",
      "Rolling window RMSE: 0.20821751575584208\n",
      "Rolling window starting at index 424\n"
     ]
    }
   ],
   "source": [
    "# Parameters for FRWCV\n",
    "train_window = 300  # Training window size\n",
    "test_window = 53    # Test window size\n",
    "\n",
    "# Store RMSEs for each window\n",
    "rolling_rmse_scores = []\n",
    "\n",
    "for start in range(0, len(X_train_scaled) - train_window - test_window, test_window):\n",
    "    print(f\"Rolling window starting at index {start}\")\n",
    "    \n",
    "    # Define train and test sets for the window\n",
    "    train = X_train_scaled[start:start + train_window]\n",
    "    test = X_train_scaled[start + train_window:start + train_window + test_window]\n",
    "    y_train_fold = y_train_scaled[start:start + train_window]\n",
    "    y_test_fold = y_train_scaled[start + train_window:start + train_window + test_window]\n",
    "    \n",
    "    # Create sequences for train and test\n",
    "    X_train_seq, y_train_seq = [], []\n",
    "    for i in range(len(train) - sequence_length):\n",
    "        X_train_seq.append(train[i:i + sequence_length])\n",
    "        y_train_seq.append(y_train_fold[i + sequence_length])\n",
    "    \n",
    "    X_test_seq, y_test_seq = [], []\n",
    "    for i in range(len(test) - sequence_length):\n",
    "        X_test_seq.append(test[i:i + sequence_length])\n",
    "        y_test_seq.append(y_test_fold[i + sequence_length])\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_train_seq, y_train_seq = np.array(X_train_seq), np.array(y_train_seq)\n",
    "    X_test_seq, y_test_seq = np.array(X_test_seq), np.array(y_test_seq)\n",
    "    \n",
    "    # Build and train the model\n",
    "    model = build_best_model(best_params)\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq, \n",
    "        epochs=best_params['epochs'], \n",
    "        batch_size=best_params['batch_size'], \n",
    "        validation_data=(X_test_seq, y_test_seq), \n",
    "        shuffle=False,\n",
    "        verbose=0,  # Suppress training logs for brevity\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    y_pred = model.predict(X_test_seq)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_seq, y_pred))\n",
    "    rolling_rmse_scores.append(rmse)\n",
    "    print(f\"Rolling window RMSE: {rmse}\")\n",
    "\n",
    "# Calculate average RMSE across rolling windows\n",
    "avg_rolling_rmse = np.mean(rolling_rmse_scores)\n",
    "print(f\"Average Rolling Window RMSE: {avg_rolling_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8317f036-759d-4be2-9cb3-b6d4ca615012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to descale RMSE\n",
    "def descale_rmse(scaled_rmse, scaler_y):\n",
    "    # Reshape RMSE value to fit scaler requirements\n",
    "    scaled_rmse_array = np.array(scaled_rmse).reshape(-1, 1)\n",
    "    # Inverse transform the RMSE\n",
    "    descaled_rmse_array = scaler_y.inverse_transform(scaled_rmse_array)\n",
    "    # Return the descaled RMSE as a list\n",
    "    return descaled_rmse_array.flatten().tolist()\n",
    "\n",
    "# Descale TSCV RMSE values\n",
    "descaled_tscv_rmse = descale_rmse(tscv_rmse_scores, scaler_y)\n",
    "print(\"Descaled TSCV RMSE Values:\", descaled_tscv_rmse)\n",
    "\n",
    "# Descale FRWCV RMSE values\n",
    "descaled_frwcv_rmse = descale_rmse(rolling_rmse_scores, scaler_y)\n",
    "print(\"Descaled FRWCV RMSE Values:\", descaled_frwcv_rmse)\n",
    "\n",
    "# Calculates average descaled RMSE\n",
    "average_tscv_rmse = np.mean(descaled_tscv_rmse)\n",
    "average_frwcv_rmse = np.mean(descaled_frwcv_rmse)\n",
    "print(f\"Average Descaled TSCV RMSE: {average_tscv_rmse}\")\n",
    "print(f\"Average Descaled FRWCV RMSE: {average_frwcv_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db30ec53-0f15-44ad-b63d-46060c324896",
   "metadata": {},
   "source": [
    "## Evaluation of using different Sequence Length with Min and Max Scaling. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b904e601-6ac1-4116-a244-52a2d20b45f5",
   "metadata": {},
   "source": [
    "### LSTM - MinMax Scaling Performance Metrics\n",
    "\n",
    "| Metrics        | **SL = 4**         | SL = 12         | SL = 26         | SL = 52         |\n",
    "|-----------------|----------------|-----------------|-----------------|-----------------|\n",
    "| **Train RMSE** | 0.020419093    | 0.020398293     | 0.020486306     | 0.020611598     |\n",
    "| **Test RMSE**  | 0.023013218    | 0.023338731     | 0.024209731     | 0.025229578     |\n",
    "| **Train MAE**  | 0.01485473     | 0.014705355     | 0.015043481     | 0.014958896     |\n",
    "| **Test MAE**   | 0.017467919    | 0.017482876     | 0.018667072     | 0.019810834     |\n",
    "| **Train Loss** | 0.111690558    | 0.127861217     | 0.133479208     | 0.117127933     |\n",
    "| **Val Loss**   | 0.130892992    | 0.131033525     | 0.125732034     | 0.155745506     |\n",
    "| **Avg RMSE - TSCV** | 0.365968045 | 0.372052441     | 0.356294733     | 0.376206325     |\n",
    "| **Avg RMSE - FRWCV** | 0.348539135 | 0.349066229     | 0.31970967      | 0.354697836     |\n",
    "| **Train DA**   | 56%            | 52%             | 65%             | 58.00%          |\n",
    "| **Test DA**    | 58%            | 51%             | 67%             | 51.00%          |\n",
    "\n",
    "---\n",
    "\n",
    "**Keys**\n",
    "\n",
    "- RMSE: Root Mean Squared Error\n",
    "- MAE: Mean Absolute Error.\n",
    "- Loss: Training or Validation Loss.\n",
    "- DA: Directional Accuracy.\n",
    "- TSCV: Time-Series Cross Validation.\n",
    "- RFWCV: Rolling Fixed Window Cross Validation.\n",
    "\n",
    "---\n",
    "| Sequence Length (SL) | Fold 1 RMSE | Fold 2 RMSE | Fold 3 RMSE | Fold 4 RMSE | Average RMSE (TSCV) |\n",
    "|-----------------------|-------------|-------------|-------------|-------------|----------------------|\n",
    "| **SL = 4**           | 0.4632      | 0.3403      | 0.2859      | 0.3745      | **0.3660**           |\n",
    "| **SL = 12**          | 0.4602      | 0.3380      | 0.2864      | 0.4036      | **0.3721**           |\n",
    "| **SL = 26**          | 0.4683      | 0.3154      | 0.2537      | 0.3877      | **0.3563**           |\n",
    "| **SL = 52**          | 0.4975      | 0.3134      | 0.2768      | 0.4172      | **0.3762**           |\n",
    "---\n",
    "\n",
    "| Sequence Length (SL) | Rolling Window RMSE Start Index 0 | Index 50 | Index 100 | Index 150 | Index 200 | Index 250 | Index 300 | Index 350 | Index 400 | Index 450 | Index 500 | Average RMSE (FRWCV) |\n",
    "|-----------------------|------------------------------------|----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------------------|\n",
    "| **SL = 4**           | 0.4323                            | 0.4383   | 0.3339    | 0.3224    | 0.2904    | 0.3024    | 0.3096    | 0.2563    | 0.2936    | 0.3808    | 0.4741    | **0.3485**            |\n",
    "| **SL = 12**          | 0.4725                            | 0.4599   | 0.3156    | 0.2990    | 0.2927    | 0.3225    | 0.3196    | 0.2258    | 0.2891    | 0.3345    | 0.5086    | **0.3491**            |\n",
    "| **SL = 26**          | 0.3992                            | 0.5099   | 0.3328    | 0.2702    | 0.2557    | 0.3192    | 0.2000    | 0.1423    | 0.2703    | 0.2369    | 0.5803    | **0.3197**            |\n",
    "| **SL = 52**          | 0.3949                            | 0.3370   | 0.4382    | 0.0881    | 0.1178    | 0.3344    | 0.2811    | 0.1673    | 0.2926    | 1.0955    | 0.3547    | **0.3547**            |\n",
    "\n",
    "---\n",
    "\n",
    "#### Best Model: SL = 4.\n",
    "\n",
    "The **Sequence Legnth of 4**, which can represent approximately **One Month** yielded the most balanced performance between accuracy metrics and its cross-validation scores. It has the **lowest RMSE test** and the **values of the Average RMSE of both CV's are close**, meaning that the LSTM model is having decent generalization across different time splits. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b61bc4d-f4a2-460f-9610-b6eff35b281d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
